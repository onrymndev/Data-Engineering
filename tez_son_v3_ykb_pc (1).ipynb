{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827ae26b-71b4-427a-88d4-c1da5ea652d0",
   "metadata": {},
   "source": [
    "##  SMOTE (Synthetic Minority Over-sampling Technique) \n",
    "\n",
    "## **Introduction**\n",
    "SMOTE is a resampling technique used to handle class imbalance by generating synthetic samples for the minority class instead of simply duplicating existing ones. It works by interpolating between real minority class instances.\n",
    "\n",
    "## **Algorithm Steps**\n",
    "1. **Select a minority class sample** $x_i$ from the dataset.\n",
    "2. **Find its k-nearest neighbors** in the minority class using Euclidean distance.\n",
    "3. **Randomly select one of these neighbors** $x_{nn}$.\n",
    "4. **Generate a synthetic sample** along the line segment joining $x_i$ and $x_{nn}$ using interpolation:\n",
    "\n",
    "   $$\n",
    "   x_{\\text{new}} = x_i + \\lambda \\cdot (x_{nn} - x_i)\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   $$\n",
    "   \\lambda \\sim U(0,1)\n",
    "   $$\n",
    "\n",
    "   is a random number between 0 and 1.\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "For a given minority class instance $x_i$, let $x_{nn}$ be one of its k-nearest neighbors. The synthetic sample is created as:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_i + \\lambda (x_{nn} - x_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i$ is a real minority class instance.\n",
    "- $x_{nn}$ is one of its k-nearest neighbors.\n",
    "- $\\lambda$ is a random number sampled from a uniform distribution:\n",
    "\n",
    "  $$\n",
    "  \\lambda \\sim U(0,1)\n",
    "  $$\n",
    "\n",
    "This process is repeated until the desired number of synthetic samples is generated.\n",
    "\n",
    "## **Advantages of SMOTE**\n",
    "- Reduces class imbalance by adding synthetic samples.\n",
    "- Prevents overfitting caused by simple duplication of minority class instances.\n",
    "- Preserves the relationships between data points.\n",
    "\n",
    "## **Limitations of SMOTE**\n",
    "- Can generate noisy samples if the minority class has a complex distribution.\n",
    "- Does not consider the majority class, which may lead to overlapping regions and potential misclassification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f0ebd57d-d8c2-4a57-8679-a306763d9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score, recall_score, accuracy_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # Ignore all warnings\n",
    "\n",
    "def calculate_metrics(y_test, y_pred_proba):\n",
    "    threshold = np.mean(y_pred_proba)  # Dynamic threshold based on mean\n",
    "    y_pred_binary = [1 if p > 0.5 else 0 for p in y_pred_proba]  # Convert probabilities to binary\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    recall = recall_score(y_test, y_pred_binary)\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "    \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    g_mean = np.sqrt(recall * specificity)\n",
    "\n",
    "    return accuracy, roc_auc, pr_auc, recall, f1, specificity, fp_rate, g_mean\n",
    "\n",
    "def model(X,y):\n",
    "    # Train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ### Train XGBoost WITHOUT SMOTE\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Train XGBoost without SMOTE\n",
    "    model_no_smote = xgb.train(params, dtrain, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_no_smote = model_no_smote.predict(dtest)\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_no_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    y_pred_binary_no_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_no_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_no_smote = calculate_metrics(y_test, y_pred_proba_no_smote)\n",
    "\n",
    "    \n",
    "        ### Apply SMOTE\n",
    "    smote = SMOTEENN(sampling_strategy=1\n",
    "                     , random_state=42)  # Fully balance classes\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    #X_train_resampled,y_train_resampled=smoteenn_with_weighted_enn(pd.DataFrame(X_train,columns=X.columns),y_train['Y'])\n",
    "\n",
    "    #X_train_resampled, y_train_resampled=manual_smoteenn(X_train,y_train)\n",
    "    \n",
    "    # Train XGBoost WITH SMOTE\n",
    "    dtrain_resampled = xgb.DMatrix(X_train_resampled, label=y_train_resampled)\n",
    "    model_with_smote = xgb.train(params, dtrain_resampled, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_with_smote = model_with_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_with_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_with_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_with_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_with_smote = calculate_metrics(y_test, y_pred_proba_with_smote)\n",
    "\n",
    "    # Print comparison\n",
    "    metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(\"{:<20} {:<10} {:<10}\".format(\"Metric\", \"No SMOTE\", \"With Regular SMOTE\"))\n",
    "    for name, no_smote, with_smote in zip(metric_names, metrics_no_smote, metrics_with_smote):\n",
    "        print(f\"{name:<20} {no_smote:.4f}   {with_smote:.4f}\")\n",
    "\n",
    "    return [metrics_no_smote, metrics_with_smote]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d8863b48-8929-4b11-b868-d6ec5e0fb89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance ---\n",
      "Metric               No SMOTE   With Regular SMOTE\n",
      "Accuracy             0.8830   0.8334\n",
      "ROC-AUC              0.9068   0.9037\n",
      "PR-AUC               0.6103   0.6001\n",
      "Recall (Sensitivity) 0.4261   0.8142\n",
      "F1                   0.5138   0.5864\n",
      "Specificity          0.9605   0.8367\n",
      "FP-Rate              0.0395   0.1633\n",
      "G-Mean               0.6398   0.8254\n"
     ]
    }
   ],
   "source": [
    "output=model(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175468c-cf8f-4276-af52-3ee901ee01da",
   "metadata": {},
   "source": [
    "# **Custom SMOTE with Cubic Interpolation (My Development)**\n",
    "\n",
    "## **Introduction**\n",
    "This is a modified version of the **Synthetic Minority Over-sampling Technique (SMOTE)**, where instead of linear interpolation, a **third-degree polynomial interpolation** is used to generate synthetic samples. This method helps preserve complex feature relationships and avoids overly simplistic synthetic samples.\n",
    "\n",
    "## **Algorithm Steps**\n",
    "1. **Identify the minority class** in the dataset.\n",
    "2. **Find its k-nearest neighbors** using Euclidean distance.\n",
    "3. **Randomly select one of these neighbors** $x_{nn}$ for interpolation.\n",
    "4. **Use cubic interpolation** between the selected sample $x_i$ and its neighbor $x_{nn}$:\n",
    "   - Define reference points between $x_i$ and $x_{nn}$.\n",
    "   - Fit a third-degree polynomial for each feature.\n",
    "   - Sample a new synthetic point using the polynomial.\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "For a given minority class instance $x_i$, let $x_{nn}$ be one of its k-nearest neighbors. We define four reference points:\n",
    "\n",
    "$$\n",
    "x_0 = x_i, \\quad x_1 = \\frac{2x_i + x_{nn}}{3}, \\quad x_2 = \\frac{x_i + 2x_{nn}}{3}, \\quad x_3 = x_{nn}\n",
    "$$\n",
    "\n",
    "These points correspond to $t$-values:\n",
    "\n",
    "$$\n",
    "t_0 = 0, \\quad t_1 = 0.33, \\quad t_2 = 0.66, \\quad t_3 = 1\n",
    "$$\n",
    "\n",
    "A third-degree polynomial is fitted for each feature using these values:\n",
    "\n",
    "$$\n",
    "P(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3\n",
    "$$\n",
    "\n",
    "where the coefficients $(a_0, a_1, a_2, a_3)$ are determined by the reference points. A synthetic sample is generated by evaluating the polynomial at a randomly chosen $t_{\\text{rand}} \\sim U(0,1)$:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = P(t_{\\text{rand}})\n",
    "$$\n",
    "\n",
    "This process is repeated until the desired number of synthetic samples is generated.\n",
    "\n",
    "## **Advantages of Custom SMOTE with Cubic Interpolation**\n",
    "- **More realistic synthetic samples**: Cubic interpolation provides a **smoother transition** between real data points.\n",
    "- **Better feature relationships**: Unlike linear SMOTE, this method **captures non-linear patterns** in the data.\n",
    "- **Less risk of generating outliers**: Intermediate points help **constrain synthetic samples** within a reasonable range.\n",
    "\n",
    "## **Limitations**\n",
    "- **Computationally expensive**: Fitting a polynomial for each feature requires more computation than linear interpolation.\n",
    "- **Risk of overfitting**: If the minority class has a complex distribution, the interpolation might introduce synthetic samples that do not generalize well.\n",
    "- **Sensitive to noisy data**: If the minority class contains outliers, the interpolation may exaggerate these variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1054f122-610d-4676-9486-f6e44ca57da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "def custom_smote_with_cubic_interpolation(X: pd.DataFrame, y: pd.Series, target_class=1, k_neighbors=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Custom SMOTE using 3rd-degree polynomial interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series): Target labels.\n",
    "        target_class (int): The minority class to oversample.\n",
    "        k_neighbors (int): Number of nearest neighbors to consider.\n",
    "        sampling_ratio (float): Ratio of synthetic samples to generate relative to minority class.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        X_resampled (pd.DataFrame): New feature matrix with synthetic samples.\n",
    "        y_resampled (pd.Series): Updated target labels.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Ensure `y` is a 1D array\n",
    "    y = y.reset_index(drop=True)  # Ensure proper indexing\n",
    "    \n",
    "    # Separate minority class\n",
    "    X_minority = X[y == target_class]\n",
    "    sampling_ratio=np.floor((X.shape[0]-X_minority.shape[0])/X_minority.shape[0])\n",
    "    \n",
    "    # Fit KNN on minority class\n",
    "    knn = NearestNeighbors(n_neighbors=min(k_neighbors, len(X_minority)))\n",
    "    knn.fit(X_minority)\n",
    "    \n",
    "    # Determine number of synthetic samples to generate\n",
    "    n_samples = int(len(X_minority) * sampling_ratio)\n",
    "\n",
    "    print(n_samples)\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    \n",
    "    for _ in tqdm(range(n_samples)):\n",
    "        # Randomly select a minority sample \n",
    "        idx = np.random.randint(0, len(X_minority))\n",
    "        x_selected = X_minority.iloc[idx].values  # Convert to NumPy array\n",
    "        \n",
    "        # Find k-nearest neighbors\n",
    "        neighbors = knn.kneighbors([x_selected], return_distance=False)[0]\n",
    "        \n",
    "        # Select a random neighbor\n",
    "        neighbor_idx = np.random.choice(neighbors[1:])  # Exclude itself\n",
    "        x_neighbor = X_minority.iloc[neighbor_idx].values  # Convert to NumPy array\n",
    "        \n",
    "        # Fit a 3rd-degree polynomial between x_selected and x_neighbor\n",
    "        t_values = np.array([0, 0.33, 0.66, 1])  # 4 reference points in [0,1]\n",
    "        x_values = np.vstack([x_selected, \n",
    "                              (2*x_selected + x_neighbor)/3, \n",
    "                              (x_selected + 2*x_neighbor)/3, \n",
    "                              x_neighbor])  # Intermediate points\n",
    "        \n",
    "        # Generate polynomial coefficients for each feature\n",
    "        x_synthetic = np.zeros_like(x_selected)\n",
    "        t_random = np.random.rand()  # Random t in [0,1]\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):  # Iterate over all features\n",
    "            poly = Polynomial.fit(t_values, x_values[:, feature_idx], 3)  # Fit cubic polynomial\n",
    "            x_synthetic[feature_idx] = poly(t_random)  # Sample new point\n",
    "        \n",
    "        synthetic_samples.append(x_synthetic)\n",
    "    \n",
    "    # Convert synthetic samples to DataFrame\n",
    "    synthetic_samples_df = pd.DataFrame(synthetic_samples, columns=X.columns)\n",
    "    \n",
    "    # Create new dataset (append synthetic data)\n",
    "    X_resampled = pd.concat([X, synthetic_samples_df], axis=0, ignore_index=True)\n",
    "    y_resampled = pd.concat([y, pd.Series(target_class, index=synthetic_samples_df.index)], axis=0, ignore_index=True)\n",
    "    \n",
    "    return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d8426cb9-68bb-4304-bf08-65d4c24a1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_smote_poly(X,y):\n",
    "    # Train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardize numerical features \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ### Train XGBoost WITHOUT SMOTE\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Train XGBoost without SMOTE\n",
    "    model_no_smote = xgb.train(params, dtrain, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_no_smote = model_no_smote.predict(dtest)\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_no_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    y_pred_binary_no_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_no_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_no_smote = calculate_metrics(y_test, y_pred_proba_no_smote)\n",
    "\n",
    "\n",
    "    X_train_resampled,y_train_resampled=smoteenn_with_weighted_enn(pd.DataFrame(X_train,columns=X.columns),y_train['Y'])\n",
    "\n",
    "    \n",
    "    # Train XGBoost WITH SMOTE\n",
    "    dtrain_resampled = xgb.DMatrix(np.array(X_train_resampled), label=y_train_resampled)\n",
    "    model_with_smote = xgb.train(params, dtrain_resampled, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_with_smote = model_with_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_with_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_with_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_with_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_with_smote = calculate_metrics(y_test, y_pred_proba_with_smote)\n",
    "\n",
    "    # Print comparison\n",
    "    metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(\"{:<20} {:<10} {:<10}\".format(\"Metric\", \"No SMOTE\", \"With Cubic Polynomial SMOTE\"))\n",
    "    for name, no_smote, with_smote in zip(metric_names, metrics_no_smote, metrics_with_smote):\n",
    "        print(f\"{name:<20} {no_smote:.4f}   {with_smote:.4f}\")\n",
    "\n",
    "    return [metrics_no_smote, metrics_with_smote]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fe782-096d-4ff4-ad5b-10b1987f275c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **Model Performance Metrics for Credit Risk Default Prediction**\n",
    "\n",
    "In credit risk modeling, correctly classifying **defaulting customers** is crucial, as misclassifications can lead to **financial losses** (false negatives) or **lost opportunities** (false positives). The following metrics help assess model performance:\n",
    "\n",
    "## **1. Accuracy**\n",
    "Accuracy measures the proportion of correctly classified instances over the total dataset:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $TP$ = True Positives (correctly predicted defaults)\n",
    "- $TN$ = True Negatives (correctly predicted non-defaults)\n",
    "- $FP$ = False Positives (incorrectly predicted defaults)\n",
    "- $FN$ = False Negatives (incorrectly predicted non-defaults)\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- Accuracy gives an overall measure of correctness but can be **misleading in imbalanced datasets** (e.g., if defaults are rare, a model predicting all customers as non-defaults can still have high accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**\n",
    "The **ROC-AUC** measures a model’s ability to distinguish between the positive (default) and negative (non-default) classes. The **ROC curve** plots the **True Positive Rate (Recall)** against the **False Positive Rate (FPR)** at different classification thresholds.  \n",
    "\n",
    "### **Mathematical Formulation:**\n",
    "The **AUC (Area Under Curve)** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{AUC} = \\int_0^1 \\text{TPR} \\, d(\\text{FPR})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **True Positive Rate (TPR) / Recall:**\n",
    "  $$\n",
    "  \\text{TPR} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "- **False Positive Rate (FPR):**\n",
    "  $$\n",
    "  \\text{FPR} = \\frac{FP}{FP + TN}\n",
    "  $$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Higher AUC** means the model **better separates defaults from non-defaults**.\n",
    "- **AUC close to 0.5** suggests the model is **random** (not useful).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. PR-AUC (Precision-Recall Area Under Curve)**\n",
    "PR-AUC measures the area under the **Precision-Recall (PR) curve**, focusing on **positive (default) predictions**.\n",
    "\n",
    "### **Mathematical Formulation:**\n",
    "The **AUC for Precision-Recall** is:\n",
    "\n",
    "$$\n",
    "\\text{PR-AUC} = \\int_0^1 \\text{Precision} \\, d(\\text{Recall})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **Precision (Positive Predictive Value, PPV):**\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "\n",
    "- **Recall (Sensitivity / TPR) (as defined above)**\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **More useful than ROC-AUC** for **imbalanced data** since it focuses on **true defaults**.\n",
    "- **Higher PR-AUC** indicates a better balance between **precision and recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Recall (Sensitivity)**\n",
    "Recall, also called **Sensitivity or True Positive Rate (TPR)**, measures the ability to detect **actual defaults**:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **High recall** ensures **most actual defaults are detected**, minimizing **false negatives**.\n",
    "- **Low recall** means many **defaulting customers** are **missed**, leading to **financial losses**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. F1-Score**\n",
    "F1-Score is the harmonic mean of **Precision** and **Recall**, balancing both:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Best when both False Positives & False Negatives are costly**.\n",
    "- **Useful in imbalanced datasets**, where a high precision or recall alone isn't enough.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Specificity (True Negative Rate)**\n",
    "Specificity measures how well the model identifies **non-defaulting customers**:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Higher specificity** reduces **false alarms (FP)**.\n",
    "- **Too high specificity may mean recall is low**, missing many defaults.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. False Positive Rate (FPR)**\n",
    "FPR is the proportion of **non-defaulting customers incorrectly classified as defaults**:\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN} = 1 - \\text{Specificity}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Low FPR** ensures fewer **non-defaulters are wrongly flagged**, reducing unnecessary **loan rejections**.\n",
    "- **High FPR** can **hurt customer experience**, causing **unnecessary loan rejections**.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. G-Mean (Geometric Mean)**\n",
    "The **G-Mean** is a performance metric balancing **recall** and **specificity**:\n",
    "\n",
    "$$\n",
    "G\\text{-Mean} = \\sqrt{\\text{Recall} \\times \\text{Specificity}}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Higher G-Mean** ensures the model performs well on **both default and non-default classes**.\n",
    "- Useful for **handling class imbalance**, where one metric alone (like accuracy) can be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "# **Summary Table of Metrics**\n",
    "| **Metric**         | **Interpretation** |\n",
    "|--------------------|------------------|\n",
    "| **Accuracy**      | Overall correctness, but misleading in imbalanced data |\n",
    "| **ROC-AUC**       | Ability to distinguish defaults vs. non-defaults |\n",
    "| **PR-AUC**        | Performance on the default class, useful for imbalance |\n",
    "| **Recall**        | Ability to detect defaults (avoid false negatives) |\n",
    "| **F1-Score**      | Balance between Precision & Recall |\n",
    "| **Specificity**   | Correctly identifying non-defaulters |\n",
    "| **FPR**           | Incorrectly flagging non-defaulters as defaults |\n",
    "| **G-Mean**        | Balance between Recall & Specificity (useful for imbalance) |\n",
    "\n",
    "---\n",
    "\n",
    "# **Final Thoughts**\n",
    "For **credit risk prediction**, metrics should be **carefully chosen** based on **business priorities**:\n",
    "\n",
    "- **If missing defaults is costly** → **High Recall (Sensitivity)**.\n",
    "- **If wrongly flagging non-defaulters is a concern** → **Low False Positive Rate (FPR)**.\n",
    "- **For overall balance** → **High G-Mean & F1-Score**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba468a0-0d7a-47a1-8880-546940336b76",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f189a895-6caf-4240-be53-b071af2e5239",
   "metadata": {},
   "source": [
    "## Data Fetch in YKB Computer - Taiwan Credit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "680f11c0-af5f-49e4-9f54-8114a4a35607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13559.0</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49291.0</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35835.0</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1   X2   X3   X4    X5   X6   X7   X8   X9  X10  ...      X14  \\\n",
       "1   20000.0  2.0  2.0  1.0  24.0  2.0  2.0 -1.0 -1.0 -2.0  ...    689.0   \n",
       "2  120000.0  2.0  2.0  2.0  26.0 -1.0  2.0  0.0  0.0  0.0  ...   2682.0   \n",
       "3   90000.0  2.0  2.0  2.0  34.0  0.0  0.0  0.0  0.0  0.0  ...  13559.0   \n",
       "4   50000.0  2.0  2.0  1.0  37.0  0.0  0.0  0.0  0.0  0.0  ...  49291.0   \n",
       "5   50000.0  1.0  2.0  1.0  57.0 -1.0  0.0 -1.0  0.0  0.0  ...  35835.0   \n",
       "\n",
       "       X15      X16      X17     X18      X19      X20     X21     X22     X23  \n",
       "1      0.0      0.0      0.0     0.0    689.0      0.0     0.0     0.0     0.0  \n",
       "2   3272.0   3455.0   3261.0     0.0   1000.0   1000.0  1000.0     0.0  2000.0  \n",
       "3  14331.0  14948.0  15549.0  1518.0   1500.0   1000.0  1000.0  1000.0  5000.0  \n",
       "4  28314.0  28959.0  29547.0  2000.0   2019.0   1200.0  1100.0  1069.0  1000.0  \n",
       "5  20940.0  19146.0  19131.0  2000.0  36681.0  10000.0  9000.0   689.0   679.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('default of credit card clients.xls',index_col=0).iloc[1:,:]\n",
    "\n",
    "X=df.iloc[:,:-1]\n",
    "y=pd.DataFrame(df.iloc[:,-1],columns=['Y'])\n",
    "X=X.astype(float)\n",
    "y=y.astype(int)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b73559c9-8c8d-4cc1-8030-07a54d5dc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "def remove_tomek_links(X, y):\n",
    "    \"\"\"\n",
    "    Removes Tomek Links from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pandas.DataFrame): Feature matrix of shape (n_samples, n_features)\n",
    "    y (pandas.Series): Target vector of shape (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "    X_resampled, y_resampled: The feature matrix and target vector after removing Tomek Links\n",
    "                              returned as pandas DataFrame and Series respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    tl = TomekLinks()\n",
    "    X_resampled, y_resampled = tl.fit_resample(X.values, y.values)\n",
    "    \n",
    "    # Convert back to DataFrame and Series\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    y_resampled = pd.Series(y_resampled)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your data in variables `X` as a DataFrame and `y` as a Series\n",
    "# X_resampled, y_resampled = remove_tomek_links(X, y)\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "def apply_near_miss(X, y):\n",
    "    \"\"\"\n",
    "    Applies the NearMiss-1 technique to under-sample the majority class.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pandas.DataFrame): Feature matrix of shape (n_samples, n_features)\n",
    "    y (pandas.Series): Target vector of shape (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "    X_resampled, y_resampled: The feature matrix and target vector after applying NearMiss,\n",
    "                              returned as pandas DataFrame and Series respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    nm = NearMiss(version=1)  # Version 1 for NearMiss-1\n",
    "    X_resampled, y_resampled = nm.fit_resample(X.values, y.values)\n",
    "    \n",
    "    # Convert back to DataFrame and Series\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    y_resampled = pd.Series(y_resampled)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "import pandas as pd\n",
    "\n",
    "def apply_enn(X, y):\n",
    "    \"\"\"\n",
    "    Applies the Edited Nearest Neighbors (ENN) technique to clean the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pandas.DataFrame): Feature matrix of shape (n_samples, n_features)\n",
    "    y (pandas.Series): Target vector of shape (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "    X_resampled, y_resampled: The feature matrix and target vector after applying ENN,\n",
    "                              returned as pandas DataFrame and Series respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    enn = EditedNearestNeighbours()\n",
    "    X_resampled, y_resampled = enn.fit_resample(X.values, y.values)\n",
    "    \n",
    "    # Convert back to DataFrame and Series\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    y_resampled = pd.Series(y_resampled)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "def manual_smoteenn(X, y, k_neighbors=5):\n",
    "    \"\"\"\n",
    "    Replicate the behavior of SMOTEENN by applying SMOTE and then ENN only to the majority class.\n",
    "\n",
    "    Parameters:\n",
    "        X (pandas.DataFrame): Feature matrix of shape (n_samples, n_features).\n",
    "        y (pandas.Series): Target labels of shape (n_samples,).\n",
    "        k_neighbors (int): Number of neighbors to use for SMOTE and ENN.\n",
    "\n",
    "    Returns:\n",
    "        X_resampled (pandas.DataFrame): Resampled and cleaned feature matrix.\n",
    "        y_resampled (pandas.Series): Resampled and cleaned target labels.\n",
    "    \"\"\"\n",
    "    # Convert X and y to numpy arrays for compatibility with imblearn\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "\n",
    "    # Step 1: Apply SMOTE to oversample the minority class\n",
    "    smote = SMOTE(k_neighbors=k_neighbors)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Step 2: Apply ENN only to the majority class\n",
    "    # Identify the majority class\n",
    "    majority_class = 0\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    X_majority = X_resampled[y_resampled == majority_class]\n",
    "    y_majority = y_resampled[y_resampled == majority_class]\n",
    "    X_minority = X_resampled[y_resampled != majority_class]\n",
    "    y_minority = y_resampled[y_resampled != majority_class]\n",
    "\n",
    "    # Apply ENN only to the majority class\n",
    "    enn = EditedNearestNeighbours(n_neighbors=3)\n",
    "    X_majority_resampled, y_majority_resampled = enn.fit_resample(X_majority, y_majority)\n",
    "\n",
    "    # Combine the resampled majority class with the minority class\n",
    "    X_resampled = np.vstack([X_minority, X_majority_resampled])\n",
    "    y_resampled = np.hstack([y_minority, y_majority_resampled])\n",
    "\n",
    "    # Convert back to pandas DataFrame and Series\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    y_resampled = pd.Series(y_resampled)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------------------\n",
    "# SMOTE - Synthetic Minority Oversampling\n",
    "# -------------------------------------\n",
    "\n",
    "def synthetic_sample(point, neighbor, random_state=None):\n",
    "    \"\"\" Generate a synthetic sample between a point and its neighbor. \"\"\"\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    return point + np.random.rand() * (neighbor - point)\n",
    "\n",
    "def smote(X, y, minority_class, n_neighbors=5, random_state=None):\n",
    "    \"\"\" Implements SMOTE: Generates synthetic samples for the minority class. \"\"\"\n",
    "    minority_samples = X[y == minority_class]\n",
    "    n_samples_to_generate = len(X[y != minority_class]) - len(minority_samples)\n",
    "\n",
    "    if n_samples_to_generate <= 0:\n",
    "        return X, y\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors).fit(minority_samples)\n",
    "    synthetic_samples = []\n",
    "\n",
    "    for _ in range(n_samples_to_generate):\n",
    "        idx = np.random.randint(0, len(minority_samples))\n",
    "        point = minority_samples[idx]\n",
    "        neighbors = nn.kneighbors([point], return_distance=False).flatten()\n",
    "        neighbor = minority_samples[np.random.choice(neighbors[1:])]\n",
    "        synthetic_samples.append(synthetic_sample(point, neighbor, random_state))\n",
    "\n",
    "    synthetic_samples = np.array(synthetic_samples)\n",
    "    X_resampled = np.vstack((X, synthetic_samples))\n",
    "    y_resampled = np.hstack((y, np.array([minority_class] * len(synthetic_samples))))\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# -------------------------------------\n",
    "# ENN - Distance Weighted Edited Nearest Neighbors\n",
    "# -------------------------------------\n",
    "\n",
    "def weighted_majority_voting(neighbors_labels, neighbors_distances):\n",
    "    \"\"\"\n",
    "    Performs weighted majority voting based on inverse distance.\n",
    "    Closer neighbors have more influence.\n",
    "    \"\"\"\n",
    "    weights = 1 / (neighbors_distances + 1e-5)  # Avoid division by zero\n",
    "    label_weights = {}\n",
    "\n",
    "    for label, weight in zip(neighbors_labels, weights):\n",
    "        label_weights[label] = label_weights.get(label, 0) + weight\n",
    "\n",
    "    return max(label_weights, key=label_weights.get)\n",
    "\n",
    "def enn_with_distance_weighting(X, y, n_neighbors=3):\n",
    "    \"\"\"\n",
    "    ENN with distance weighting. Keeps samples whose label agrees \n",
    "    with the weighted majority of their neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors+1).fit(X)\n",
    "    keep_indices = []\n",
    "\n",
    "    for idx, (point, label) in enumerate(zip(X, y)):\n",
    "        neighbors_distances, neighbors = nn.kneighbors([point])\n",
    "        neighbors_distances = neighbors_distances.flatten()[1:]  # Exclude self\n",
    "        neighbors_labels = y[neighbors.flatten()[1:]]\n",
    "\n",
    "        # Get weighted majority class among neighbors\n",
    "        majority_label = weighted_majority_voting(neighbors_labels, neighbors_distances)\n",
    "\n",
    "        # Keep the point if the weighted majority agrees with the label\n",
    "        if label == majority_label:\n",
    "            keep_indices.append(idx)\n",
    "\n",
    "    return X[keep_indices], y[keep_indices]\n",
    "\n",
    "# -------------------------------------\n",
    "# SMOTEENN - Full Pipeline\n",
    "# -------------------------------------\n",
    "\n",
    "def smoteenn_with_weighted_enn(X, y, n_neighbors_smote=5, n_neighbors_enn=3, random_state=None):\n",
    "    \"\"\" Combines SMOTE oversampling and distance-weighted ENN undersampling. \"\"\"\n",
    "    X=X.values\n",
    "    y=y.values\n",
    "    \n",
    "    # Identify the minority class\n",
    "    class_counts = Counter(y)\n",
    "    minority_class = min(class_counts, key=class_counts.get)\n",
    "\n",
    "    # Apply SMOTE\n",
    "    X_resampled, y_resampled = smote(X, y, minority_class, n_neighbors=n_neighbors_smote, random_state=random_state)\n",
    "\n",
    "    print(type(X_resampled))\n",
    "\n",
    "    # Apply distance-weighted ENN\n",
    "    X_final, y_final = enn_with_distance_weighting(X_resampled, y_resampled, n_neighbors=n_neighbors_enn)\n",
    "\n",
    "    print(\"Original class distribution:\", class_counts)\n",
    "    print(\"After SMOTE:\", Counter(y_resampled))\n",
    "    print(\"After distance-weighted ENN:\", Counter(y_final))\n",
    "\n",
    "    return X_final, y_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b8268-4316-45ac-9358-5d5a948df58c",
   "metadata": {},
   "source": [
    "## Data Fetch in Personal Computer -- Taiwan Credit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c80ea1eb-3c70-4d9a-a243-f80a5b3868cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ucimlrepo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mucimlrepo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_ucirepo \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# fetch dataset \u001b[39;00m\n\u001b[1;32m      4\u001b[0m default_of_credit_card_clients \u001b[38;5;241m=\u001b[39m fetch_ucirepo(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m350\u001b[39m) \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ucimlrepo'"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = default_of_credit_card_clients.data.features \n",
    "y = default_of_credit_card_clients.data.targets \n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f461982a-7224-406e-b1f5-6b3eacff491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Positive targets : 22.12%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of Positive targets : {((y.sum()/y.count())*100).values[0]}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb95e0-1164-4b60-8dc2-531b93cbd712",
   "metadata": {},
   "source": [
    "## Model Training And Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "18a76959-6f33-43a0-b516-86a6f0ea4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taiwan Credit dataset: \n",
      "\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No SMOTE   With Regular SMOTE\n",
      "Accuracy             0.8190   0.7567\n",
      "ROC-AUC              0.7771   0.7688\n",
      "PR-AUC               0.5392   0.5191\n",
      "Recall (Sensitivity) 0.3566   0.6087\n",
      "F1                   0.4618   0.5214\n",
      "Specificity          0.9477   0.7979\n",
      "FP-Rate              0.0523   0.2021\n",
      "G-Mean               0.5814   0.6969\n",
      "<class 'numpy.ndarray'>\n",
      "Original class distribution: Counter({np.int64(0): 16324, np.int64(1): 4676})\n",
      "After SMOTE: Counter({np.int64(1): 16324, np.int64(0): 16324})\n",
      "After distance-weighted ENN: Counter({np.int64(1): 15671, np.int64(0): 11663})\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No SMOTE   With Cubic Polynomial SMOTE\n",
      "Accuracy             0.8190   0.7683\n",
      "ROC-AUC              0.7771   0.7660\n",
      "PR-AUC               0.5392   0.5190\n",
      "Recall (Sensitivity) 0.3566   0.5755\n",
      "F1                   0.4618   0.5197\n",
      "Specificity          0.9477   0.8220\n",
      "FP-Rate              0.0523   0.1780\n",
      "G-Mean               0.5814   0.6878\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Taiwan Credit dataset: \\n\")\n",
    "output=model(X,y)\n",
    "output_2=model_smote_poly(X,y)\n",
    "output.append(output_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b42e858-3353-4e65-80b3-e592f3ecc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance ---\n",
      "Metric                         No SMOTE   With Regular SMOTE   With Cubic Polynomial Interpolation SMOTE\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:<30}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<10}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<20}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<20}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo SMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith Regular SMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith Cubic Polynomial Interpolation SMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(metric_names)):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:<30}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m              \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m              \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m---> 11\u001b[0m         metric_names[i], \u001b[43mmetrics\u001b[49m[\u001b[38;5;241m0\u001b[39m][i], metrics[\u001b[38;5;241m1\u001b[39m][i], metrics[\u001b[38;5;241m2\u001b[39m][i]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \n",
    "                \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "\n",
    "print(\"\\n--- Model Performance ---\")\n",
    "print(\"{:<30} {:<10} {:<20} {:<20}\".format(\"Metric\", \"No SMOTE\",\n",
    "                                            \"With Regular SMOTE\",\n",
    "                                            \"With Cubic Polynomial Interpolation SMOTE\"))\n",
    "\n",
    "for i in range(len(metric_names)):\n",
    "    print(\"{:<30} {:.3f}              {:.3f}              {:.3f}\".format(\n",
    "        metric_names[i], metrics[0][i], metrics[1][i], metrics[2][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cee6d-d1e1-4220-87c9-9cf80f387bf9",
   "metadata": {},
   "source": [
    "# **ADASYN: Adaptive Synthetic Sampling Approach with Cubic Polynomial Interpolation**\n",
    "\n",
    "## **1. Introduction**\n",
    "In classification tasks with imbalanced datasets, where the number of instances in the minority class is significantly lower than in the majority class, machine learning models tend to be biased towards the majority class. To address this, various oversampling techniques have been developed, including the **Adaptive Synthetic Sampling (ADASYN) algorithm**.  \n",
    "\n",
    "ADASYN improves upon traditional oversampling techniques, such as **SMOTE**, by adaptively generating synthetic samples according to the **local distribution** of the minority class. Specifically, it focuses on generating more synthetic samples in **harder-to-learn regions**, where the local class imbalance is more pronounced.  \n",
    "\n",
    "In this work, we **replace the standard linear interpolation method** used in ADASYN with **cubic polynomial interpolation**, which provides a smoother and more diverse distribution of synthetic samples in high-dimensional feature spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Algorithm Description**\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times m}$ represent the dataset, where $n$ is the number of samples, and $m$ is the number of features. Let the class labels be $y \\in \\{C_1, C_2\\}$, where $C_1$ is the minority class and $C_2$ is the majority class.\n",
    "\n",
    "### **Step 1: Define the Minority and Majority Classes**\n",
    "The number of instances in each class is computed as:\n",
    "\n",
    "$$\n",
    "n_{\\text{min}} = |X_{\\text{min}}|, \\quad n_{\\text{maj}} = |X_{\\text{maj}}|\n",
    "$$\n",
    "\n",
    "where $X_{\\text{min}}$ and $X_{\\text{maj}}$ represent the subsets of $X$ belonging to the minority and majority classes, respectively.\n",
    "\n",
    "The class imbalance ratio is then given by:\n",
    "\n",
    "$$\n",
    "d = \\frac{n_{\\text{min}}}{n_{\\text{maj}}}\n",
    "$$\n",
    "\n",
    "ADASYN aims to **balance the dataset** by generating synthetic samples until $d \\approx 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute the Number of Synthetic Samples**\n",
    "The total number of synthetic samples to be generated is:\n",
    "\n",
    "$$\n",
    "G = n_{\\text{maj}} - n_{\\text{min}}\\cdot\\tilde{β} \n",
    "$$\n",
    "\n",
    "Each minority sample $x_i$ is assigned a weight based on its difficulty of classification.Where β∈ [0, 1] is a parameter used to specify the desired\n",
    "balance level after generation of the synthetic data. β = 1\n",
    "means a fully balanced data set is created after the generalization process.\n",
    "\n",
    "For each $x_i \\in X_{\\text{min}}$, we compute the number of its $k$-nearest neighbors belonging to the majority class $X_{\\text{maj}}$. Let $k_i^{\\text{maj}}$ denote this count. The local distribution ratio $r_i$ is computed as:\n",
    "\n",
    "$$\n",
    "r_i = \\frac{k_i^{\\text{maj}}}{k}\n",
    "$$\n",
    "\n",
    "where $k$ is the total number of nearest neighbors considered.\n",
    "\n",
    "The normalized weight for each $x_i$ is then:\n",
    "\n",
    "$$\n",
    "\\tilde{r}_i = \\frac{r_i}{\\sum_{j=1}^{n_{\\text{min}}} r_j}\n",
    "$$\n",
    "\n",
    "The number of synthetic samples required for each $x_i$ is:\n",
    "\n",
    "$$\n",
    "G_i = G \\cdot \\tilde{r}_i\n",
    "$$\n",
    "\n",
    "where $G_i$ is an integer value indicating the number of new samples to generate for $x_i$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Generate Synthetic Samples Using Cubic Polynomial Interpolation**\n",
    "For each minority sample $x_i$ requiring $G_i$ synthetic samples, a random neighbor $x_j \\in X_{\\text{min}}$ from its $k$-nearest neighbors is selected.\n",
    "\n",
    "#### **Cubic Polynomial Interpolation**\n",
    "Instead of using linear interpolation, we apply cubic polynomial interpolation for smoother synthetic data generation.\n",
    "\n",
    "For each feature $f \\in \\{1, 2, \\dots, m\\}$, we define **four control points**:\n",
    "- $P_0 = x_{i,f}$ (original sample)\n",
    "- $P_1 = 0.5 (x_{i,f} + x_{j,f})$ (midpoint control)\n",
    "- $P_2 = 0.5 (x_{i,f} + x_{j,f})$ (another midpoint control)\n",
    "- $P_3 = x_{j,f}$ (selected neighbor)\n",
    "\n",
    "The corresponding interpolation domain values are:\n",
    "\n",
    "$$\n",
    "X_{\\text{points}} = [0, 0.33, 0.67, 1]\n",
    "$$\n",
    "\n",
    "The values at these points are:\n",
    "\n",
    "$$\n",
    "Y_{\\text{points}} = [P_0, P_1, P_2, P_3]\n",
    "$$\n",
    "\n",
    "Using **CubicSpline interpolation**, we generate a synthetic sample by selecting a random interpolation coefficient $g \\sim U(0,1)$ and computing:\n",
    "\n",
    "$$\n",
    "\\tilde{x}_{f} = \\text{CubicSpline}(g)\n",
    "$$\n",
    "\n",
    "This process is repeated for all $m$ features, resulting in a synthetic sample $\\tilde{x}$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Update the Dataset**\n",
    "The newly generated synthetic samples $\\tilde{X}$ are added to the original dataset:\n",
    "\n",
    "$$\n",
    "X' = X \\cup \\tilde{X}, \\quad y' = y \\cup \\tilde{y}\n",
    "$$\n",
    "\n",
    "where $\\tilde{y}$ contains the label of the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Conclusion**\n",
    "The proposed ADASYN implementation with cubic polynomial interpolation provides several advantages over standard linear interpolation methods:\n",
    "- **Enhanced diversity of synthetic samples**: The cubic interpolation technique generates smoother and more naturally distributed synthetic points in the feature space.\n",
    "- **Better generalization**: By adapting sample generation based on difficulty, ADASYN reduces the risk of overfitting caused by naive oversampling.\n",
    "- **Improved robustness in high-dimensional spaces**: Unlike linear interpolation, cubic interpolation mitigates abrupt transitions in feature values, making the synthetic data more realistic.\n",
    "\n",
    "This approach is particularly beneficial for imbalanced datasets where minority class samples exhibit complex distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f7369bf-673c-421e-a5b0-0c977260d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "def model_adasyn(X,y):\n",
    "    # Train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ### Train XGBoost WITHOUT ADASYN\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Train XGBoost without ADASYN\n",
    "    model_no_smote = xgb.train(params, dtrain, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_no_smote = model_no_smote.predict(dtest)\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_no_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    y_pred_binary_no_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_no_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_no_smote = calculate_metrics(y_test, y_pred_proba_no_smote)\n",
    "\n",
    "    \n",
    "    ### Apply ADASYN\n",
    "    adasyn = ADASYN(sampling_strategy=1, random_state=42)  # Fully balance classes\n",
    "    X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train XGBoost WITH ADASYN\n",
    "    dtrain_resampled = xgb.DMatrix(X_train_resampled, label=y_train_resampled)\n",
    "    model_with_smote = xgb.train(params, dtrain_resampled, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_with_smote = model_with_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_with_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_with_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_with_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_with_smote = calculate_metrics(y_test, y_pred_proba_with_smote)\n",
    "\n",
    "    # Print comparison\n",
    "    metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(\"{:<20} {:<10} {:<10}\".format(\"Metric\", \"No ADASYN\", \"With Regular ADASYN\"))\n",
    "    for name, no_smote, with_smote in zip(metric_names, metrics_no_smote, metrics_with_smote):\n",
    "        print(f\"{name:<20} {no_smote:.4f}   {with_smote:.4f}\")\n",
    "\n",
    "    return [metrics_no_smote, metrics_with_smote]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ae260cc-c8d7-453e-a576-24dfa9783b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def custom_adasyn_with_cubic_interpolation(X, y, k_neighbors, beta, random_state=42):\n",
    "    \"\"\"\n",
    "    Implements the ADASYN algorithm with cubic polynomial interpolation for high-dimensional data.\n",
    "\n",
    "    Parameters:\n",
    "    - X: ndarray, shape (n_samples, n_features)\n",
    "        Feature matrix.\n",
    "    - y: ndarray, shape (n_samples,)\n",
    "        Target labels.\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled: ndarray\n",
    "        Resampled feature matrix with synthetic samples.\n",
    "    - y_resampled: ndarray\n",
    "        Resampled target labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure `y` is a 1D array\n",
    "    y = y.reset_index(drop=True)  # Ensure proper indexing\n",
    "    \n",
    "    # Separate minority class\n",
    "\n",
    "    # Identify minority and majority class\n",
    "    classes, class_counts = np.unique(y, return_counts=True)\n",
    "    minority_class = classes[np.argmin(class_counts)]\n",
    "    majority_class = classes[np.argmax(class_counts)]\n",
    "    \n",
    "    X_minority = X[y == minority_class]\n",
    "    n_minority = len(X_minority)\n",
    "    n_majority = len(X[y == majority_class])\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # Step 1: Compute the number of synthetic samples to generate\n",
    "    d = n_majority - n_minority  # Imbalance factor\n",
    "    G = d*beta  # Total synthetic samples needed\n",
    "\n",
    "    # Step 2: Find k-nearest neighbors for each minority sample\n",
    "    k = k_neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "    neighbors = knn.kneighbors(X_minority, return_distance=False)[:, 1:]\n",
    "\n",
    "    # Step 3: Compute the imbalance degree ri for each minority sample\n",
    "    ri = np.array([sum(y[neighbors[i]] != minority_class) / k for i in range(n_minority)])\n",
    "    if ri.sum() == 0:\n",
    "        return X, y  # No synthetic samples needed\n",
    "    ri = ri / ri.sum()  # Normalize ri to sum to 1\n",
    "\n",
    "    # Step 4: Generate synthetic samples using cubic polynomial interpolation\n",
    "    X_synthetic = []\n",
    "    for i in tqdm(range(n_minority)):\n",
    "        Gi = int(G * ri[i])  # Number of samples to generate for instance i\n",
    "        for _ in range(Gi):\n",
    "\n",
    "            neighbor_idx = np.random.choice(neighbors[i])  # Select a random neighbor\n",
    "            x_selected = X_minority.iloc[i]  # Minority instance\n",
    "            x_neighbor = X.iloc[neighbor_idx]  # Chosen neighbor\n",
    "\n",
    "            # Create synthetic sample feature-wise\n",
    "            \n",
    "            idx = np.random.choice(neighbors[i])  # Select a random neighbor\n",
    "            t_values = np.array([0, 0.33, 0.66, 1])  # 4 reference points in [0,1]\n",
    "            x_values = np.vstack([x_selected, \n",
    "                                  (2*x_selected + x_neighbor)/3, \n",
    "                                  (x_selected + 2*x_neighbor)/3, \n",
    "                                  x_neighbor])  # Intermediate points\n",
    "            # Generate polynomial coefficients for each feature\n",
    "            x_synthetic = np.zeros_like(x_selected)\n",
    "            t_random = np.random.rand()  # Random t in [0,1]\n",
    "            \n",
    "            for feature_idx in range(X.shape[1]):  # Iterate over all features\n",
    "                poly = Polynomial.fit(t_values, x_values[:, feature_idx], 3)  # Fit cubic polynomial\n",
    "                x_synthetic[feature_idx] = poly(t_random)  # Sample new point\n",
    "            \n",
    "\n",
    "            X_synthetic.append(x_synthetic)\n",
    "\n",
    "    X_synthetic = np.array(X_synthetic)\n",
    "    y_synthetic = np.full(len(X_synthetic), minority_class)\n",
    "\n",
    "    # Step 5: Return the augmented dataset\n",
    "    X_resampled = np.vstack((X, X_synthetic))\n",
    "    y_resampled = np.hstack((y, y_synthetic))\n",
    "\n",
    "    print(X_resampled.shape)\n",
    "\n",
    "    return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ee9a384-6c2d-4882-b177-9543acc4a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "637daa8c-dab8-489c-8f3a-cf34777585ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_adasyn_poly(X,y):\n",
    "    # Train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardize numerical features \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ### Train XGBoost WITHOUT ADASYN\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Train XGBoost without ADASYN\n",
    "    model_no_smote = xgb.train(params, dtrain, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_no_smote = model_no_smote.predict(dtest)\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_no_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    y_pred_binary_no_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_no_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_no_smote = calculate_metrics(y_test, y_pred_proba_no_smote)\n",
    "\n",
    "    pd.DataFrame(X_train,columns=X.columns)\n",
    "\n",
    "    X_train_resampled, y_train_resampled = custom_adasyn_with_cubic_interpolation(pd.DataFrame(X_train,columns=X.columns), y_train,5,1)\n",
    "    \n",
    "    # Train XGBoost WITH ADASYN\n",
    "    dtrain_resampled = xgb.DMatrix(np.array(X_train_resampled), label=y_train_resampled)\n",
    "    model_with_smote = xgb.train(params, dtrain_resampled, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_with_smote = model_with_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_with_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_with_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_with_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_with_smote = calculate_metrics(y_test, y_pred_proba_with_smote)\n",
    "\n",
    "    # Print comparison\n",
    "    metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(\"{:<20} {:<10} {:<10}\".format(\"Metric\", \"No ADASYN\", \"With Cubic Polynomial ADASYN\"))\n",
    "    for name, no_smote, with_smote in zip(metric_names, metrics_no_smote, metrics_with_smote):\n",
    "        print(f\"{name:<20} {no_smote:.4f}   {with_smote:.4f}\")\n",
    "\n",
    "    return [metrics_no_smote, metrics_with_smote]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0a8df4e-9e67-4666-b63f-d32976b63b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taiwan Credit dataset: \n",
      "\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No ADASYN  With Regular ADASYN\n",
      "Accuracy             0.8190   0.8018\n",
      "ROC-AUC              0.7771   0.7582\n",
      "PR-AUC               0.5392   0.5191\n",
      "Recall (Sensitivity) 0.3566   0.4306\n",
      "F1                   0.4618   0.4862\n",
      "Specificity          0.9477   0.9051\n",
      "FP-Rate              0.0523   0.0949\n",
      "G-Mean               0.5814   0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4676/4676 [00:16<00:00, 278.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30908, 23)\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No ADASYN  With Cubic Polynomial ADASYN\n",
      "Accuracy             0.8190   0.8036\n",
      "ROC-AUC              0.7771   0.7596\n",
      "PR-AUC               0.5392   0.5172\n",
      "Recall (Sensitivity) 0.3566   0.4388\n",
      "F1                   0.4618   0.4931\n",
      "Specificity          0.9477   0.9051\n",
      "FP-Rate              0.0523   0.0949\n",
      "G-Mean               0.5814   0.6302\n"
     ]
    }
   ],
   "source": [
    "print(\"Taiwan Credit dataset: \\n\")\n",
    "output=model_adasyn(X,y)\n",
    "output_2=model_adasyn_poly(X,y['Y'])\n",
    "output.append(output_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4bd5c2bb-7139-495b-afdc-cd1daec6d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "163fde3a-d4c3-4ca9-b34f-ae3256997672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance ---\n",
      "Metric                         No ADASYN  With Regular ADASYN  With Cubic Polynomial Interpolation ADASYN\n",
      "Accuracy                       0.819              0.802              0.804\n",
      "ROC-AUC                        0.777              0.758              0.760\n",
      "PR-AUC                         0.539              0.519              0.517\n",
      "Recall (Sensitivity)           0.357              0.431              0.439\n",
      "F1                             0.462              0.486              0.493\n",
      "Specificity                    0.948              0.905              0.905\n",
      "FP-Rate                        0.052              0.095              0.095\n",
      "G-Mean                         0.581              0.624              0.630\n"
     ]
    }
   ],
   "source": [
    "metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \n",
    "                \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "\n",
    "print(\"\\n--- Model Performance ---\")\n",
    "print(\"{:<30} {:<10} {:<20} {:<20}\".format(\"Metric\", \"No ADASYN\",\n",
    "                                            \"With Regular ADASYN\",\n",
    "                                            \"With Cubic Polynomial Interpolation ADASYN\"))\n",
    "\n",
    "for i in range(len(metric_names)):\n",
    "    print(\"{:<30} {:.3f}              {:.3f}              {:.3f}\".format(\n",
    "        metric_names[i], metrics[0][i], metrics[1][i], metrics[2][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c109093-81e9-4d9d-a029-4e6b402b5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Taiwan Credit dataset: \\n\")\n",
    "output=model(X,y)\n",
    "output_2=model_smote_poly(X,y['Y'])\n",
    "output.append(output_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fd78e-d6a1-4e79-82f2-5aec21260315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa86591-b3bb-459a-910f-45d8cdf1591e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "65d907f3-9f53-46c9-a8dd-5468aaa8d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lending=pd.read_csv('filtered_loans.csv')\n",
    "X=df_lending.drop('loan_status',axis=1)\n",
    "y=df_lending[['loan_status']]\n",
    "y = y.replace({0: 1, 1: 0})\n",
    "y.columns=['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "17a4e792-9341-48dc-9d60-eaa059bf96dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lending Club Credit dataset: \n",
      "\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No SMOTE   With Regular SMOTE\n",
      "Accuracy             0.8830   0.8334\n",
      "ROC-AUC              0.9068   0.9037\n",
      "PR-AUC               0.6103   0.6001\n",
      "Recall (Sensitivity) 0.4261   0.8142\n",
      "F1                   0.5138   0.5864\n",
      "Specificity          0.9605   0.8367\n",
      "FP-Rate              0.0395   0.1633\n",
      "G-Mean               0.6398   0.8254\n",
      "<class 'numpy.ndarray'>\n",
      "Original class distribution: Counter({np.int64(0): 23497, np.int64(1): 3944})\n",
      "After SMOTE: Counter({np.int64(0): 23497, np.int64(1): 23497})\n",
      "After distance-weighted ENN: Counter({np.int64(1): 23467, np.int64(0): 18529})\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No SMOTE   With Cubic Polynomial SMOTE\n",
      "Accuracy             0.8830   0.8479\n",
      "ROC-AUC              0.9068   0.9040\n",
      "PR-AUC               0.6103   0.6009\n",
      "Recall (Sensitivity) 0.4261   0.7579\n",
      "F1                   0.5138   0.5911\n",
      "Specificity          0.9605   0.8632\n",
      "FP-Rate              0.0395   0.1368\n",
      "G-Mean               0.6398   0.8088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Lending Club Credit dataset: \\n\")\n",
    "output=model(X,y)\n",
    "output_2=model_smote_poly(X,y)\n",
    "#output.append(output_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd3057c-38f2-42b3-9e90-d78b3a0edcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Positive targets : 14.412529972960563%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of Positive targets : {((y.sum()/y.count())*100).values[0]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cebd42be-a967-44a2-8544-75023fd13ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from tqdm import tqdm\n",
    "\n",
    "def custom_smote_with_density(X: pd.DataFrame, y: pd.Series, target_class=1, k_neighbors=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Custom SMOTE using cubic interpolation with density-aware sampling.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series): Target labels.\n",
    "        target_class (int): The minority class to oversample.\n",
    "        k_neighbors (int): Number of nearest neighbors to consider.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        X_resampled (pd.DataFrame): New feature matrix with synthetic samples.\n",
    "        y_resampled (pd.Series): Updated target labels.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    y = y.reset_index(drop=True)  \n",
    "    X_minority = X[y == target_class]\n",
    "    X_majority = X[y != target_class]\n",
    "\n",
    "    # Fit KNN on minority class to get density estimate\n",
    "    knn = NearestNeighbors(n_neighbors=min(k_neighbors, len(X_minority)))\n",
    "    knn.fit(X_minority)\n",
    "\n",
    "    # Calculate minority density: Number of neighbors within distance threshold\n",
    "    distances, _ = knn.kneighbors(X_minority)\n",
    "    density_scores = np.mean(distances, axis=1)  # Higher value → lower density\n",
    "\n",
    "    # Normalize density scores to get sampling weights (inverse relationship)\n",
    "    sampling_weights = (density_scores - density_scores.min()) / (density_scores.max() - density_scores.min() + 1e-6)\n",
    "    sampling_weights = sampling_weights / sampling_weights.sum()  # Normalize to sum=1\n",
    "\n",
    "    # Compute the number of samples to generate based on imbalance and density\n",
    "    target_samples = len(X_majority) - len(X_minority)\n",
    "    num_samples_per_point = (sampling_weights * target_samples).astype(int)\n",
    "\n",
    "    synthetic_samples = []\n",
    "\n",
    "    for idx, count in tqdm(enumerate(num_samples_per_point), total=len(num_samples_per_point)):\n",
    "        x_selected = X_minority.iloc[idx].values\n",
    "        \n",
    "        for _ in range(count):\n",
    "            # Find k-nearest neighbors\n",
    "            neighbors = knn.kneighbors([x_selected], return_distance=False)[0]\n",
    "\n",
    "            # Select a random neighbor\n",
    "            neighbor_idx = np.random.choice(neighbors[1:])  # Exclude itself\n",
    "            x_neighbor = X_minority.iloc[neighbor_idx].values\n",
    "\n",
    "            # Fit a 3rd-degree polynomial between x_selected and x_neighbor\n",
    "            t_values = np.array([0, 0.33, 0.66, 1])\n",
    "            x_values = np.vstack([x_selected, \n",
    "                                  (2*x_selected + x_neighbor)/3,\n",
    "                                  (x_selected + 2*x_neighbor)/3,\n",
    "                                  x_neighbor])\n",
    "            \n",
    "            # Generate synthetic sample\n",
    "            x_synthetic = np.zeros_like(x_selected)\n",
    "            t_random = np.random.rand()\n",
    "\n",
    "            for feature_idx in range(X.shape[1]):\n",
    "                poly = Polynomial.fit(t_values, x_values[:, feature_idx], 3)\n",
    "                x_synthetic[feature_idx] = poly(t_random)\n",
    "\n",
    "            synthetic_samples.append(x_synthetic)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    synthetic_samples_df = pd.DataFrame(synthetic_samples, columns=X.columns)\n",
    "\n",
    "    # Append synthetic samples\n",
    "    X_resampled = pd.concat([X, synthetic_samples_df], axis=0, ignore_index=True)\n",
    "    y_resampled = pd.concat([y, pd.Series(target_class, index=synthetic_samples_df.index)], axis=0, ignore_index=True)\n",
    "\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd6bf35-4e43-4ad6-be7f-faa2d6964fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>...</th>\n",
       "      <th>purpose_major_purchase</th>\n",
       "      <th>purpose_medical</th>\n",
       "      <th>purpose_moving</th>\n",
       "      <th>purpose_other</th>\n",
       "      <th>purpose_renewable_energy</th>\n",
       "      <th>purpose_small_business</th>\n",
       "      <th>purpose_vacation</th>\n",
       "      <th>purpose_wedding</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>10.65</td>\n",
       "      <td>162.87</td>\n",
       "      <td>10</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>860</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>15.27</td>\n",
       "      <td>59.83</td>\n",
       "      <td>0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>309</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>15.96</td>\n",
       "      <td>84.33</td>\n",
       "      <td>10</td>\n",
       "      <td>12252.0</td>\n",
       "      <td>606</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>13.49</td>\n",
       "      <td>339.31</td>\n",
       "      <td>10</td>\n",
       "      <td>49200.0</td>\n",
       "      <td>917</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>156.46</td>\n",
       "      <td>3</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>852</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>734.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39197</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>8.07</td>\n",
       "      <td>78.42</td>\n",
       "      <td>4</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>802</td>\n",
       "      <td>11.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>764.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39198</th>\n",
       "      <td>8500.0</td>\n",
       "      <td>10.28</td>\n",
       "      <td>275.38</td>\n",
       "      <td>3</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>274</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39199</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>8.07</td>\n",
       "      <td>156.84</td>\n",
       "      <td>0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>170</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39200</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>155.38</td>\n",
       "      <td>0</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>208</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39201</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>13.75</td>\n",
       "      <td>255.43</td>\n",
       "      <td>0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>270</td>\n",
       "      <td>14.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39202 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loan_amnt  int_rate  installment  emp_length  annual_inc  zip_code  \\\n",
       "0         5000.0     10.65       162.87          10     24000.0       860   \n",
       "1         2500.0     15.27        59.83           0     30000.0       309   \n",
       "2         2400.0     15.96        84.33          10     12252.0       606   \n",
       "3        10000.0     13.49       339.31          10     49200.0       917   \n",
       "4         5000.0      7.90       156.46           3     36000.0       852   \n",
       "...          ...       ...          ...         ...         ...       ...   \n",
       "39197     2500.0      8.07        78.42           4    110000.0       802   \n",
       "39198     8500.0     10.28       275.38           3     18000.0       274   \n",
       "39199     5000.0      8.07       156.84           0    100000.0       170   \n",
       "39200     5000.0      7.43       155.38           0    200000.0       208   \n",
       "39201     7500.0     13.75       255.43           0     22000.0       270   \n",
       "\n",
       "         dti  delinq_2yrs  fico_range_high  inq_last_6mths  ...  \\\n",
       "0      27.65          0.0            739.0             1.0  ...   \n",
       "1       1.00          0.0            744.0             5.0  ...   \n",
       "2       8.72          0.0            739.0             2.0  ...   \n",
       "3      20.00          0.0            694.0             1.0  ...   \n",
       "4      11.20          0.0            734.0             3.0  ...   \n",
       "...      ...          ...              ...             ...  ...   \n",
       "39197  11.33          0.0            764.0             0.0  ...   \n",
       "39198   6.40          1.0            694.0             1.0  ...   \n",
       "39199   2.30          0.0            744.0             0.0  ...   \n",
       "39200   3.72          0.0            814.0             0.0  ...   \n",
       "39201  14.29          1.0            664.0             0.0  ...   \n",
       "\n",
       "       purpose_major_purchase  purpose_medical  purpose_moving  purpose_other  \\\n",
       "0                         0.0              0.0             0.0            0.0   \n",
       "1                         0.0              0.0             0.0            0.0   \n",
       "2                         0.0              0.0             0.0            0.0   \n",
       "3                         0.0              0.0             0.0            1.0   \n",
       "4                         0.0              0.0             0.0            0.0   \n",
       "...                       ...              ...             ...            ...   \n",
       "39197                     0.0              0.0             0.0            0.0   \n",
       "39198                     0.0              0.0             0.0            0.0   \n",
       "39199                     0.0              0.0             0.0            0.0   \n",
       "39200                     0.0              0.0             0.0            1.0   \n",
       "39201                     0.0              0.0             0.0            0.0   \n",
       "\n",
       "       purpose_renewable_energy  purpose_small_business  purpose_vacation  \\\n",
       "0                           0.0                     0.0               0.0   \n",
       "1                           0.0                     0.0               0.0   \n",
       "2                           0.0                     1.0               0.0   \n",
       "3                           0.0                     0.0               0.0   \n",
       "4                           0.0                     0.0               0.0   \n",
       "...                         ...                     ...               ...   \n",
       "39197                       0.0                     0.0               0.0   \n",
       "39198                       0.0                     0.0               0.0   \n",
       "39199                       0.0                     0.0               0.0   \n",
       "39200                       0.0                     0.0               0.0   \n",
       "39201                       0.0                     0.0               0.0   \n",
       "\n",
       "       purpose_wedding  term_ 36 months  term_ 60 months  \n",
       "0                  0.0              1.0              0.0  \n",
       "1                  0.0              0.0              1.0  \n",
       "2                  0.0              1.0              0.0  \n",
       "3                  0.0              1.0              0.0  \n",
       "4                  1.0              1.0              0.0  \n",
       "...                ...              ...              ...  \n",
       "39197              0.0              1.0              0.0  \n",
       "39198              0.0              1.0              0.0  \n",
       "39199              0.0              1.0              0.0  \n",
       "39200              0.0              1.0              0.0  \n",
       "39201              0.0              1.0              0.0  \n",
       "\n",
       "[39202 rows x 40 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ec5987-715b-4359-bc43-b2a993ed547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def gaussian_based_smote_class_independent(X, y, k=5, sampling_ratio=1.0, regularization=1e-6):\n",
    "    \"\"\"\n",
    "    Gaussian-Based SMOTE with Class-Independent Sampling.\n",
    "\n",
    "    Parameters:\n",
    "        X (pandas.DataFrame): Feature matrix of shape (n_samples, n_features).\n",
    "        y (pandas.Series): Target labels of shape (n_samples,).\n",
    "        k (int): Number of nearest neighbors to consider for each minority sample.\n",
    "        sampling_ratio (float): Ratio of synthetic samples to generate relative to the imbalance.\n",
    "        regularization (float): Small value to add to the diagonal of the covariance matrix for regularization.\n",
    "\n",
    "    Returns:\n",
    "        X_balanced (pandas.DataFrame): Balanced feature matrix.\n",
    "        y_balanced (pandas.Series): Balanced target labels.\n",
    "    \"\"\"\n",
    "    cols=X.columns\n",
    "    # Convert X and y to numpy arrays for easier manipulation\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "\n",
    "    # Identify minority and majority classes\n",
    "    minority_class = 1\n",
    "    X_minority = X[y == minority_class]\n",
    "    X_majority = X[y != minority_class]\n",
    "    n_minority = X_minority.shape[0]\n",
    "    n_majority = X_majority.shape[0]\n",
    "    n_synthetic = int(sampling_ratio * (n_majority - n_minority))\n",
    "\n",
    "    # Find k-nearest neighbors for each minority sample (including itself)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X)  # +1 to include itself\n",
    "    _, indices = nbrs.kneighbors(X_minority)\n",
    "\n",
    "    synthetic_samples = []\n",
    "    for i in range(n_minority):\n",
    "        # Get the neighborhood of x_i (excluding itself)\n",
    "        neighborhood_indices = indices[i][1:]  # Exclude the first index (itself)\n",
    "        neighborhood = X[neighborhood_indices]\n",
    "\n",
    "        # Calculate the number of synthetic samples needed to balance the minority class\n",
    "        n_samples_needed = int(n_synthetic / n_minority)\n",
    "\n",
    "        # Fit a Gaussian distribution to the neighborhood (class-independent)\n",
    "        if len(neighborhood) > 1:  # Ensure there are enough samples to compute covariance\n",
    "            try:\n",
    "                # Compute mean and regularized covariance matrix\n",
    "                mu = np.mean(neighborhood, axis=0)\n",
    "                sigma = np.cov(neighborhood, rowvar=False) + regularization * np.eye(neighborhood.shape[1])\n",
    "\n",
    "                # Generate synthetic samples from the Gaussian distribution\n",
    "                samples = multivariate_normal.rvs(mean=mu, cov=sigma, size=n_samples_needed)\n",
    "                synthetic_samples.extend(samples)\n",
    "            except:\n",
    "                # Fallback to linear interpolation if Gaussian sampling fails\n",
    "                for _ in range(n_samples_needed):\n",
    "                    x1 = X_minority[i]  # Use the instance itself as x1\n",
    "                    x2 = neighborhood[np.random.choice(len(neighborhood))]  # Randomly select a neighbor\n",
    "                    lambda_ = np.random.uniform(0, 1)\n",
    "                    x_new = x1 + lambda_ * (x2 - x1)\n",
    "                    synthetic_samples.append(x_new)\n",
    "        else:\n",
    "            # Fallback to linear interpolation if there are too few samples in the neighborhood\n",
    "            for _ in range(n_samples_needed):\n",
    "                x1 = X_minority[i]  # Use the instance itself as x1\n",
    "                x2 = neighborhood[np.random.choice(len(neighborhood))]  # Randomly select a neighbor\n",
    "                lambda_ = np.random.uniform(0, 1)\n",
    "                x_new = x1 + lambda_ * (x2 - x1)\n",
    "                synthetic_samples.append(x_new)\n",
    "\n",
    "    # Combine synthetic samples with the original data\n",
    "    synthetic_samples = np.array(synthetic_samples)\n",
    "    X_balanced = np.vstack([X, synthetic_samples])\n",
    "    y_balanced = np.hstack([y, np.full(len(synthetic_samples), minority_class)])\n",
    "\n",
    "    # Convert back to pandas DataFrame and Series\n",
    "    X_balanced = pd.DataFrame(X_balanced, columns=cols)\n",
    "    y_balanced = pd.Series(y_balanced)\n",
    "\n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1fd0623-2ab4-45e6-9c60-2b975ab59d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=gaussian_based_smote_class_independent(X,y['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d1d1a6f-a018-41d8-bc46-6406a37a2857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>689.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>689.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2682.000000</td>\n",
       "      <td>3272.000000</td>\n",
       "      <td>3455.000000</td>\n",
       "      <td>3261.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13559.000000</td>\n",
       "      <td>14331.000000</td>\n",
       "      <td>14948.000000</td>\n",
       "      <td>15549.000000</td>\n",
       "      <td>1518.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>49291.000000</td>\n",
       "      <td>28314.000000</td>\n",
       "      <td>28959.000000</td>\n",
       "      <td>29547.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2019.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>1069.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>35835.000000</td>\n",
       "      <td>20940.000000</td>\n",
       "      <td>19146.000000</td>\n",
       "      <td>19131.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>36681.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>9000.000000</td>\n",
       "      <td>689.000000</td>\n",
       "      <td>679.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43267</th>\n",
       "      <td>19999.997754</td>\n",
       "      <td>1.590705</td>\n",
       "      <td>0.690469</td>\n",
       "      <td>2.000980</td>\n",
       "      <td>20.140812</td>\n",
       "      <td>-0.427931</td>\n",
       "      <td>-1.767432</td>\n",
       "      <td>-2.346084</td>\n",
       "      <td>-1.717785</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-2343.358514</td>\n",
       "      <td>18933.902872</td>\n",
       "      <td>19595.052821</td>\n",
       "      <td>19538.304847</td>\n",
       "      <td>4988.422335</td>\n",
       "      <td>-546.119572</td>\n",
       "      <td>23810.272459</td>\n",
       "      <td>148.529457</td>\n",
       "      <td>294.484036</td>\n",
       "      <td>293.191282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43268</th>\n",
       "      <td>86971.859065</td>\n",
       "      <td>1.353860</td>\n",
       "      <td>1.708047</td>\n",
       "      <td>1.999114</td>\n",
       "      <td>26.868859</td>\n",
       "      <td>-0.337955</td>\n",
       "      <td>-0.693410</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>-0.000434</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>...</td>\n",
       "      <td>67707.924713</td>\n",
       "      <td>60457.667099</td>\n",
       "      <td>49660.394272</td>\n",
       "      <td>55211.826387</td>\n",
       "      <td>67551.083507</td>\n",
       "      <td>3381.324091</td>\n",
       "      <td>3472.497479</td>\n",
       "      <td>3162.551651</td>\n",
       "      <td>9440.956293</td>\n",
       "      <td>3334.792868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43269</th>\n",
       "      <td>70410.304003</td>\n",
       "      <td>1.968966</td>\n",
       "      <td>1.489951</td>\n",
       "      <td>1.999000</td>\n",
       "      <td>22.726311</td>\n",
       "      <td>0.363263</td>\n",
       "      <td>-0.605962</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>...</td>\n",
       "      <td>51855.362798</td>\n",
       "      <td>46788.393379</td>\n",
       "      <td>38264.989374</td>\n",
       "      <td>21628.928316</td>\n",
       "      <td>62030.914811</td>\n",
       "      <td>1766.358053</td>\n",
       "      <td>1695.613379</td>\n",
       "      <td>1312.018989</td>\n",
       "      <td>-2237.339539</td>\n",
       "      <td>817.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43270</th>\n",
       "      <td>50000.000468</td>\n",
       "      <td>2.210320</td>\n",
       "      <td>1.609737</td>\n",
       "      <td>2.207398</td>\n",
       "      <td>15.204226</td>\n",
       "      <td>-0.415965</td>\n",
       "      <td>-0.415813</td>\n",
       "      <td>-0.417778</td>\n",
       "      <td>-0.417467</td>\n",
       "      <td>-0.417081</td>\n",
       "      <td>...</td>\n",
       "      <td>43883.285731</td>\n",
       "      <td>39913.173032</td>\n",
       "      <td>30423.695417</td>\n",
       "      <td>22435.766820</td>\n",
       "      <td>3154.384491</td>\n",
       "      <td>2908.568465</td>\n",
       "      <td>1742.417325</td>\n",
       "      <td>2017.077081</td>\n",
       "      <td>-87.138071</td>\n",
       "      <td>1511.930330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43271</th>\n",
       "      <td>50000.000058</td>\n",
       "      <td>2.380305</td>\n",
       "      <td>1.327028</td>\n",
       "      <td>2.378795</td>\n",
       "      <td>23.690609</td>\n",
       "      <td>-0.761981</td>\n",
       "      <td>-0.762010</td>\n",
       "      <td>-0.759399</td>\n",
       "      <td>-0.760162</td>\n",
       "      <td>-0.761343</td>\n",
       "      <td>...</td>\n",
       "      <td>44595.074609</td>\n",
       "      <td>37111.223203</td>\n",
       "      <td>24968.271150</td>\n",
       "      <td>19738.721890</td>\n",
       "      <td>1959.063768</td>\n",
       "      <td>1735.991095</td>\n",
       "      <td>1601.644107</td>\n",
       "      <td>810.765877</td>\n",
       "      <td>2313.869126</td>\n",
       "      <td>65.191401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43272 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  X1        X2        X3        X4         X5        X6  \\\n",
       "0       20000.000000  2.000000  2.000000  1.000000  24.000000  2.000000   \n",
       "1      120000.000000  2.000000  2.000000  2.000000  26.000000 -1.000000   \n",
       "2       90000.000000  2.000000  2.000000  2.000000  34.000000  0.000000   \n",
       "3       50000.000000  2.000000  2.000000  1.000000  37.000000  0.000000   \n",
       "4       50000.000000  1.000000  2.000000  1.000000  57.000000 -1.000000   \n",
       "...              ...       ...       ...       ...        ...       ...   \n",
       "43267   19999.997754  1.590705  0.690469  2.000980  20.140812 -0.427931   \n",
       "43268   86971.859065  1.353860  1.708047  1.999114  26.868859 -0.337955   \n",
       "43269   70410.304003  1.968966  1.489951  1.999000  22.726311  0.363263   \n",
       "43270   50000.000468  2.210320  1.609737  2.207398  15.204226 -0.415965   \n",
       "43271   50000.000058  2.380305  1.327028  2.378795  23.690609 -0.761981   \n",
       "\n",
       "             X7        X8        X9       X10  ...           X14  \\\n",
       "0      2.000000 -1.000000 -1.000000 -2.000000  ...    689.000000   \n",
       "1      2.000000  0.000000  0.000000  0.000000  ...   2682.000000   \n",
       "2      0.000000  0.000000  0.000000  0.000000  ...  13559.000000   \n",
       "3      0.000000  0.000000  0.000000  0.000000  ...  49291.000000   \n",
       "4      0.000000 -1.000000  0.000000  0.000000  ...  35835.000000   \n",
       "...         ...       ...       ...       ...  ...           ...   \n",
       "43267 -1.767432 -2.346084 -1.717785  0.000014  ...  -2343.358514   \n",
       "43268 -0.693410 -0.000248 -0.000434  0.000522  ...  67707.924713   \n",
       "43269 -0.605962  0.000176 -0.000506  0.000807  ...  51855.362798   \n",
       "43270 -0.415813 -0.417778 -0.417467 -0.417081  ...  43883.285731   \n",
       "43271 -0.762010 -0.759399 -0.760162 -0.761343  ...  44595.074609   \n",
       "\n",
       "                X15           X16           X17           X18           X19  \\\n",
       "0          0.000000      0.000000      0.000000      0.000000    689.000000   \n",
       "1       3272.000000   3455.000000   3261.000000      0.000000   1000.000000   \n",
       "2      14331.000000  14948.000000  15549.000000   1518.000000   1500.000000   \n",
       "3      28314.000000  28959.000000  29547.000000   2000.000000   2019.000000   \n",
       "4      20940.000000  19146.000000  19131.000000   2000.000000  36681.000000   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "43267  18933.902872  19595.052821  19538.304847   4988.422335   -546.119572   \n",
       "43268  60457.667099  49660.394272  55211.826387  67551.083507   3381.324091   \n",
       "43269  46788.393379  38264.989374  21628.928316  62030.914811   1766.358053   \n",
       "43270  39913.173032  30423.695417  22435.766820   3154.384491   2908.568465   \n",
       "43271  37111.223203  24968.271150  19738.721890   1959.063768   1735.991095   \n",
       "\n",
       "                X20          X21          X22          X23  \n",
       "0          0.000000     0.000000     0.000000     0.000000  \n",
       "1       1000.000000  1000.000000     0.000000  2000.000000  \n",
       "2       1000.000000  1000.000000  1000.000000  5000.000000  \n",
       "3       1200.000000  1100.000000  1069.000000  1000.000000  \n",
       "4      10000.000000  9000.000000   689.000000   679.000000  \n",
       "...             ...          ...          ...          ...  \n",
       "43267  23810.272459   148.529457   294.484036   293.191282  \n",
       "43268   3472.497479  3162.551651  9440.956293  3334.792868  \n",
       "43269   1695.613379  1312.018989 -2237.339539   817.783333  \n",
       "43270   1742.417325  2017.077081   -87.138071  1511.930330  \n",
       "43271   1601.644107   810.765877  2313.869126    65.191401  \n",
       "\n",
       "[43272 rows x 23 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "822e7477-6790-4fd6-9b10-a0f16f1ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def gaussian_based_smote_minority_neighborhood(X, y, k=10, sampling_ratio=1.0, regularization=1e-6):\n",
    "    \"\"\"\n",
    "    Gaussian-Based SMOTE with Sampling from Minority Class in Neighborhood.\n",
    "\n",
    "    Parameters:\n",
    "        X (pandas.DataFrame): Feature matrix of shape (n_samples, n_features).\n",
    "        y (pandas.Series): Target labels of shape (n_samples,).\n",
    "        k (int): Number of nearest neighbors to consider for each minority sample.\n",
    "        sampling_ratio (float): Ratio of synthetic samples to generate relative to the imbalance.\n",
    "        regularization (float): Small value to add to the diagonal of the covariance matrix for regularization.\n",
    "\n",
    "    Returns:\n",
    "        X_balanced (pandas.DataFrame): Balanced feature matrix.\n",
    "        y_balanced (pandas.Series): Balanced target labels.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    cols=X.columns\n",
    "\n",
    "    # Convert X and y to numpy arrays for easier manipulation\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "\n",
    "    # Identify minority and majority classes\n",
    "    minority_class = np.argmin(np.bincount(y))\n",
    "    X_minority = X[y == minority_class]\n",
    "    X_majority = X[y != minority_class]\n",
    "    n_minority = X_minority.shape[0]\n",
    "    n_majority = X_majority.shape[0]\n",
    "    n_synthetic = int(sampling_ratio * (n_majority - n_minority))\n",
    "\n",
    "    # Find k-nearest neighbors for each minority sample (including itself)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X)  # +1 to include itself\n",
    "    _, indices = nbrs.kneighbors(X_minority)\n",
    "\n",
    "    synthetic_samples = []\n",
    "    for i in range(n_minority):\n",
    "        # Get the neighborhood of x_i (excluding itself)\n",
    "        neighborhood_indices = indices[i][1:]  # Exclude the first index (itself)\n",
    "        neighborhood = X[neighborhood_indices]\n",
    "        neighborhood_labels = y[neighborhood_indices]\n",
    "\n",
    "        # Filter minority class instances in the neighborhood\n",
    "        minority_neighborhood = neighborhood[neighborhood_labels == minority_class]\n",
    "\n",
    "        # Skip if there are no minority instances in the neighborhood\n",
    "        if len(minority_neighborhood) == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate the number of synthetic samples needed to balance the minority class\n",
    "        n_samples_needed = int(n_synthetic / n_minority)\n",
    "\n",
    "        # Fit a Gaussian distribution to the minority neighborhood\n",
    "        if len(minority_neighborhood) > 1:  # Ensure there are enough samples to compute covariance\n",
    "            try:\n",
    "                # Compute mean and regularized covariance matrix\n",
    "                mu = np.mean(minority_neighborhood, axis=0)\n",
    "                sigma = np.cov(minority_neighborhood, rowvar=False) + regularization * np.eye(minority_neighborhood.shape[1])\n",
    "\n",
    "                # Generate synthetic samples from the Gaussian distribution\n",
    "                samples = multivariate_normal.rvs(mean=mu, cov=sigma, size=n_samples_needed)\n",
    "                synthetic_samples.extend(samples)\n",
    "            except:\n",
    "                # Fallback to linear interpolation if Gaussian sampling fails\n",
    "                for _ in range(n_samples_needed):\n",
    "                    x1 = X_minority[i]  # Use the instance itself as x1\n",
    "                    x2 = minority_neighborhood[np.random.choice(len(minority_neighborhood))]  # Randomly select a minority neighbor\n",
    "                    lambda_ = np.random.uniform(0, 1)\n",
    "                    x_new = x1 + lambda_ * (x2 - x1)\n",
    "                    synthetic_samples.append(x_new)\n",
    "        else:\n",
    "            # Fallback to linear interpolation if there are too few minority samples in the neighborhood\n",
    "            for _ in range(n_samples_needed):\n",
    "                x1 = X_minority[i]  # Use the instance itself as x1\n",
    "                x2 = minority_neighborhood[np.random.choice(len(minority_neighborhood))]  # Randomly select a minority neighbor\n",
    "                lambda_ = np.random.uniform(0, 1)\n",
    "                x_new = x1 + lambda_ * (x2 - x1)\n",
    "                synthetic_samples.append(x_new)\n",
    "\n",
    "    # Combine synthetic samples with the original data\n",
    "    synthetic_samples = np.array(synthetic_samples)\n",
    "    X_balanced = np.vstack([X, synthetic_samples])\n",
    "    y_balanced = np.hstack([y, np.full(len(synthetic_samples), minority_class)])\n",
    "\n",
    "    # Convert back to pandas DataFrame and Series\n",
    "    X_balanced = pd.DataFrame(X_balanced, columns=cols)\n",
    "    y_balanced = pd.Series(y_balanced)\n",
    "\n",
    "    return X_balanced, y_balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109d341-1b46-450e-a3ba-da6c169f609c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd722b8-1172-44c4-a074-585b85e63911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f750f-2a74-45c9-9eb4-e1e2cfb55e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a0c80-c4e7-4705-bd19-a937a78d9052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6209b-4049-4c38-8003-72910d0ffb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606047a-095f-4a52-a8e8-72cf1fe8545a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9ddc3-e2ab-42b1-a67b-739ab73884ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feed0d1-494d-47bc-88b0-d2595e2eb322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6114d0-1427-405b-a808-a8ddcce2ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b3ba9-a456-415c-9514-99e4abca8870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e56b74-914f-4d19-b53a-2ddc845ba81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963dd59f-60a3-4d72-9aa2-023dbd2dc097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd72180-7710-4afd-8c2f-7a8009b8411a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
