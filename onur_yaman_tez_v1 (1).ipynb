{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827ae26b-71b4-427a-88d4-c1da5ea652d0",
   "metadata": {},
   "source": [
    "##  SMOTE (Synthetic Minority Over-sampling Technique) \n",
    "\n",
    "## **Introduction**\n",
    "SMOTE is a resampling technique used to handle class imbalance by generating synthetic samples for the minority class instead of simply duplicating existing ones. It works by interpolating between real minority class instances.\n",
    "\n",
    "## **Algorithm Steps**\n",
    "1. **Select a minority class sample** $x_i$ from the dataset.\n",
    "2. **Find its k-nearest neighbors** in the minority class using Euclidean distance.\n",
    "3. **Randomly select one of these neighbors** $x_{nn}$.\n",
    "4. **Generate a synthetic sample** along the line segment joining $x_i$ and $x_{nn}$ using interpolation:\n",
    "\n",
    "   $$\n",
    "   x_{\\text{new}} = x_i + \\lambda \\cdot (x_{nn} - x_i)\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   $$\n",
    "   \\lambda \\sim U(0,1)\n",
    "   $$\n",
    "\n",
    "   is a random number between 0 and 1.\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "For a given minority class instance $x_i$, let $x_{nn}$ be one of its k-nearest neighbors. The synthetic sample is created as:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_i + \\lambda (x_{nn} - x_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i$ is a real minority class instance.\n",
    "- $x_{nn}$ is one of its k-nearest neighbors.\n",
    "- $\\lambda$ is a random number sampled from a uniform distribution:\n",
    "\n",
    "  $$\n",
    "  \\lambda \\sim U(0,1)\n",
    "  $$\n",
    "\n",
    "This process is repeated until the desired number of synthetic samples is generated.\n",
    "\n",
    "## **Advantages of SMOTE**\n",
    "- Reduces class imbalance by adding synthetic samples.\n",
    "- Prevents overfitting caused by simple duplication of minority class instances.\n",
    "- Preserves the relationships between data points.\n",
    "\n",
    "## **Limitations of SMOTE**\n",
    "- Can generate noisy samples if the minority class has a complex distribution.\n",
    "- Does not consider the majority class, which may lead to overlapping regions and potential misclassification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ebd57d-d8c2-4a57-8679-a306763d9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score, recall_score, accuracy_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # Ignore all warnings\n",
    "\n",
    "def calculate_metrics(y_test, y_pred_proba):\n",
    "    threshold = np.mean(y_pred_proba)  # Dynamic threshold based on mean\n",
    "    y_pred_binary = [1 if p > 0.5 else 0 for p in y_pred_proba]  # Convert probabilities to binary\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    recall = recall_score(y_test, y_pred_binary)\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "    \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    g_mean = np.sqrt(recall * specificity)\n",
    "\n",
    "    return accuracy, roc_auc, pr_auc, recall, f1, specificity, fp_rate, g_mean\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "\n",
    "def objective(trial, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Objective function for Optuna to optimize hyperparameters.\"\"\"\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 1),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1),\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=200)\n",
    "    y_pred_proba = model.predict(dtest)\n",
    "    return roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "def model(X, y):\n",
    "    # Train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    ### Hyperparameter Optimization for Original Data (No SMOTE)\n",
    "    study_no_smote = optuna.create_study(direction=\"maximize\")\n",
    "    study_no_smote.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=50)\n",
    "\n",
    "    # Best hyperparameters for original data\n",
    "    best_params_no_smote = study_no_smote.best_params\n",
    "    print(\"Best Hyperparameters (No SMOTE):\", best_params_no_smote)\n",
    "\n",
    "    ### Train XGBoost WITHOUT SMOTE using best hyperparameters\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params_no_smote = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    params_no_smote.update(best_params_no_smote)  # Add best hyperparameters\n",
    "\n",
    "    model_no_smote = xgb.train(params_no_smote, dtrain, num_boost_round=200)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_proba_no_smote = model_no_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_no_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_no_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_no_smote]\n",
    "\n",
    "    metrics_no_smote = calculate_metrics(y_test, y_pred_proba_no_smote)\n",
    "\n",
    "    ### Apply SMOTE\n",
    "    smote = SMOTE(sampling_strategy=1, random_state=42)  # Fully balance classes\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    ### Hyperparameter Optimization for Resampled Data (With SMOTE)\n",
    "    study_with_smote = optuna.create_study(direction=\"maximize\")\n",
    "    study_with_smote.optimize(lambda trial: objective(trial, X_train_resampled, y_train_resampled, X_test, y_test), n_trials=50)\n",
    "\n",
    "    # Best hyperparameters for resampled data\n",
    "    best_params_with_smote = study_with_smote.best_params\n",
    "    print(\"Best Hyperparameters (With SMOTE):\", best_params_with_smote)\n",
    "\n",
    "    ### Train XGBoost WITH SMOTE using best hyperparameters\n",
    "    dtrain_resampled = xgb.DMatrix(X_train_resampled, label=y_train_resampled)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params_with_smote = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    params_with_smote.update(best_params_with_smote)  # Add best hyperparameters\n",
    "\n",
    "    model_with_smote = xgb.train(params_with_smote, dtrain_resampled, num_boost_round=200)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_proba_with_smote = model_with_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_with_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_with_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_with_smote]\n",
    "\n",
    "    metrics_with_smote = calculate_metrics(y_test, y_pred_proba_with_smote)\n",
    "\n",
    "    # Print comparison\n",
    "    metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(\"{:<20} {:<10} {:<10}\".format(\"Metric\", \"No SMOTE\", \"With Regular SMOTE\"))\n",
    "    for name, no_smote, with_smote in zip(metric_names, metrics_no_smote, metrics_with_smote):\n",
    "        print(f\"{name:<20} {no_smote:.4f}   {with_smote:.4f}\")\n",
    "\n",
    "    return [metrics_no_smote, metrics_with_smote]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57d1ad5-dd31-4745-a7ea-ee522fe6e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy=1, random_state=42)  # Fully balance classes\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90f6db7c-1764-4fa6-a642-09e000c50ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19908/19908 [01:42<00:00, 193.53it/s]\n"
     ]
    }
   ],
   "source": [
    "X_res,_y_res=custom_smote_with_cubic_interpolation(X,y['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "875527bf-0c4f-4dc7-ac2d-86d92079bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79c09ff5-dab0-4074-8813-49c6edc6758d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11581</td>\n",
       "      <td>12580</td>\n",
       "      <td>13716</td>\n",
       "      <td>14828</td>\n",
       "      <td>1500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>150000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>116684</td>\n",
       "      <td>101581</td>\n",
       "      <td>77741</td>\n",
       "      <td>77264</td>\n",
       "      <td>4486</td>\n",
       "      <td>4235</td>\n",
       "      <td>3161</td>\n",
       "      <td>2647</td>\n",
       "      <td>2669</td>\n",
       "      <td>2669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23397</th>\n",
       "      <td>70000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>68530</td>\n",
       "      <td>69753</td>\n",
       "      <td>70111</td>\n",
       "      <td>70212</td>\n",
       "      <td>2431</td>\n",
       "      <td>3112</td>\n",
       "      <td>3000</td>\n",
       "      <td>2438</td>\n",
       "      <td>2500</td>\n",
       "      <td>2554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25058</th>\n",
       "      <td>130000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16172</td>\n",
       "      <td>16898</td>\n",
       "      <td>11236</td>\n",
       "      <td>6944</td>\n",
       "      <td>1610</td>\n",
       "      <td>1808</td>\n",
       "      <td>7014</td>\n",
       "      <td>27</td>\n",
       "      <td>7011</td>\n",
       "      <td>4408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>42361</td>\n",
       "      <td>19574</td>\n",
       "      <td>20295</td>\n",
       "      <td>19439</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3941</th>\n",
       "      <td>410000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>666</td>\n",
       "      <td>13621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17854</th>\n",
       "      <td>210000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>45622</td>\n",
       "      <td>47232</td>\n",
       "      <td>47583</td>\n",
       "      <td>53032</td>\n",
       "      <td>8000</td>\n",
       "      <td>5000</td>\n",
       "      <td>4000</td>\n",
       "      <td>3000</td>\n",
       "      <td>8000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>90000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>87653</td>\n",
       "      <td>35565</td>\n",
       "      <td>30942</td>\n",
       "      <td>30835</td>\n",
       "      <td>3621</td>\n",
       "      <td>3597</td>\n",
       "      <td>1179</td>\n",
       "      <td>1112</td>\n",
       "      <td>1104</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6279</th>\n",
       "      <td>220000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>142295</td>\n",
       "      <td>145127</td>\n",
       "      <td>148159</td>\n",
       "      <td>151462</td>\n",
       "      <td>5100</td>\n",
       "      <td>5163</td>\n",
       "      <td>5196</td>\n",
       "      <td>5372</td>\n",
       "      <td>5761</td>\n",
       "      <td>5396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26428</th>\n",
       "      <td>250000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>6560</td>\n",
       "      <td>4438</td>\n",
       "      <td>4309</td>\n",
       "      <td>816</td>\n",
       "      <td>3978</td>\n",
       "      <td>6560</td>\n",
       "      <td>4438</td>\n",
       "      <td>4311</td>\n",
       "      <td>816</td>\n",
       "      <td>9165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1  X2  X3  X4  X5  X6  X7  X8  X9  X10  ...     X14     X15  \\\n",
       "2308    30000   1   2   2  25   0   0   0   0    0  ...   11581   12580   \n",
       "22404  150000   2   1   2  26   0   0   0   0    0  ...  116684  101581   \n",
       "23397   70000   2   3   1  32   0   0   0   0    0  ...   68530   69753   \n",
       "25058  130000   1   3   2  49   0   0   0   0    0  ...   16172   16898   \n",
       "2664    50000   2   2   2  36   0   0   0   0    0  ...   42361   19574   \n",
       "...       ...  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...     ...     ...   \n",
       "3941   410000   2   1   2  34   1  -1  -1  -2   -2  ...       0       0   \n",
       "17854  210000   1   1   2  27   0   0   0   0    0  ...   45622   47232   \n",
       "95      90000   1   2   2  35   0   0   0   0    0  ...   87653   35565   \n",
       "6279   220000   2   2   1  36   0   0   0   0    0  ...  142295  145127   \n",
       "26428  250000   2   1   1  36  -2  -2  -2  -2   -2  ...    6560    4438   \n",
       "\n",
       "          X16     X17    X18   X19   X20   X21   X22   X23  \n",
       "2308    13716   14828   1500  2000  1500  1500  1500  2000  \n",
       "22404   77741   77264   4486  4235  3161  2647  2669  2669  \n",
       "23397   70111   70212   2431  3112  3000  2438  2500  2554  \n",
       "25058   11236    6944   1610  1808  7014    27  7011  4408  \n",
       "2664    20295   19439   2000  1500  1000  1800     0  1000  \n",
       "...       ...     ...    ...   ...   ...   ...   ...   ...  \n",
       "3941        0     666  13621     0     0     0   666     0  \n",
       "17854   47583   53032   8000  5000  4000  3000  8000  3000  \n",
       "95      30942   30835   3621  3597  1179  1112  1104  1143  \n",
       "6279   148159  151462   5100  5163  5196  5372  5761  5396  \n",
       "26428    4309     816   3978  6560  4438  4311   816  9165  \n",
       "\n",
       "[9000 rows x 23 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175468c-cf8f-4276-af52-3ee901ee01da",
   "metadata": {},
   "source": [
    "# **Custom SMOTE with Cubic Interpolation (My Development)**\n",
    "\n",
    "## **Introduction**\n",
    "This is a modified version of the **Synthetic Minority Over-sampling Technique (SMOTE)**, where instead of linear interpolation, a **third-degree polynomial interpolation** is used to generate synthetic samples. This method helps preserve complex feature relationships and avoids overly simplistic synthetic samples.\n",
    "\n",
    "## **Algorithm Steps**\n",
    "1. **Identify the minority class** in the dataset.\n",
    "2. **Find its k-nearest neighbors** using Euclidean distance.\n",
    "3. **Randomly select one of these neighbors** $x_{nn}$ for interpolation.\n",
    "4. **Use cubic interpolation** between the selected sample $x_i$ and its neighbor $x_{nn}$:\n",
    "   - Define reference points between $x_i$ and $x_{nn}$.\n",
    "   - Fit a third-degree polynomial for each feature.\n",
    "   - Sample a new synthetic point using the polynomial.\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "For a given minority class instance $x_i$, let $x_{nn}$ be one of its k-nearest neighbors. We define four reference points:\n",
    "\n",
    "$$\n",
    "x_0 = x_i, \\quad x_1 = \\frac{2x_i + x_{nn}}{3}, \\quad x_2 = \\frac{x_i + 2x_{nn}}{3}, \\quad x_3 = x_{nn}\n",
    "$$\n",
    "\n",
    "These points correspond to $t$-values:\n",
    "\n",
    "$$\n",
    "t_0 = 0, \\quad t_1 = 0.33, \\quad t_2 = 0.66, \\quad t_3 = 1\n",
    "$$\n",
    "\n",
    "A third-degree polynomial is fitted for each feature using these values:\n",
    "\n",
    "$$\n",
    "P(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3\n",
    "$$\n",
    "\n",
    "where the coefficients $(a_0, a_1, a_2, a_3)$ are determined by the reference points. A synthetic sample is generated by evaluating the polynomial at a randomly chosen $t_{\\text{rand}} \\sim U(0,1)$:\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = P(t_{\\text{rand}})\n",
    "$$\n",
    "\n",
    "This process is repeated until the desired number of synthetic samples is generated.\n",
    "\n",
    "## **Advantages of Custom SMOTE with Cubic Interpolation**\n",
    "- **More realistic synthetic samples**: Cubic interpolation provides a **smoother transition** between real data points.\n",
    "- **Better feature relationships**: Unlike linear SMOTE, this method **captures non-linear patterns** in the data.\n",
    "- **Less risk of generating outliers**: Intermediate points help **constrain synthetic samples** within a reasonable range.\n",
    "\n",
    "## **Limitations**\n",
    "- **Computationally expensive**: Fitting a polynomial for each feature requires more computation than linear interpolation.\n",
    "- **Risk of overfitting**: If the minority class has a complex distribution, the interpolation might introduce synthetic samples that do not generalize well.\n",
    "- **Sensitive to noisy data**: If the minority class contains outliers, the interpolation may exaggerate these variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1054f122-610d-4676-9486-f6e44ca57da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "def custom_smote_with_cubic_interpolation(X: pd.DataFrame, y: pd.Series, target_class=1, k_neighbors=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Custom SMOTE using 3rd-degree polynomial interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series): Target labels.\n",
    "        target_class (int): The minority class to oversample.\n",
    "        k_neighbors (int): Number of nearest neighbors to consider.\n",
    "        sampling_ratio (float): Ratio of synthetic samples to generate relative to minority class.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        X_resampled (pd.DataFrame): New feature matrix with synthetic samples.\n",
    "        y_resampled (pd.Series): Updated target labels.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Ensure `y` is a 1D array\n",
    "    y = y.reset_index(drop=True)  # Ensure proper indexing\n",
    "    \n",
    "    # Separate minority class\n",
    "    X_minority = X[y == target_class]\n",
    "    sampling_ratio=np.floor((X.shape[0]-X_minority.shape[0])/X_minority.shape[0])\n",
    "    \n",
    "    # Fit KNN on minority class\n",
    "    knn = NearestNeighbors(n_neighbors=min(k_neighbors, len(X_minority)))\n",
    "    knn.fit(X_minority)\n",
    "    \n",
    "    # Determine number of synthetic samples to generate\n",
    "    n_samples = int(len(X_minority) * sampling_ratio)\n",
    "\n",
    "    print(n_samples)\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    \n",
    "    for _ in tqdm(range(n_samples)):\n",
    "        # Randomly select a minority sample\n",
    "        idx = np.random.randint(0, len(X_minority))\n",
    "        x_selected = X_minority.iloc[idx].values  # Convert to NumPy array\n",
    "        \n",
    "        # Find k-nearest neighbors\n",
    "        neighbors = knn.kneighbors([x_selected], return_distance=False)[0]\n",
    "        \n",
    "        # Select a random neighbor\n",
    "        neighbor_idx = np.random.choice(neighbors[1:])  # Exclude itself\n",
    "        x_neighbor = X_minority.iloc[neighbor_idx].values  # Convert to NumPy array\n",
    "        \n",
    "        # Fit a 3rd-degree polynomial between x_selected and x_neighbor\n",
    "        t_values = np.array([0, 0.33, 0.66, 1])  # 4 reference points in [0,1]\n",
    "        x_values = np.vstack([x_selected, \n",
    "                              (2*x_selected + x_neighbor)/3, \n",
    "                              (x_selected + 2*x_neighbor)/3, \n",
    "                              x_neighbor])  # Intermediate points\n",
    "        \n",
    "        # Generate polynomial coefficients for each feature\n",
    "        x_synthetic = np.zeros_like(x_selected)\n",
    "        t_random = np.random.rand()  # Random t in [0,1]\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):  # Iterate over all features\n",
    "            poly = Polynomial.fit(t_values, x_values[:, feature_idx], 3)  # Fit cubic polynomial\n",
    "            x_synthetic[feature_idx] = poly(t_random)  # Sample new point\n",
    "        \n",
    "        synthetic_samples.append(x_synthetic)\n",
    "    \n",
    "    # Convert synthetic samples to DataFrame\n",
    "    synthetic_samples_df = pd.DataFrame(synthetic_samples, columns=X.columns)\n",
    "    \n",
    "    # Create new dataset (append synthetic data)\n",
    "    X_resampled = pd.concat([X, synthetic_samples_df], axis=0, ignore_index=True)\n",
    "    y_resampled = pd.concat([y, pd.Series(target_class, index=synthetic_samples_df.index)], axis=0, ignore_index=True)\n",
    "    \n",
    "    return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8426cb9-68bb-4304-bf08-65d4c24a1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_smote_poly(X, y):\n",
    "    # Train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    ### Hyperparameter Optimization for Original Data (No SMOTE)\n",
    "    study_no_smote = optuna.create_study(direction=\"maximize\")\n",
    "    study_no_smote.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=50)\n",
    "\n",
    "    # Best hyperparameters for original data\n",
    "    best_params_no_smote = study_no_smote.best_params\n",
    "    print(\"Best Hyperparameters (No SMOTE):\", best_params_no_smote)\n",
    "\n",
    "    ### Train XGBoost WITHOUT SMOTE using best hyperparameters\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params_no_smote = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    params_no_smote.update(best_params_no_smote)  # Add best hyperparameters\n",
    "\n",
    "    model_no_smote = xgb.train(params_no_smote, dtrain, num_boost_round=200)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_proba_no_smote = model_no_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_no_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_no_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_no_smote]\n",
    "\n",
    "    metrics_no_smote = calculate_metrics(y_test, y_pred_proba_no_smote)\n",
    "\n",
    "    ### Apply Custom SMOTE with Cubic Polynomial Interpolation\n",
    "    X_train_resampled, y_train_resampled = custom_smote_with_cubic_interpolation(pd.DataFrame(X_train, columns=X.columns), y_train)\n",
    "\n",
    "    ### Hyperparameter Optimization for Resampled Data (With Cubic Polynomial SMOTE)\n",
    "    study_with_smote = optuna.create_study(direction=\"maximize\")\n",
    "    study_with_smote.optimize(lambda trial: objective(trial, np.array(X_train_resampled), y_train_resampled, X_test, y_test), n_trials=50)\n",
    "\n",
    "    # Best hyperparameters for resampled data\n",
    "\n",
    "    \n",
    "    best_params_with_smote = study_with_smote.best_params\n",
    "\n",
    "    params_with_smote = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    params_with_smote.update(best_params_with_smote)  # Add best hyperparameters\n",
    "    print(\"Best Hyperparameters (With Cubic Polynomial SMOTE):\", best_params_with_smote)\n",
    "\n",
    "    ### Train XGBoost WITH SMOTE using best hyperparameters\n",
    "    dtrain_resampled = xgb.DMatrix(np.array(X_train_resampled), label=y_train_resampled)\n",
    "    model_with_smote = xgb.train(params_with_smote, dtrain_resampled, num_boost_round=200)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_proba_with_smote = model_with_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_with_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_with_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_with_smote]\n",
    "\n",
    "    metrics_with_smote = calculate_metrics(y_test, y_pred_proba_with_smote)\n",
    "\n",
    "    # Print comparison\n",
    "    metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(\"{:<20} {:<10} {:<10}\".format(\"Metric\", \"No SMOTE\", \"With Cubic Polynomial SMOTE\"))\n",
    "    for name, no_smote, with_smote in zip(metric_names, metrics_no_smote, metrics_with_smote):\n",
    "        print(f\"{name:<20} {no_smote:.4f}   {with_smote:.4f}\")\n",
    "\n",
    "    return [metrics_no_smote, metrics_with_smote]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fe782-096d-4ff4-ad5b-10b1987f275c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **Model Performance Metrics for Credit Risk Default Prediction**\n",
    "\n",
    "In credit risk modeling, correctly classifying **defaulting customers** is crucial, as misclassifications can lead to **financial losses** (false negatives) or **lost opportunities** (false positives). The following metrics help assess model performance:\n",
    "\n",
    "## **1. Accuracy**\n",
    "Accuracy measures the proportion of correctly classified instances over the total dataset:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $TP$ = True Positives (correctly predicted defaults)\n",
    "- $TN$ = True Negatives (correctly predicted non-defaults)\n",
    "- $FP$ = False Positives (incorrectly predicted defaults)\n",
    "- $FN$ = False Negatives (incorrectly predicted non-defaults)\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- Accuracy gives an overall measure of correctness but can be **misleading in imbalanced datasets** (e.g., if defaults are rare, a model predicting all customers as non-defaults can still have high accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**\n",
    "The **ROC-AUC** measures a model’s ability to distinguish between the positive (default) and negative (non-default) classes. The **ROC curve** plots the **True Positive Rate (Recall)** against the **False Positive Rate (FPR)** at different classification thresholds.  \n",
    "\n",
    "### **Mathematical Formulation:**\n",
    "The **AUC (Area Under Curve)** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{AUC} = \\int_0^1 \\text{TPR} \\, d(\\text{FPR})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **True Positive Rate (TPR) / Recall:**\n",
    "  $$\n",
    "  \\text{TPR} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "- **False Positive Rate (FPR):**\n",
    "  $$\n",
    "  \\text{FPR} = \\frac{FP}{FP + TN}\n",
    "  $$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Higher AUC** means the model **better separates defaults from non-defaults**.\n",
    "- **AUC close to 0.5** suggests the model is **random** (not useful).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. PR-AUC (Precision-Recall Area Under Curve)**\n",
    "PR-AUC measures the area under the **Precision-Recall (PR) curve**, focusing on **positive (default) predictions**.\n",
    "\n",
    "### **Mathematical Formulation:**\n",
    "The **AUC for Precision-Recall** is:\n",
    "\n",
    "$$\n",
    "\\text{PR-AUC} = \\int_0^1 \\text{Precision} \\, d(\\text{Recall})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **Precision (Positive Predictive Value, PPV):**\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "\n",
    "- **Recall (Sensitivity / TPR) (as defined above)**\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **More useful than ROC-AUC** for **imbalanced data** since it focuses on **true defaults**.\n",
    "- **Higher PR-AUC** indicates a better balance between **precision and recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Recall (Sensitivity)**\n",
    "Recall, also called **Sensitivity or True Positive Rate (TPR)**, measures the ability to detect **actual defaults**:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **High recall** ensures **most actual defaults are detected**, minimizing **false negatives**.\n",
    "- **Low recall** means many **defaulting customers** are **missed**, leading to **financial losses**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. F1-Score**\n",
    "F1-Score is the harmonic mean of **Precision** and **Recall**, balancing both:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Best when both False Positives & False Negatives are costly**.\n",
    "- **Useful in imbalanced datasets**, where a high precision or recall alone isn't enough.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Specificity (True Negative Rate)**\n",
    "Specificity measures how well the model identifies **non-defaulting customers**:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Higher specificity** reduces **false alarms (FP)**.\n",
    "- **Too high specificity may mean recall is low**, missing many defaults.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. False Positive Rate (FPR)**\n",
    "FPR is the proportion of **non-defaulting customers incorrectly classified as defaults**:\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN} = 1 - \\text{Specificity}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Low FPR** ensures fewer **non-defaulters are wrongly flagged**, reducing unnecessary **loan rejections**.\n",
    "- **High FPR** can **hurt customer experience**, causing **unnecessary loan rejections**.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. G-Mean (Geometric Mean)**\n",
    "The **G-Mean** is a performance metric balancing **recall** and **specificity**:\n",
    "\n",
    "$$\n",
    "G\\text{-Mean} = \\sqrt{\\text{Recall} \\times \\text{Specificity}}\n",
    "$$\n",
    "\n",
    "### **Importance in Credit Risk:**\n",
    "- **Higher G-Mean** ensures the model performs well on **both default and non-default classes**.\n",
    "- Useful for **handling class imbalance**, where one metric alone (like accuracy) can be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "# **Summary Table of Metrics**\n",
    "| **Metric**         | **Interpretation** |\n",
    "|--------------------|------------------|\n",
    "| **Accuracy**      | Overall correctness, but misleading in imbalanced data |\n",
    "| **ROC-AUC**       | Ability to distinguish defaults vs. non-defaults |\n",
    "| **PR-AUC**        | Performance on the default class, useful for imbalance |\n",
    "| **Recall**        | Ability to detect defaults (avoid false negatives) |\n",
    "| **F1-Score**      | Balance between Precision & Recall |\n",
    "| **Specificity**   | Correctly identifying non-defaulters |\n",
    "| **FPR**           | Incorrectly flagging non-defaulters as defaults |\n",
    "| **G-Mean**        | Balance between Recall & Specificity (useful for imbalance) |\n",
    "\n",
    "---\n",
    "\n",
    "# **Final Thoughts**\n",
    "For **credit risk prediction**, metrics should be **carefully chosen** based on **business priorities**:\n",
    "\n",
    "- **If missing defaults is costly** → **High Recall (Sensitivity)**.\n",
    "- **If wrongly flagging non-defaulters is a concern** → **Low False Positive Rate (FPR)**.\n",
    "- **For overall balance** → **High G-Mean & F1-Score**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba468a0-0d7a-47a1-8880-546940336b76",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f189a895-6caf-4240-be53-b071af2e5239",
   "metadata": {},
   "source": [
    "## Data Fetch in YKB Computer - Taiwan Credit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680f11c0-af5f-49e4-9f54-8114a4a35607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13559.0</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49291.0</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35835.0</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1   X2   X3   X4    X5   X6   X7   X8   X9  X10  ...      X14  \\\n",
       "1   20000.0  2.0  2.0  1.0  24.0  2.0  2.0 -1.0 -1.0 -2.0  ...    689.0   \n",
       "2  120000.0  2.0  2.0  2.0  26.0 -1.0  2.0  0.0  0.0  0.0  ...   2682.0   \n",
       "3   90000.0  2.0  2.0  2.0  34.0  0.0  0.0  0.0  0.0  0.0  ...  13559.0   \n",
       "4   50000.0  2.0  2.0  1.0  37.0  0.0  0.0  0.0  0.0  0.0  ...  49291.0   \n",
       "5   50000.0  1.0  2.0  1.0  57.0 -1.0  0.0 -1.0  0.0  0.0  ...  35835.0   \n",
       "\n",
       "       X15      X16      X17     X18      X19      X20     X21     X22     X23  \n",
       "1      0.0      0.0      0.0     0.0    689.0      0.0     0.0     0.0     0.0  \n",
       "2   3272.0   3455.0   3261.0     0.0   1000.0   1000.0  1000.0     0.0  2000.0  \n",
       "3  14331.0  14948.0  15549.0  1518.0   1500.0   1000.0  1000.0  1000.0  5000.0  \n",
       "4  28314.0  28959.0  29547.0  2000.0   2019.0   1200.0  1100.0  1069.0  1000.0  \n",
       "5  20940.0  19146.0  19131.0  2000.0  36681.0  10000.0  9000.0   689.0   679.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('default of credit card clients.xls',index_col=0).iloc[1:,:]\n",
    "\n",
    "X=df.iloc[:,:-1]\n",
    "y=pd.DataFrame(df.iloc[:,-1],columns=['Y'])\n",
    "X=X.astype(float)\n",
    "y=y.astype(int)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd64911-54ac-4d96-8d03-38b0e632f8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "958b8268-4316-45ac-9358-5d5a948df58c",
   "metadata": {},
   "source": [
    "## Data Fetch in Personal Computer -- Taiwan Credit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80ea1eb-3c70-4d9a-a243-f80a5b3868cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error connecting to server",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\onur_1\\Lib\\site-packages\\ucimlrepo\\fetch.py:68\u001b[0m, in \u001b[0;36mfetch_ucirepo\u001b[1;34m(name, id)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     response \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(api_url, context\u001b[38;5;241m=\u001b[39mssl\u001b[38;5;241m.\u001b[39mcreate_default_context(cafile\u001b[38;5;241m=\u001b[39mcertifi\u001b[38;5;241m.\u001b[39mwhere()))\n\u001b[0;32m     69\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(response)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\onur_1\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\onur_1\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\onur_1\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\onur_1\\Lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    562\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\onur_1\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\onur_1\\Lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mucimlrepo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_ucirepo \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# fetch dataset \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m default_of_credit_card_clients \u001b[38;5;241m=\u001b[39m fetch_ucirepo(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m350\u001b[39m) \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# data (as pandas dataframes) \u001b[39;00m\n\u001b[0;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m default_of_credit_card_clients\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfeatures \n",
      "File \u001b[1;32m~\\anaconda3\\envs\\onur_1\\Lib\\site-packages\\ucimlrepo\\fetch.py:71\u001b[0m, in \u001b[0;36mfetch_ucirepo\u001b[1;34m(name, id)\u001b[0m\n\u001b[0;32m     69\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(response)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError):\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError connecting to server\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# verify that dataset exists \u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[1;31mConnectionError\u001b[0m: Error connecting to server"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = default_of_credit_card_clients.data.features \n",
    "y = default_of_credit_card_clients.data.targets \n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461982a-7224-406e-b1f5-6b3eacff491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of Positive targets : {((y.sum()/y.count())*100).values[0]}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb95e0-1164-4b60-8dc2-531b93cbd712",
   "metadata": {},
   "source": [
    "## Model Training And Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18a76959-6f33-43a0-b516-86a6f0ea4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 00:44:52,470] A new study created in memory with name: no-name-a595808f-8f73-4fda-bfb5-d0e1620124bc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taiwan Credit dataset: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 00:44:53,877] Trial 0 finished with value: 0.766811333198052 and parameters: {'learning_rate': 0.08728447645777467, 'max_depth': 9, 'subsample': 0.791290072361565, 'colsample_bytree': 0.6557158967004103, 'gamma': 0.826981418165238, 'reg_alpha': 0.9882217675139366, 'reg_lambda': 0.422528159447663}. Best is trial 0 with value: 0.766811333198052.\n",
      "[I 2025-03-13 00:44:54,293] Trial 1 finished with value: 0.7689011769480519 and parameters: {'learning_rate': 0.15783644765001797, 'max_depth': 4, 'subsample': 0.8085995812503914, 'colsample_bytree': 0.8038851810180707, 'gamma': 0.7505087497670536, 'reg_alpha': 0.07409263055057169, 'reg_lambda': 0.16118872198583567}. Best is trial 1 with value: 0.7689011769480519.\n",
      "[I 2025-03-13 00:44:54,709] Trial 2 finished with value: 0.7571003884508349 and parameters: {'learning_rate': 0.28303142172776985, 'max_depth': 4, 'subsample': 0.6041032480194249, 'colsample_bytree': 0.7960379801770414, 'gamma': 0.5667587176457386, 'reg_alpha': 0.41275050975571026, 'reg_lambda': 0.5048962719781125}. Best is trial 1 with value: 0.7689011769480519.\n",
      "[I 2025-03-13 00:44:55,186] Trial 3 finished with value: 0.7597555513682746 and parameters: {'learning_rate': 0.25602635530747075, 'max_depth': 6, 'subsample': 0.9378644061518595, 'colsample_bytree': 0.7978989872480987, 'gamma': 0.7991988834156323, 'reg_alpha': 0.7094823126152717, 'reg_lambda': 0.22137465025050496}. Best is trial 1 with value: 0.7689011769480519.\n",
      "[I 2025-03-13 00:44:55,991] Trial 4 finished with value: 0.7573756739911874 and parameters: {'learning_rate': 0.13468591293280377, 'max_depth': 7, 'subsample': 0.6419484099039061, 'colsample_bytree': 0.6616915786622919, 'gamma': 0.27780231148434376, 'reg_alpha': 0.02322074726963519, 'reg_lambda': 0.8164064119746425}. Best is trial 1 with value: 0.7689011769480519.\n",
      "[I 2025-03-13 00:44:56,587] Trial 5 finished with value: 0.7675985621521335 and parameters: {'learning_rate': 0.11051659737998089, 'max_depth': 6, 'subsample': 0.8341260170847746, 'colsample_bytree': 0.9510552447853418, 'gamma': 0.5322071661689214, 'reg_alpha': 0.24741461900053519, 'reg_lambda': 0.4043345988951532}. Best is trial 1 with value: 0.7689011769480519.\n",
      "[I 2025-03-13 00:44:58,100] Trial 6 finished with value: 0.7781414511827458 and parameters: {'learning_rate': 0.012589313241616391, 'max_depth': 9, 'subsample': 0.7150990924877889, 'colsample_bytree': 0.8533053678697813, 'gamma': 0.5130891994015488, 'reg_alpha': 0.5653610727019779, 'reg_lambda': 0.7769596799530271}. Best is trial 6 with value: 0.7781414511827458.\n",
      "[I 2025-03-13 00:44:58,884] Trial 7 finished with value: 0.7555133203849721 and parameters: {'learning_rate': 0.18509070921611734, 'max_depth': 7, 'subsample': 0.7792266849022723, 'colsample_bytree': 0.8995869009613767, 'gamma': 0.6771786971269117, 'reg_alpha': 0.9825557222856364, 'reg_lambda': 0.7198364277174769}. Best is trial 6 with value: 0.7781414511827458.\n",
      "[I 2025-03-13 00:44:59,681] Trial 8 finished with value: 0.7792016828038033 and parameters: {'learning_rate': 0.016689458612705568, 'max_depth': 7, 'subsample': 0.9171501186659916, 'colsample_bytree': 0.6126204222112038, 'gamma': 0.8885055079910207, 'reg_alpha': 0.89675421586718, 'reg_lambda': 0.6107061329824253}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:00,624] Trial 9 finished with value: 0.7456783757537105 and parameters: {'learning_rate': 0.24651536905183907, 'max_depth': 9, 'subsample': 0.8776883988524176, 'colsample_bytree': 0.9378719092542535, 'gamma': 0.36185734058858354, 'reg_alpha': 0.05152567267123531, 'reg_lambda': 0.2597107094033567}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:01,012] Trial 10 finished with value: 0.7734124246289424 and parameters: {'learning_rate': 0.01595518639562345, 'max_depth': 3, 'subsample': 0.981351643066904, 'colsample_bytree': 0.7139002852035078, 'gamma': 0.017002919803618788, 'reg_alpha': 0.7349831332991065, 'reg_lambda': 0.9967526929182695}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:03,040] Trial 11 finished with value: 0.7780641958487942 and parameters: {'learning_rate': 0.011102503952225801, 'max_depth': 10, 'subsample': 0.7211752481063356, 'colsample_bytree': 0.8699935429894055, 'gamma': 0.9639310500735748, 'reg_alpha': 0.6694364676522265, 'reg_lambda': 0.6605819846842551}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:04,058] Trial 12 finished with value: 0.7717647698283858 and parameters: {'learning_rate': 0.06056189869716311, 'max_depth': 8, 'subsample': 0.7349192209438803, 'colsample_bytree': 0.7269463610720316, 'gamma': 0.9894563661627103, 'reg_alpha': 0.5198244643930934, 'reg_lambda': 0.8887093173928373}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:05,926] Trial 13 finished with value: 0.7678596069109461 and parameters: {'learning_rate': 0.05497516137658805, 'max_depth': 10, 'subsample': 0.8886054153938164, 'colsample_bytree': 0.8517183182071929, 'gamma': 0.3311842284337141, 'reg_alpha': 0.8284319438015788, 'reg_lambda': 0.5927570615784059}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:07,083] Trial 14 finished with value: 0.7760584198167902 and parameters: {'learning_rate': 0.04663632392348506, 'max_depth': 8, 'subsample': 0.6880894227664578, 'colsample_bytree': 0.7339101923990601, 'gamma': 0.12320829792741894, 'reg_alpha': 0.47429569323973514, 'reg_lambda': 0.7868371414977082}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:07,749] Trial 15 finished with value: 0.7592060311340445 and parameters: {'learning_rate': 0.19445475924866984, 'max_depth': 8, 'subsample': 0.9790496681424972, 'colsample_bytree': 0.6097534575319072, 'gamma': 0.42404965006225226, 'reg_alpha': 0.8563833760088474, 'reg_lambda': 0.5691390700941579}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:08,311] Trial 16 finished with value: 0.7767970199443414 and parameters: {'learning_rate': 0.08632148174848642, 'max_depth': 5, 'subsample': 0.882743114540206, 'colsample_bytree': 0.6033697272860776, 'gamma': 0.6329882356891523, 'reg_alpha': 0.5975371431953169, 'reg_lambda': 0.9409654135948651}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:09,732] Trial 17 finished with value: 0.7734522843228201 and parameters: {'learning_rate': 0.03439181739418426, 'max_depth': 9, 'subsample': 0.6749412593309624, 'colsample_bytree': 0.8361070855674139, 'gamma': 0.2232044959407009, 'reg_alpha': 0.3188597279270035, 'reg_lambda': 0.3504073349495237}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:10,559] Trial 18 finished with value: 0.7677876782815399 and parameters: {'learning_rate': 0.09053553846092269, 'max_depth': 7, 'subsample': 0.7375491196178688, 'colsample_bytree': 0.9927447203608672, 'gamma': 0.8850780276858989, 'reg_alpha': 0.8515166035099948, 'reg_lambda': 0.6888869362716847}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:11,072] Trial 19 finished with value: 0.7727188297193877 and parameters: {'learning_rate': 0.1270010470032695, 'max_depth': 5, 'subsample': 0.9388604273907115, 'colsample_bytree': 0.778996006425381, 'gamma': 0.6779280021049454, 'reg_alpha': 0.21746689885556025, 'reg_lambda': 0.01732990647574595}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:12,137] Trial 20 finished with value: 0.766786439007421 and parameters: {'learning_rate': 0.07207099580639907, 'max_depth': 8, 'subsample': 0.8448001288639344, 'colsample_bytree': 0.9012795635149045, 'gamma': 0.46629231514222086, 'reg_alpha': 0.581288806220497, 'reg_lambda': 0.8105645105929533}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:13,902] Trial 21 finished with value: 0.7778144567486085 and parameters: {'learning_rate': 0.016013999234283833, 'max_depth': 10, 'subsample': 0.7321911396916283, 'colsample_bytree': 0.8608101712049011, 'gamma': 0.9741614802167017, 'reg_alpha': 0.689947343925035, 'reg_lambda': 0.6431630837180525}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:15,735] Trial 22 finished with value: 0.7776055194805194 and parameters: {'learning_rate': 0.020375924885845054, 'max_depth': 10, 'subsample': 0.6928400671471419, 'colsample_bytree': 0.8823739385958206, 'gamma': 0.8682207091649053, 'reg_alpha': 0.6446307071500266, 'reg_lambda': 0.5278040777028931}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:17,024] Trial 23 finished with value: 0.7734786641929499 and parameters: {'learning_rate': 0.037506741325786186, 'max_depth': 9, 'subsample': 0.7602215350284924, 'colsample_bytree': 0.764050667343689, 'gamma': 0.9289135658687492, 'reg_alpha': 0.8193566940122787, 'reg_lambda': 0.7282004245803166}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:18,885] Trial 24 finished with value: 0.777576965445269 and parameters: {'learning_rate': 0.011379455657701371, 'max_depth': 10, 'subsample': 0.6465250958821224, 'colsample_bytree': 0.8245609209608858, 'gamma': 0.8028066122257462, 'reg_alpha': 0.7692420165252667, 'reg_lambda': 0.6111798231331738}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:20,471] Trial 25 finished with value: 0.7744659888102968 and parameters: {'learning_rate': 0.037273467710009785, 'max_depth': 9, 'subsample': 0.7066365325432366, 'colsample_bytree': 0.9304370704504182, 'gamma': 0.7241882270280716, 'reg_alpha': 0.40870602235165765, 'reg_lambda': 0.8988652890146158}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:22,083] Trial 26 finished with value: 0.7668603606215213 and parameters: {'learning_rate': 0.0678810300296026, 'max_depth': 10, 'subsample': 0.764788581581993, 'colsample_bytree': 0.6730966404983155, 'gamma': 0.5730507815365089, 'reg_alpha': 0.906289689812722, 'reg_lambda': 0.7493134473845382}. Best is trial 8 with value: 0.7792016828038033.\n",
      "[I 2025-03-13 00:45:23,306] Trial 27 finished with value: 0.779497043135436 and parameters: {'learning_rate': 0.010226558398113629, 'max_depth': 8, 'subsample': 0.8205408730004504, 'colsample_bytree': 0.8819755976567215, 'gamma': 0.9998911855882852, 'reg_alpha': 0.5702648428148127, 'reg_lambda': 0.647428600852368}. Best is trial 27 with value: 0.779497043135436.\n",
      "[I 2025-03-13 00:45:23,965] Trial 28 finished with value: 0.7688672237360853 and parameters: {'learning_rate': 0.10481649099726187, 'max_depth': 7, 'subsample': 0.9254105366587871, 'colsample_bytree': 0.9725139258147684, 'gamma': 0.9021886046903735, 'reg_alpha': 0.5250960411750836, 'reg_lambda': 0.41935671395464313}. Best is trial 27 with value: 0.779497043135436.\n",
      "[I 2025-03-13 00:45:24,950] Trial 29 finished with value: 0.7703246028525047 and parameters: {'learning_rate': 0.07384632901998667, 'max_depth': 8, 'subsample': 0.8079380548581712, 'colsample_bytree': 0.692169911988725, 'gamma': 0.8231588214095369, 'reg_alpha': 0.9865593673461214, 'reg_lambda': 0.507910494425102}. Best is trial 27 with value: 0.779497043135436.\n",
      "[I 2025-03-13 00:45:25,623] Trial 30 finished with value: 0.7793966691790352 and parameters: {'learning_rate': 0.035474687747127726, 'max_depth': 6, 'subsample': 0.8358201941239992, 'colsample_bytree': 0.6307052502146412, 'gamma': 0.9995450498036323, 'reg_alpha': 0.38435282632955714, 'reg_lambda': 0.8526117686983417}. Best is trial 27 with value: 0.779497043135436.\n",
      "[I 2025-03-13 00:45:26,288] Trial 31 finished with value: 0.7780541946892393 and parameters: {'learning_rate': 0.037562457299103805, 'max_depth': 6, 'subsample': 0.8442831804017539, 'colsample_bytree': 0.6212586832581909, 'gamma': 0.9958136823929582, 'reg_alpha': 0.35246069040424954, 'reg_lambda': 0.8387521351108469}. Best is trial 27 with value: 0.779497043135436.\n",
      "[I 2025-03-13 00:45:26,853] Trial 32 finished with value: 0.7791318196312617 and parameters: {'learning_rate': 0.05179364088361426, 'max_depth': 5, 'subsample': 0.8196800888199258, 'colsample_bytree': 0.6411176442030787, 'gamma': 0.867736307343308, 'reg_alpha': 0.1459117925408876, 'reg_lambda': 0.6664975589219142}. Best is trial 27 with value: 0.779497043135436.\n",
      "[I 2025-03-13 00:45:27,398] Trial 33 finished with value: 0.7782376217532468 and parameters: {'learning_rate': 0.05392981978098493, 'max_depth': 5, 'subsample': 0.823917381032369, 'colsample_bytree': 0.6353259192926346, 'gamma': 0.855423585664226, 'reg_alpha': 0.1958132433407063, 'reg_lambda': 0.5575386484902032}. Best is trial 27 with value: 0.779497043135436.\n",
      "[I 2025-03-13 00:45:27,855] Trial 34 finished with value: 0.7799920642973098 and parameters: {'learning_rate': 0.03236797181728346, 'max_depth': 4, 'subsample': 0.9110917189190372, 'colsample_bytree': 0.6485239890021529, 'gamma': 0.7655273377085615, 'reg_alpha': 0.2939936896201535, 'reg_lambda': 0.47588460652455156}. Best is trial 34 with value: 0.7799920642973098.\n",
      "[I 2025-03-13 00:45:28,321] Trial 35 finished with value: 0.7805333227040816 and parameters: {'learning_rate': 0.031239832980427443, 'max_depth': 4, 'subsample': 0.9249309198259971, 'colsample_bytree': 0.6860194343374963, 'gamma': 0.7578822768156139, 'reg_alpha': 0.29017177367967617, 'reg_lambda': 0.4638467463260988}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:28,717] Trial 36 finished with value: 0.7793124927527829 and parameters: {'learning_rate': 0.08642548235340573, 'max_depth': 3, 'subsample': 0.8613091897776586, 'colsample_bytree': 0.6818153975703775, 'gamma': 0.737345390877582, 'reg_alpha': 0.32635679497669867, 'reg_lambda': 0.4663389367846627}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:29,178] Trial 37 finished with value: 0.7745660366419295 and parameters: {'learning_rate': 0.14668361305292527, 'max_depth': 4, 'subsample': 0.9529446066342162, 'colsample_bytree': 0.6537268925624553, 'gamma': 0.7887879537727306, 'reg_alpha': 0.40093336209218344, 'reg_lambda': 0.31083043000498173}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:29,640] Trial 38 finished with value: 0.7796622434485158 and parameters: {'learning_rate': 0.0324159655973453, 'max_depth': 4, 'subsample': 0.9004502973669704, 'colsample_bytree': 0.6990472171671026, 'gamma': 0.9248927227834047, 'reg_alpha': 0.2903807079513349, 'reg_lambda': 0.46508642237931597}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:30,103] Trial 39 finished with value: 0.7743497072124305 and parameters: {'learning_rate': 0.11393578135661712, 'max_depth': 4, 'subsample': 0.8965652673609127, 'colsample_bytree': 0.7031323759441217, 'gamma': 0.7701634250403893, 'reg_alpha': 0.2731197854790719, 'reg_lambda': 0.46000183089897906}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:30,496] Trial 40 finished with value: 0.7724990216256957 and parameters: {'learning_rate': 0.19861861389856503, 'max_depth': 3, 'subsample': 0.9597439871144057, 'colsample_bytree': 0.750658521167503, 'gamma': 0.6475200504145884, 'reg_alpha': 0.15011857254997452, 'reg_lambda': 0.35634734629489684}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:31,057] Trial 41 finished with value: 0.7802479272959183 and parameters: {'learning_rate': 0.02669812754723797, 'max_depth': 5, 'subsample': 0.9095281761889362, 'colsample_bytree': 0.6602838220079927, 'gamma': 0.9488344798104601, 'reg_alpha': 0.4593391842358749, 'reg_lambda': 0.4541466412743952}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:31,515] Trial 42 finished with value: 0.7801744405148423 and parameters: {'learning_rate': 0.03048115898633468, 'max_depth': 4, 'subsample': 0.9017022332430147, 'colsample_bytree': 0.6617498832841431, 'gamma': 0.9291800262421799, 'reg_alpha': 0.47806767466129263, 'reg_lambda': 0.46125837120924057}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:31,970] Trial 43 finished with value: 0.7799748521567719 and parameters: {'learning_rate': 0.029317311971645405, 'max_depth': 4, 'subsample': 0.9083263951687626, 'colsample_bytree': 0.6599364614057922, 'gamma': 0.9315748115560114, 'reg_alpha': 0.4676437410278832, 'reg_lambda': 0.450981217401046}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:32,436] Trial 44 finished with value: 0.7795019712430427 and parameters: {'learning_rate': 0.025715535175730303, 'max_depth': 4, 'subsample': 0.9091587756588174, 'colsample_bytree': 0.6563687856459146, 'gamma': 0.9376181406636737, 'reg_alpha': 0.45210032591794963, 'reg_lambda': 0.20796048160414893}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:32,818] Trial 45 finished with value: 0.7793109708371986 and parameters: {'learning_rate': 0.045566980401763726, 'max_depth': 3, 'subsample': 0.9338473264659775, 'colsample_bytree': 0.6687024372279731, 'gamma': 0.8313416643087481, 'reg_alpha': 0.4727133555882166, 'reg_lambda': 0.36408222004444424}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:33,280] Trial 46 finished with value: 0.7778580487592766 and parameters: {'learning_rate': 0.06691233355137707, 'max_depth': 4, 'subsample': 0.8575209761067096, 'colsample_bytree': 0.6494342328556316, 'gamma': 0.7105280998934992, 'reg_alpha': 0.3552910259998986, 'reg_lambda': 0.40674270757558934}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:33,780] Trial 47 finished with value: 0.7621629319341373 and parameters: {'learning_rate': 0.22987930391704914, 'max_depth': 5, 'subsample': 0.966355268422179, 'colsample_bytree': 0.7155278548855535, 'gamma': 0.5865453663141099, 'reg_alpha': 0.44343488229353695, 'reg_lambda': 0.32507408277475364}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:34,109] Trial 48 finished with value: 0.7747089879986085 and parameters: {'learning_rate': 0.2943068395245978, 'max_depth': 4, 'subsample': 0.9954181863543115, 'colsample_bytree': 0.6822920666660435, 'gamma': 0.9445657305291592, 'reg_alpha': 0.505206599374142, 'reg_lambda': 0.2766953058161762}. Best is trial 35 with value: 0.7805333227040816.\n",
      "[I 2025-03-13 00:45:34,498] Trial 49 finished with value: 0.7798934296730056 and parameters: {'learning_rate': 0.05820332265421739, 'max_depth': 3, 'subsample': 0.8715311893634323, 'colsample_bytree': 0.732722919652733, 'gamma': 0.7573368725634461, 'reg_alpha': 0.1261545706910704, 'reg_lambda': 0.45415177986134425}. Best is trial 35 with value: 0.7805333227040816.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (No SMOTE): {'learning_rate': 0.031239832980427443, 'max_depth': 4, 'subsample': 0.9249309198259971, 'colsample_bytree': 0.6860194343374963, 'gamma': 0.7578822768156139, 'reg_alpha': 0.29017177367967617, 'reg_lambda': 0.4638467463260988}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 00:45:35,017] A new study created in memory with name: no-name-dd6d9995-41a5-42d1-8537-603f79da2fc6\n",
      "[I 2025-03-13 00:45:35,364] Trial 0 finished with value: 0.7626787526089981 and parameters: {'learning_rate': 0.09990632415720277, 'max_depth': 3, 'subsample': 0.7328023989930054, 'colsample_bytree': 0.9423241643731001, 'gamma': 0.22641075985623604, 'reg_alpha': 0.6608855030281018, 'reg_lambda': 0.8599217643626863}. Best is trial 0 with value: 0.7626787526089981.\n",
      "[I 2025-03-13 00:45:35,798] Trial 1 finished with value: 0.7596382913961038 and parameters: {'learning_rate': 0.21263352997017904, 'max_depth': 4, 'subsample': 0.7310097762922552, 'colsample_bytree': 0.6454912955828934, 'gamma': 0.7329541956573411, 'reg_alpha': 0.6175842694108722, 'reg_lambda': 0.531095746302933}. Best is trial 0 with value: 0.7626787526089981.\n",
      "[I 2025-03-13 00:45:37,458] Trial 2 finished with value: 0.7489874188311689 and parameters: {'learning_rate': 0.16308269889961977, 'max_depth': 10, 'subsample': 0.7595705217097859, 'colsample_bytree': 0.7498197122389616, 'gamma': 0.20028007220260202, 'reg_alpha': 0.8646445871835209, 'reg_lambda': 0.0731425151282935}. Best is trial 0 with value: 0.7626787526089981.\n",
      "[I 2025-03-13 00:45:38,475] Trial 3 finished with value: 0.7482387813079777 and parameters: {'learning_rate': 0.17587121199851863, 'max_depth': 10, 'subsample': 0.7639383432403075, 'colsample_bytree': 0.6792192903858987, 'gamma': 0.9412400394668929, 'reg_alpha': 0.788655842895051, 'reg_lambda': 0.7621923687464918}. Best is trial 0 with value: 0.7626787526089981.\n",
      "[I 2025-03-13 00:45:38,831] Trial 4 finished with value: 0.7644379783163265 and parameters: {'learning_rate': 0.044515180141141475, 'max_depth': 3, 'subsample': 0.9190589330908873, 'colsample_bytree': 0.602817594549353, 'gamma': 0.8687058751812786, 'reg_alpha': 0.8737280981582098, 'reg_lambda': 0.2494484674086389}. Best is trial 4 with value: 0.7644379783163265.\n",
      "[I 2025-03-13 00:45:39,995] Trial 5 finished with value: 0.7525228649698515 and parameters: {'learning_rate': 0.12124585284415249, 'max_depth': 9, 'subsample': 0.650232347458205, 'colsample_bytree': 0.7683935653271695, 'gamma': 0.8447740222370738, 'reg_alpha': 0.6706799252730368, 'reg_lambda': 0.9017951141861789}. Best is trial 4 with value: 0.7644379783163265.\n",
      "[I 2025-03-13 00:45:41,752] Trial 6 finished with value: 0.7561907177643784 and parameters: {'learning_rate': 0.0944878946240634, 'max_depth': 10, 'subsample': 0.8138414621180965, 'colsample_bytree': 0.9857465997977032, 'gamma': 0.18574902947677252, 'reg_alpha': 0.3720527838144704, 'reg_lambda': 0.9504918929558628}. Best is trial 4 with value: 0.7644379783163265.\n",
      "[I 2025-03-13 00:45:42,679] Trial 7 finished with value: 0.7491326530612245 and parameters: {'learning_rate': 0.16908127457118674, 'max_depth': 8, 'subsample': 0.7766162138770597, 'colsample_bytree': 0.9481278104670435, 'gamma': 0.8476206800654718, 'reg_alpha': 0.16157688524636782, 'reg_lambda': 0.3374228585815412}. Best is trial 4 with value: 0.7644379783163265.\n",
      "[I 2025-03-13 00:45:43,112] Trial 8 finished with value: 0.7604110259160481 and parameters: {'learning_rate': 0.14930343470867352, 'max_depth': 4, 'subsample': 0.740903492322228, 'colsample_bytree': 0.6566497941163184, 'gamma': 0.45719076211224574, 'reg_alpha': 0.6217157456366176, 'reg_lambda': 0.9674072776884604}. Best is trial 4 with value: 0.7644379783163265.\n",
      "[I 2025-03-13 00:45:44,373] Trial 9 finished with value: 0.7488063108766233 and parameters: {'learning_rate': 0.2004523714099245, 'max_depth': 10, 'subsample': 0.8050485525598002, 'colsample_bytree': 0.7573444128953273, 'gamma': 0.437688874091422, 'reg_alpha': 0.9338113817514004, 'reg_lambda': 0.6827035725152881}. Best is trial 4 with value: 0.7644379783163265.\n",
      "[I 2025-03-13 00:45:45,068] Trial 10 finished with value: 0.7671674614448052 and parameters: {'learning_rate': 0.013318321162732641, 'max_depth': 6, 'subsample': 0.9679798121848968, 'colsample_bytree': 0.8578646330727814, 'gamma': 0.6513752564718724, 'reg_alpha': 0.4023750878484702, 'reg_lambda': 0.10826009661560343}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:45,761] Trial 11 finished with value: 0.7657273307050093 and parameters: {'learning_rate': 0.018979219905266624, 'max_depth': 6, 'subsample': 0.9799366278825256, 'colsample_bytree': 0.859546118192242, 'gamma': 0.6311452876718214, 'reg_alpha': 0.37247966357077605, 'reg_lambda': 0.09317798593402937}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:46,455] Trial 12 finished with value: 0.7665010073631726 and parameters: {'learning_rate': 0.01481442223610722, 'max_depth': 6, 'subsample': 0.9999843836962967, 'colsample_bytree': 0.8579561940504418, 'gamma': 0.6245904075158825, 'reg_alpha': 0.3575056824221814, 'reg_lambda': 0.006838226470756853}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:47,128] Trial 13 finished with value: 0.7618997854823748 and parameters: {'learning_rate': 0.05793324002476717, 'max_depth': 6, 'subsample': 0.9987876835753221, 'colsample_bytree': 0.8674155378735335, 'gamma': 0.669347892774308, 'reg_alpha': 0.19163713724433284, 'reg_lambda': 0.032812419809085475}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:47,865] Trial 14 finished with value: 0.7449210053339518 and parameters: {'learning_rate': 0.2800445216421401, 'max_depth': 7, 'subsample': 0.902053546687749, 'colsample_bytree': 0.8496683669530778, 'gamma': 0.5653277116358422, 'reg_alpha': 0.3774157441297406, 'reg_lambda': 0.2570410114035664}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:48,432] Trial 15 finished with value: 0.7665174585459184 and parameters: {'learning_rate': 0.02244012096310219, 'max_depth': 5, 'subsample': 0.9247289977744042, 'colsample_bytree': 0.8192147836178593, 'gamma': 0.3348880869669528, 'reg_alpha': 0.2700844688496856, 'reg_lambda': 0.41221663879376175}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:49,006] Trial 16 finished with value: 0.7622291714981447 and parameters: {'learning_rate': 0.06351787050210307, 'max_depth': 5, 'subsample': 0.8941463007071381, 'colsample_bytree': 0.8121406744664063, 'gamma': 0.016008936124086826, 'reg_alpha': 0.012461780032950731, 'reg_lambda': 0.4739922931554345}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:49,852] Trial 17 finished with value: 0.7459285859230056 and parameters: {'learning_rate': 0.27946251601570515, 'max_depth': 7, 'subsample': 0.9440884866682429, 'colsample_bytree': 0.9105608217488068, 'gamma': 0.3148266961155065, 'reg_alpha': 0.49865148388025443, 'reg_lambda': 0.449547863633577}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:50,407] Trial 18 finished with value: 0.7640812340561225 and parameters: {'learning_rate': 0.08180058128010317, 'max_depth': 5, 'subsample': 0.8590424062168281, 'colsample_bytree': 0.8081248793516417, 'gamma': 0.3665351811765427, 'reg_alpha': 0.23291464219938524, 'reg_lambda': 0.17834792976859792}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:50,990] Trial 19 finished with value: 0.7652400640653989 and parameters: {'learning_rate': 0.026771191392681, 'max_depth': 5, 'subsample': 0.8495551313984923, 'colsample_bytree': 0.7351286890753184, 'gamma': 0.010148724365305895, 'reg_alpha': 0.036292875042360506, 'reg_lambda': 0.6214274377028912}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:51,953] Trial 20 finished with value: 0.7594770408163265 and parameters: {'learning_rate': 0.1262886459486551, 'max_depth': 8, 'subsample': 0.9544341749764121, 'colsample_bytree': 0.7080948943519016, 'gamma': 0.5018932000244067, 'reg_alpha': 0.486891525188977, 'reg_lambda': 0.34865028787096375}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:52,679] Trial 21 finished with value: 0.766617941210575 and parameters: {'learning_rate': 0.0151659371447313, 'max_depth': 6, 'subsample': 0.9643594355875403, 'colsample_bytree': 0.8926089193398145, 'gamma': 0.7167790079711266, 'reg_alpha': 0.3019964595882634, 'reg_lambda': 0.0003056827967306908}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:53,181] Trial 22 finished with value: 0.7659635537453617 and parameters: {'learning_rate': 0.04282611431634116, 'max_depth': 4, 'subsample': 0.9426387852635006, 'colsample_bytree': 0.8938288551115714, 'gamma': 0.5476767424389675, 'reg_alpha': 0.279130908833255, 'reg_lambda': 0.15393911360285206}. Best is trial 10 with value: 0.7671674614448052.\n",
      "[I 2025-03-13 00:45:54,064] Trial 23 finished with value: 0.7677800324675323 and parameters: {'learning_rate': 0.013401610138046709, 'max_depth': 7, 'subsample': 0.8663547656405323, 'colsample_bytree': 0.8247436786151422, 'gamma': 0.7308384061223758, 'reg_alpha': 0.13903745937635814, 'reg_lambda': 0.1514877938642434}. Best is trial 23 with value: 0.7677800324675323.\n",
      "[I 2025-03-13 00:45:54,928] Trial 24 finished with value: 0.7584255420918368 and parameters: {'learning_rate': 0.07533435287123316, 'max_depth': 7, 'subsample': 0.8699630133449437, 'colsample_bytree': 0.9118768682691115, 'gamma': 0.7405100084131102, 'reg_alpha': 0.10048903661302128, 'reg_lambda': 0.15552126649171155}. Best is trial 23 with value: 0.7677800324675323.\n",
      "[I 2025-03-13 00:45:55,995] Trial 25 finished with value: 0.7613654844039889 and parameters: {'learning_rate': 0.043787157060819606, 'max_depth': 8, 'subsample': 0.9639594557685208, 'colsample_bytree': 0.7882905770942412, 'gamma': 0.7472098431571579, 'reg_alpha': 0.12260389697619009, 'reg_lambda': 0.23498286923224806}. Best is trial 23 with value: 0.7677800324675323.\n",
      "[I 2025-03-13 00:45:56,683] Trial 26 finished with value: 0.7692484273538961 and parameters: {'learning_rate': 0.010083131020809412, 'max_depth': 6, 'subsample': 0.6872327687738344, 'colsample_bytree': 0.8349811092485093, 'gamma': 0.9912353535745069, 'reg_alpha': 0.4686915956620613, 'reg_lambda': 0.10090995138391858}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:45:57,511] Trial 27 finished with value: 0.7417230983302412 and parameters: {'learning_rate': 0.23402815062289822, 'max_depth': 7, 'subsample': 0.6531230124599613, 'colsample_bytree': 0.836049998382207, 'gamma': 0.9622859888715635, 'reg_alpha': 0.4339549063139007, 'reg_lambda': 0.32127429271839625}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:45:58,910] Trial 28 finished with value: 0.7611021930078851 and parameters: {'learning_rate': 0.043229776063678026, 'max_depth': 9, 'subsample': 0.6874300433027064, 'colsample_bytree': 0.7858090890010042, 'gamma': 0.9967642532499346, 'reg_alpha': 0.5505853226812073, 'reg_lambda': 0.10844208933689763}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:45:59,613] Trial 29 finished with value: 0.7585749434717068 and parameters: {'learning_rate': 0.10870260309220837, 'max_depth': 6, 'subsample': 0.6242156683625086, 'colsample_bytree': 0.9436833912361571, 'gamma': 0.8097105649306344, 'reg_alpha': 0.7266894083559478, 'reg_lambda': 0.1963998513492034}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:00,702] Trial 30 finished with value: 0.7603091662801484 and parameters: {'learning_rate': 0.06208722919704022, 'max_depth': 8, 'subsample': 0.7019929588467514, 'colsample_bytree': 0.8837868311450153, 'gamma': 0.9191478151319171, 'reg_alpha': 0.5359955059944799, 'reg_lambda': 0.09928107082306213}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:01,416] Trial 31 finished with value: 0.768567261421614 and parameters: {'learning_rate': 0.015637434546041336, 'max_depth': 6, 'subsample': 0.8202218422805795, 'colsample_bytree': 0.8294383456743144, 'gamma': 0.6863012133218912, 'reg_alpha': 0.44071394970783984, 'reg_lambda': 0.004471878123619531}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:02,292] Trial 32 finished with value: 0.7642861853548237 and parameters: {'learning_rate': 0.02937095919747483, 'max_depth': 7, 'subsample': 0.8346070351967775, 'colsample_bytree': 0.8348410736898115, 'gamma': 0.7980316266464274, 'reg_alpha': 0.46089780450209195, 'reg_lambda': 0.06161044561228424}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:02,890] Trial 33 finished with value: 0.7692043642741188 and parameters: {'learning_rate': 0.012965761709837764, 'max_depth': 5, 'subsample': 0.824897661348157, 'colsample_bytree': 0.8171404177304246, 'gamma': 0.6761185549952111, 'reg_alpha': 0.5862335943700623, 'reg_lambda': 0.13892991058511148}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:03,479] Trial 34 finished with value: 0.7656018089053803 and parameters: {'learning_rate': 0.03442556069987248, 'max_depth': 5, 'subsample': 0.7847694590087253, 'colsample_bytree': 0.7252420652457066, 'gamma': 0.5771500315646496, 'reg_alpha': 0.6012129123121285, 'reg_lambda': 0.5522295830224915}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:03,890] Trial 35 finished with value: 0.764750949385436 and parameters: {'learning_rate': 0.05617398671531146, 'max_depth': 3, 'subsample': 0.8160598965880748, 'colsample_bytree': 0.7893708936148773, 'gamma': 0.9126372733369248, 'reg_alpha': 0.7328396332470637, 'reg_lambda': 0.27447035400896025}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:04,380] Trial 36 finished with value: 0.7623287120245826 and parameters: {'learning_rate': 0.09324886555264614, 'max_depth': 4, 'subsample': 0.8801647590024048, 'colsample_bytree': 0.8372288187008727, 'gamma': 0.7745783015232507, 'reg_alpha': 0.5663548693395307, 'reg_lambda': 0.20257757408947613}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:04,952] Trial 37 finished with value: 0.7615685514262523 and parameters: {'learning_rate': 0.07648167913509618, 'max_depth': 5, 'subsample': 0.8359545518559349, 'colsample_bytree': 0.8201877049211107, 'gamma': 0.6921063625908339, 'reg_alpha': 0.6602245639844757, 'reg_lambda': 0.055817396437951194}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:05,662] Trial 38 finished with value: 0.7689492259972172 and parameters: {'learning_rate': 0.011831525373879432, 'max_depth': 6, 'subsample': 0.7470683905503801, 'colsample_bytree': 0.7649993669825731, 'gamma': 0.8319836200534898, 'reg_alpha': 0.7664713193397192, 'reg_lambda': 0.14360051688932549}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:06,343] Trial 39 finished with value: 0.7646644538497216 and parameters: {'learning_rate': 0.03530750055683766, 'max_depth': 6, 'subsample': 0.7300573916519617, 'colsample_bytree': 0.6995946843791256, 'gamma': 0.883490723861824, 'reg_alpha': 0.8347277875155944, 'reg_lambda': 0.8420459948102379}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:06,821] Trial 40 finished with value: 0.765387255044063 and parameters: {'learning_rate': 0.05177054153852692, 'max_depth': 4, 'subsample': 0.7505950125756744, 'colsample_bytree': 0.7713176783942399, 'gamma': 0.9984965796002361, 'reg_alpha': 0.7515812901252812, 'reg_lambda': 0.3703582038036485}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:07,708] Trial 41 finished with value: 0.7676149408627088 and parameters: {'learning_rate': 0.010686905225105603, 'max_depth': 7, 'subsample': 0.7130253610378979, 'colsample_bytree': 0.7482120301168506, 'gamma': 0.8267796518399384, 'reg_alpha': 0.9021368447509865, 'reg_lambda': 0.13511817114060648}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:08,411] Trial 42 finished with value: 0.7629357751623377 and parameters: {'learning_rate': 0.0336085213339271, 'max_depth': 6, 'subsample': 0.7835813554826666, 'colsample_bytree': 0.7702534271662355, 'gamma': 0.886824542607483, 'reg_alpha': 0.6681123873847507, 'reg_lambda': 0.297294693281853}. Best is trial 26 with value: 0.7692484273538961.\n",
      "[I 2025-03-13 00:46:09,022] Trial 43 finished with value: 0.7696516625115956 and parameters: {'learning_rate': 0.010448007849115029, 'max_depth': 5, 'subsample': 0.7671801494534726, 'colsample_bytree': 0.8014523771518339, 'gamma': 0.784383417261917, 'reg_alpha': 0.5961087097696677, 'reg_lambda': 0.2172689724060965}. Best is trial 43 with value: 0.7696516625115956.\n",
      "[I 2025-03-13 00:46:09,630] Trial 44 finished with value: 0.7664000173933211 and parameters: {'learning_rate': 0.03129171496045524, 'max_depth': 5, 'subsample': 0.6812329120349535, 'colsample_bytree': 0.8006740215120018, 'gamma': 0.8468020571890796, 'reg_alpha': 0.6083503419185103, 'reg_lambda': 0.2308493552069662}. Best is trial 43 with value: 0.7696516625115956.\n",
      "[I 2025-03-13 00:46:10,227] Trial 45 finished with value: 0.763271973562152 and parameters: {'learning_rate': 0.06825778919661145, 'max_depth': 5, 'subsample': 0.7300166381034962, 'colsample_bytree': 0.6163372753797856, 'gamma': 0.7872325037052528, 'reg_alpha': 0.9628101296180218, 'reg_lambda': 0.04296333991739414}. Best is trial 43 with value: 0.7696516625115956.\n",
      "[I 2025-03-13 00:46:10,654] Trial 46 finished with value: 0.7586834705473098 and parameters: {'learning_rate': 0.1993505903348551, 'max_depth': 3, 'subsample': 0.7728221439739995, 'colsample_bytree': 0.752866468104672, 'gamma': 0.6104986500622346, 'reg_alpha': 0.813251453405748, 'reg_lambda': 0.07720503662392686}. Best is trial 43 with value: 0.7696516625115956.\n",
      "[I 2025-03-13 00:46:11,143] Trial 47 finished with value: 0.7606071356099258 and parameters: {'learning_rate': 0.15222919463481896, 'max_depth': 4, 'subsample': 0.7979056413290758, 'colsample_bytree': 0.8746959751795712, 'gamma': 0.9435574851407004, 'reg_alpha': 0.43286520195755096, 'reg_lambda': 0.12283797418046359}. Best is trial 43 with value: 0.7696516625115956.\n",
      "[I 2025-03-13 00:46:11,881] Trial 48 finished with value: 0.768433695211039 and parameters: {'learning_rate': 0.010531454939709195, 'max_depth': 6, 'subsample': 0.756210398432022, 'colsample_bytree': 0.9727480406393548, 'gamma': 0.6823665607374646, 'reg_alpha': 0.701947150164616, 'reg_lambda': 0.2248684197708196}. Best is trial 43 with value: 0.7696516625115956.\n",
      "[I 2025-03-13 00:46:12,591] Trial 49 finished with value: 0.7627812282583488 and parameters: {'learning_rate': 0.051370251474164734, 'max_depth': 6, 'subsample': 0.8178769386085448, 'colsample_bytree': 0.8474241152041202, 'gamma': 0.8651047872182219, 'reg_alpha': 0.32240475407677616, 'reg_lambda': 0.028780689589449196}. Best is trial 43 with value: 0.7696516625115956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (With SMOTE): {'learning_rate': 0.010448007849115029, 'max_depth': 5, 'subsample': 0.7671801494534726, 'colsample_bytree': 0.8014523771518339, 'gamma': 0.784383417261917, 'reg_alpha': 0.5961087097696677, 'reg_lambda': 0.2172689724060965}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 00:46:13,226] A new study created in memory with name: no-name-c1f39914-8d78-4b37-ab59-b76f47e37ad9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance ---\n",
      "Metric               No SMOTE   With Regular SMOTE\n",
      "Accuracy             0.8218   0.7816\n",
      "ROC-AUC              0.7805   0.7697\n",
      "PR-AUC               0.5427   0.5272\n",
      "Recall (Sensitivity) 0.3556   0.5291\n",
      "F1                   0.4650   0.5134\n",
      "Specificity          0.9516   0.8518\n",
      "FP-Rate              0.0484   0.1482\n",
      "G-Mean               0.5817   0.6713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 00:46:13,649] Trial 0 finished with value: 0.7794380145524119 and parameters: {'learning_rate': 0.05877623479915064, 'max_depth': 4, 'subsample': 0.9182037552014629, 'colsample_bytree': 0.7424724220209391, 'gamma': 0.6668936595584265, 'reg_alpha': 0.45863905304892605, 'reg_lambda': 0.36546334928933266}. Best is trial 0 with value: 0.7794380145524119.\n",
      "[I 2025-03-13 00:46:14,078] Trial 1 finished with value: 0.7788166381609462 and parameters: {'learning_rate': 0.021603889078995624, 'max_depth': 4, 'subsample': 0.8329710637190378, 'colsample_bytree': 0.8228019556312106, 'gamma': 0.9701677970425782, 'reg_alpha': 0.6944120579842806, 'reg_lambda': 0.1041478084684766}. Best is trial 0 with value: 0.7794380145524119.\n",
      "[I 2025-03-13 00:46:14,506] Trial 2 finished with value: 0.7708765146683673 and parameters: {'learning_rate': 0.1633129741915907, 'max_depth': 4, 'subsample': 0.7614380227720041, 'colsample_bytree': 0.6189159570090189, 'gamma': 0.24508241297376332, 'reg_alpha': 0.2757663488389862, 'reg_lambda': 0.8194589719273219}. Best is trial 0 with value: 0.7794380145524119.\n",
      "[I 2025-03-13 00:46:15,597] Trial 3 finished with value: 0.7574652858302412 and parameters: {'learning_rate': 0.16191681351571993, 'max_depth': 8, 'subsample': 0.8728227632785153, 'colsample_bytree': 0.7997768589572596, 'gamma': 0.05582282963620655, 'reg_alpha': 0.4206589466911461, 'reg_lambda': 0.38307172033483483}. Best is trial 0 with value: 0.7794380145524119.\n",
      "[I 2025-03-13 00:46:16,997] Trial 4 finished with value: 0.7664561108534322 and parameters: {'learning_rate': 0.11112147753533888, 'max_depth': 9, 'subsample': 0.9909488750439057, 'colsample_bytree': 0.9597310202589616, 'gamma': 0.1330497275921676, 'reg_alpha': 0.5780407457418396, 'reg_lambda': 0.9198255830713002}. Best is trial 0 with value: 0.7794380145524119.\n",
      "[I 2025-03-13 00:46:17,650] Trial 5 finished with value: 0.7809128594619666 and parameters: {'learning_rate': 0.019637241170928747, 'max_depth': 6, 'subsample': 0.7492742171276892, 'colsample_bytree': 0.713204192215958, 'gamma': 0.27849864691005444, 'reg_alpha': 0.6489156504242536, 'reg_lambda': 0.2106809204925585}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:18,079] Trial 6 finished with value: 0.773669338474026 and parameters: {'learning_rate': 0.12671351950647863, 'max_depth': 4, 'subsample': 0.8087480693345195, 'colsample_bytree': 0.7012994110656996, 'gamma': 0.44496282792983366, 'reg_alpha': 0.8771991568067344, 'reg_lambda': 0.9248067013310869}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:19,438] Trial 7 finished with value: 0.7653526495825602 and parameters: {'learning_rate': 0.07356680120508369, 'max_depth': 9, 'subsample': 0.8699907230478279, 'colsample_bytree': 0.7350116645198446, 'gamma': 0.1987660917771552, 'reg_alpha': 0.4326851868047846, 'reg_lambda': 0.47134691452312716}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:21,196] Trial 8 finished with value: 0.7684662352156771 and parameters: {'learning_rate': 0.04301947543850567, 'max_depth': 10, 'subsample': 0.6350090737473343, 'colsample_bytree': 0.7813764060945199, 'gamma': 0.2864326714923153, 'reg_alpha': 0.5467736857445062, 'reg_lambda': 0.4138167818030678}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:21,545] Trial 9 finished with value: 0.767159308325603 and parameters: {'learning_rate': 0.2847335461187877, 'max_depth': 3, 'subsample': 0.7258866452891769, 'colsample_bytree': 0.8940152987260515, 'gamma': 0.3692141523588909, 'reg_alpha': 0.04841592712052267, 'reg_lambda': 0.6623952915233757}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:22,229] Trial 10 finished with value: 0.7523577371289425 and parameters: {'learning_rate': 0.2244468353180989, 'max_depth': 6, 'subsample': 0.6567173688075028, 'colsample_bytree': 0.6026184047971647, 'gamma': 0.6629350418891624, 'reg_alpha': 0.980923379911928, 'reg_lambda': 0.046295060760329076}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:22,937] Trial 11 finished with value: 0.7783013610273655 and parameters: {'learning_rate': 0.012027590019240558, 'max_depth': 6, 'subsample': 0.9798029162581892, 'colsample_bytree': 0.6925433825012072, 'gamma': 0.6347990086553438, 'reg_alpha': 0.22432441507844791, 'reg_lambda': 0.28298255946938533}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:23,753] Trial 12 finished with value: 0.7732558122680891 and parameters: {'learning_rate': 0.07354848058172855, 'max_depth': 7, 'subsample': 0.9144683248537948, 'colsample_bytree': 0.6703155907541837, 'gamma': 0.6938532183454507, 'reg_alpha': 0.7376189926431923, 'reg_lambda': 0.2154120770660178}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:24,322] Trial 13 finished with value: 0.7767153438079777 and parameters: {'learning_rate': 0.06694872533577004, 'max_depth': 5, 'subsample': 0.7047500243014774, 'colsample_bytree': 0.756858756363619, 'gamma': 0.861002121607312, 'reg_alpha': 0.7484968544914024, 'reg_lambda': 0.576560546376318}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:24,897] Trial 14 finished with value: 0.771465894596475 and parameters: {'learning_rate': 0.1217574044251266, 'max_depth': 5, 'subsample': 0.9331459732653471, 'colsample_bytree': 0.8505276942247181, 'gamma': 0.5287917213921179, 'reg_alpha': 0.28057591522767467, 'reg_lambda': 0.2305115572379992}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:25,298] Trial 15 finished with value: 0.7733823124420223 and parameters: {'learning_rate': 0.21064319797216818, 'max_depth': 3, 'subsample': 0.766611162381193, 'colsample_bytree': 0.6566689057300676, 'gamma': 0.8136424557693763, 'reg_alpha': 0.6007859710064402, 'reg_lambda': 0.3289381826827907}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:26,185] Trial 16 finished with value: 0.7758355316558441 and parameters: {'learning_rate': 0.04587731641185186, 'max_depth': 7, 'subsample': 0.709107400194297, 'colsample_bytree': 0.7281536280718973, 'gamma': 0.49626107611576326, 'reg_alpha': 0.41870950281545377, 'reg_lambda': 0.12421855736349441}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:26,778] Trial 17 finished with value: 0.7778954443993507 and parameters: {'learning_rate': 0.0110343158947524, 'max_depth': 5, 'subsample': 0.6069732279824164, 'colsample_bytree': 0.8797721490455914, 'gamma': 0.0156534617860844, 'reg_alpha': 0.08903687228528612, 'reg_lambda': 0.5851550174318773}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:27,488] Trial 18 finished with value: 0.7675846112592764 and parameters: {'learning_rate': 0.09544668124120127, 'max_depth': 6, 'subsample': 0.8369040186518809, 'colsample_bytree': 0.9920991223777595, 'gamma': 0.36166904263585853, 'reg_alpha': 0.8349580116923551, 'reg_lambda': 0.17313858336653629}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:28,564] Trial 19 finished with value: 0.7747442819457327 and parameters: {'learning_rate': 0.042310558667899324, 'max_depth': 8, 'subsample': 0.9232088157568556, 'colsample_bytree': 0.7672767319117856, 'gamma': 0.5589731937723902, 'reg_alpha': 0.6491074545110432, 'reg_lambda': 0.01620648771805977}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:29,107] Trial 20 finished with value: 0.7766606273191095 and parameters: {'learning_rate': 0.0887693200225844, 'max_depth': 5, 'subsample': 0.7754819363169991, 'colsample_bytree': 0.6463317163084996, 'gamma': 0.7515569865831566, 'reg_alpha': 0.48198014536808065, 'reg_lambda': 0.7294088337213032}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:29,582] Trial 21 finished with value: 0.7747520364679963 and parameters: {'learning_rate': 0.010705175039631004, 'max_depth': 4, 'subsample': 0.8191166621673536, 'colsample_bytree': 0.815781328339086, 'gamma': 0.9699196146556872, 'reg_alpha': 0.688119439630697, 'reg_lambda': 0.105235185747629}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:29,973] Trial 22 finished with value: 0.779068623898423 and parameters: {'learning_rate': 0.04110439552328815, 'max_depth': 3, 'subsample': 0.8541822219447541, 'colsample_bytree': 0.7370772194614208, 'gamma': 0.9840412805484569, 'reg_alpha': 0.8186008706565219, 'reg_lambda': 0.300197658229014}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:30,377] Trial 23 finished with value: 0.7804683514030613 and parameters: {'learning_rate': 0.04936729671805004, 'max_depth': 3, 'subsample': 0.871136248775319, 'colsample_bytree': 0.7267820039159614, 'gamma': 0.7857232867898485, 'reg_alpha': 0.8497337448514847, 'reg_lambda': 0.3261681885490151}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:30,777] Trial 24 finished with value: 0.7793647451878479 and parameters: {'learning_rate': 0.05784719213209273, 'max_depth': 3, 'subsample': 0.893667722579835, 'colsample_bytree': 0.7049624030630126, 'gamma': 0.8570841935385656, 'reg_alpha': 0.9444832050902567, 'reg_lambda': 0.47268957283392377}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:31,225] Trial 25 finished with value: 0.7736066500463822 and parameters: {'learning_rate': 0.14494090645479396, 'max_depth': 4, 'subsample': 0.9589697119060618, 'colsample_bytree': 0.6851712144799497, 'gamma': 0.7668961570295243, 'reg_alpha': 0.3489400097519169, 'reg_lambda': 0.32602268010525526}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:31,610] Trial 26 finished with value: 0.778806165932282 and parameters: {'learning_rate': 0.09353714810235335, 'max_depth': 3, 'subsample': 0.8962922841313004, 'colsample_bytree': 0.7222293553404185, 'gamma': 0.6336699298988073, 'reg_alpha': 0.7752528130014461, 'reg_lambda': 0.3868795995076889}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:32,180] Trial 27 finished with value: 0.7807306644248608 and parameters: {'learning_rate': 0.03268822669943926, 'max_depth': 5, 'subsample': 0.9535951863064832, 'colsample_bytree': 0.6410594032986943, 'gamma': 0.41854139473394636, 'reg_alpha': 0.8835408470837781, 'reg_lambda': 0.532794367520902}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:33,019] Trial 28 finished with value: 0.777907619724026 and parameters: {'learning_rate': 0.030372509652634567, 'max_depth': 7, 'subsample': 0.9565003807192403, 'colsample_bytree': 0.6419936568115989, 'gamma': 0.41282201046176203, 'reg_alpha': 0.8893450877969737, 'reg_lambda': 0.5511106942525664}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:33,675] Trial 29 finished with value: 0.7759162656539889 and parameters: {'learning_rate': 0.061769533456680396, 'max_depth': 6, 'subsample': 0.7905310545893068, 'colsample_bytree': 0.6189573603821861, 'gamma': 0.26438704952158, 'reg_alpha': 0.9343719455357598, 'reg_lambda': 0.6671886495243744}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:34,214] Trial 30 finished with value: 0.7589522698283859 and parameters: {'learning_rate': 0.18101582588884257, 'max_depth': 5, 'subsample': 0.7434449417896505, 'colsample_bytree': 0.6615131270198921, 'gamma': 0.340181115133264, 'reg_alpha': 0.9974122756954241, 'reg_lambda': 0.4334903900282368}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:34,666] Trial 31 finished with value: 0.78028822182282 and parameters: {'learning_rate': 0.028181063037537354, 'max_depth': 4, 'subsample': 0.9347542751574414, 'colsample_bytree': 0.7548100519685854, 'gamma': 0.5637785061232149, 'reg_alpha': 0.655077353682765, 'reg_lambda': 0.23433254727430564}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:35,111] Trial 32 finished with value: 0.7802326356679035 and parameters: {'learning_rate': 0.0297262415568791, 'max_depth': 4, 'subsample': 0.9423038463544577, 'colsample_bytree': 0.774961051032953, 'gamma': 0.5703447820576499, 'reg_alpha': 0.617854152824478, 'reg_lambda': 0.25089839778275624}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:35,559] Trial 33 finished with value: 0.7800545353084416 and parameters: {'learning_rate': 0.025210126094130092, 'max_depth': 4, 'subsample': 0.8947563261270504, 'colsample_bytree': 0.7138425762294225, 'gamma': 0.46399042400305335, 'reg_alpha': 0.6893104625362866, 'reg_lambda': 0.1709506647140475}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:36,095] Trial 34 finished with value: 0.7808430687615955 and parameters: {'learning_rate': 0.0524984117339105, 'max_depth': 5, 'subsample': 0.9745061597013853, 'colsample_bytree': 0.7997037604020057, 'gamma': 0.15148336735165602, 'reg_alpha': 0.8307150122301837, 'reg_lambda': 0.5119290594054593}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:36,756] Trial 35 finished with value: 0.7755250971127088 and parameters: {'learning_rate': 0.08121930812079496, 'max_depth': 6, 'subsample': 0.9744525082293684, 'colsample_bytree': 0.8286715389218948, 'gamma': 0.1576583508359521, 'reg_alpha': 0.8157890245663306, 'reg_lambda': 0.5282183615955673}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:37,289] Trial 36 finished with value: 0.777763508812616 and parameters: {'learning_rate': 0.05558002154650918, 'max_depth': 5, 'subsample': 0.996234138093134, 'colsample_bytree': 0.7945725303942835, 'gamma': 0.0725202795398171, 'reg_alpha': 0.890337444308405, 'reg_lambda': 0.6322240236506205}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:38,306] Trial 37 finished with value: 0.7630805383232838 and parameters: {'learning_rate': 0.10503842700448757, 'max_depth': 8, 'subsample': 0.8420314445605052, 'colsample_bytree': 0.6323785113692768, 'gamma': 0.19550207141348303, 'reg_alpha': 0.7865673136405256, 'reg_lambda': 0.8494283255937607}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:39,155] Trial 38 finished with value: 0.7624800701530612 and parameters: {'learning_rate': 0.13761385468884857, 'max_depth': 7, 'subsample': 0.8753302827392277, 'colsample_bytree': 0.6765347835100355, 'gamma': 0.2989052940275299, 'reg_alpha': 0.8615508281417469, 'reg_lambda': 0.4966801750305228}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:39,703] Trial 39 finished with value: 0.758795077690167 and parameters: {'learning_rate': 0.2848199015968754, 'max_depth': 5, 'subsample': 0.9634289619751124, 'colsample_bytree': 0.9204121640030284, 'gamma': 0.09920070215874491, 'reg_alpha': 0.7214487594080186, 'reg_lambda': 0.7616751800742225}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:40,380] Trial 40 finished with value: 0.7781816007653062 and parameters: {'learning_rate': 0.0514185604909886, 'max_depth': 6, 'subsample': 0.6712793299383941, 'colsample_bytree': 0.8413850897783146, 'gamma': 0.20177690525340294, 'reg_alpha': 0.5389974367674272, 'reg_lambda': 0.45233324400839675}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:40,856] Trial 41 finished with value: 0.7802521669179034 and parameters: {'learning_rate': 0.030651626939979013, 'max_depth': 4, 'subsample': 0.932012169208389, 'colsample_bytree': 0.748896133348794, 'gamma': 0.4175379717396551, 'reg_alpha': 0.6640383821315432, 'reg_lambda': 0.3623666000049721}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:41,322] Trial 42 finished with value: 0.7793436557861781 and parameters: {'learning_rate': 0.021338599477255672, 'max_depth': 4, 'subsample': 0.9112870922952615, 'colsample_bytree': 0.8065514857330889, 'gamma': 0.23516261902293478, 'reg_alpha': 0.9392789574022787, 'reg_lambda': 0.18075244110070127}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:41,736] Trial 43 finished with value: 0.7786129188891466 and parameters: {'learning_rate': 0.036328882087794376, 'max_depth': 3, 'subsample': 0.9497650523642187, 'colsample_bytree': 0.7870231182635976, 'gamma': 0.3126423167623958, 'reg_alpha': 0.5522745499472567, 'reg_lambda': 0.270298135168626}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:42,285] Trial 44 finished with value: 0.7773985751971244 and parameters: {'learning_rate': 0.07705787221660776, 'max_depth': 5, 'subsample': 0.9980657602397177, 'colsample_bytree': 0.7617340229174601, 'gamma': 0.3811895339095782, 'reg_alpha': 0.77178448872239, 'reg_lambda': 0.3694945492277755}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:42,693] Trial 45 finished with value: 0.7795582096474953 and parameters: {'learning_rate': 0.06804937885758564, 'max_depth': 3, 'subsample': 0.9766570356932385, 'colsample_bytree': 0.6957301023852567, 'gamma': 0.15563097535374748, 'reg_alpha': 0.7236092383800751, 'reg_lambda': 0.9875201297673692}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:43,151] Trial 46 finished with value: 0.7780781467416513 and parameters: {'learning_rate': 0.01879908642182609, 'max_depth': 4, 'subsample': 0.8694182634990784, 'colsample_bytree': 0.7521132371281171, 'gamma': 0.5822877609485596, 'reg_alpha': 0.9007474536377553, 'reg_lambda': 0.4128077932576911}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:43,676] Trial 47 finished with value: 0.7589976736433209 and parameters: {'learning_rate': 0.24364814463039872, 'max_depth': 5, 'subsample': 0.8024873914083597, 'colsample_bytree': 0.7192570289282388, 'gamma': 0.9257479807590687, 'reg_alpha': 0.8444697060144328, 'reg_lambda': 0.05193440220515391}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:44,126] Trial 48 finished with value: 0.7784227881493507 and parameters: {'learning_rate': 0.04978649408350968, 'max_depth': 4, 'subsample': 0.7420725924972921, 'colsample_bytree': 0.8717030114511145, 'gamma': 0.46974706296839397, 'reg_alpha': 0.624795454879163, 'reg_lambda': 0.6244600706840465}. Best is trial 5 with value: 0.7809128594619666.\n",
      "[I 2025-03-13 00:46:44,773] Trial 49 finished with value: 0.7792697704081633 and parameters: {'learning_rate': 0.03658358003386182, 'max_depth': 6, 'subsample': 0.9137632837839553, 'colsample_bytree': 0.6048047832063391, 'gamma': 0.7073519782400058, 'reg_alpha': 0.5153287651314422, 'reg_lambda': 0.5067738667794714}. Best is trial 5 with value: 0.7809128594619666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (No SMOTE): {'learning_rate': 0.019637241170928747, 'max_depth': 6, 'subsample': 0.7492742171276892, 'colsample_bytree': 0.713204192215958, 'gamma': 0.27849864691005444, 'reg_alpha': 0.6489156504242536, 'reg_lambda': 0.2106809204925585}\n",
      "14028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 14028/14028 [00:56<00:00, 247.43it/s]\n",
      "[I 2025-03-13 00:47:42,217] A new study created in memory with name: no-name-1525bb92-fdd6-45f4-a2b1-f2ae8de3bb12\n",
      "[I 2025-03-13 00:47:43,444] Trial 0 finished with value: 0.7448773046150278 and parameters: {'learning_rate': 0.20075005694944115, 'max_depth': 8, 'subsample': 0.8660751646270958, 'colsample_bytree': 0.8906098113009548, 'gamma': 0.21266880437132518, 'reg_alpha': 0.45895279370173747, 'reg_lambda': 0.7918204184793575}. Best is trial 0 with value: 0.7448773046150278.\n",
      "[I 2025-03-13 00:47:44,075] Trial 1 finished with value: 0.7515789874188311 and parameters: {'learning_rate': 0.2066000288033237, 'max_depth': 6, 'subsample': 0.9796395478986142, 'colsample_bytree': 0.9527873506418316, 'gamma': 0.062010815490333715, 'reg_alpha': 0.5853472831391942, 'reg_lambda': 0.31463824831779263}. Best is trial 1 with value: 0.7515789874188311.\n",
      "[I 2025-03-13 00:47:44,438] Trial 2 finished with value: 0.7565830820964748 and parameters: {'learning_rate': 0.19718944432179367, 'max_depth': 3, 'subsample': 0.6107177997000789, 'colsample_bytree': 0.9250252549890448, 'gamma': 0.9361460595419033, 'reg_alpha': 0.4275595498632684, 'reg_lambda': 0.12951891846089714}. Best is trial 2 with value: 0.7565830820964748.\n",
      "[I 2025-03-13 00:47:46,175] Trial 3 finished with value: 0.7597169237012986 and parameters: {'learning_rate': 0.03242311998765357, 'max_depth': 10, 'subsample': 0.7324028109946139, 'colsample_bytree': 0.904225787880294, 'gamma': 0.8441038613746975, 'reg_alpha': 0.3373936158185442, 'reg_lambda': 0.9985065857004641}. Best is trial 3 with value: 0.7597169237012986.\n",
      "[I 2025-03-13 00:47:46,840] Trial 4 finished with value: 0.7445800600069573 and parameters: {'learning_rate': 0.28310510098740704, 'max_depth': 9, 'subsample': 0.9340744469169194, 'colsample_bytree': 0.697343877161017, 'gamma': 0.7734455637661104, 'reg_alpha': 0.4987219977600109, 'reg_lambda': 0.8096060117420452}. Best is trial 3 with value: 0.7597169237012986.\n",
      "[I 2025-03-13 00:47:47,260] Trial 5 finished with value: 0.7592191485969387 and parameters: {'learning_rate': 0.2041371209251037, 'max_depth': 4, 'subsample': 0.790313410081136, 'colsample_bytree': 0.9042443365003506, 'gamma': 0.798394286413974, 'reg_alpha': 0.6320815257948299, 'reg_lambda': 0.6076348116243064}. Best is trial 3 with value: 0.7597169237012986.\n",
      "[I 2025-03-13 00:47:47,764] Trial 6 finished with value: 0.7587246347402596 and parameters: {'learning_rate': 0.13567062314489714, 'max_depth': 5, 'subsample': 0.7933451062713192, 'colsample_bytree': 0.705815774643969, 'gamma': 0.10754753691919261, 'reg_alpha': 0.38524895885999333, 'reg_lambda': 0.49959774829011283}. Best is trial 3 with value: 0.7597169237012986.\n",
      "[I 2025-03-13 00:47:48,414] Trial 7 finished with value: 0.7400233722750463 and parameters: {'learning_rate': 0.2882494614718021, 'max_depth': 6, 'subsample': 0.7470617389205353, 'colsample_bytree': 0.9906844710558924, 'gamma': 0.31989945743279213, 'reg_alpha': 0.729878736649599, 'reg_lambda': 0.8988966318341765}. Best is trial 3 with value: 0.7597169237012986.\n",
      "[I 2025-03-13 00:47:49,438] Trial 8 finished with value: 0.7612730823863637 and parameters: {'learning_rate': 0.027309391201689515, 'max_depth': 8, 'subsample': 0.7506807129754006, 'colsample_bytree': 0.8731737350048698, 'gamma': 0.909397109281709, 'reg_alpha': 0.6349030969068319, 'reg_lambda': 0.6578188968933022}. Best is trial 8 with value: 0.7612730823863637.\n",
      "[I 2025-03-13 00:47:50,492] Trial 9 finished with value: 0.7473762175324674 and parameters: {'learning_rate': 0.18628850625845758, 'max_depth': 8, 'subsample': 0.8682337830954248, 'colsample_bytree': 0.9765684189800078, 'gamma': 0.08959814746612915, 'reg_alpha': 0.5974934698841162, 'reg_lambda': 0.3729644346229076}. Best is trial 8 with value: 0.7612730823863637.\n",
      "[I 2025-03-13 00:47:51,612] Trial 10 finished with value: 0.7650837053571429 and parameters: {'learning_rate': 0.012540925758732436, 'max_depth': 8, 'subsample': 0.6426915423662479, 'colsample_bytree': 0.794811740171699, 'gamma': 0.5878736420508428, 'reg_alpha': 0.9607673058127588, 'reg_lambda': 0.03256858111957117}. Best is trial 10 with value: 0.7650837053571429.\n",
      "[I 2025-03-13 00:47:52,694] Trial 11 finished with value: 0.7600207270408164 and parameters: {'learning_rate': 0.030987026250302682, 'max_depth': 8, 'subsample': 0.6357153328631554, 'colsample_bytree': 0.802835804342803, 'gamma': 0.5757478083952552, 'reg_alpha': 0.9642163927004881, 'reg_lambda': 0.0007713747167140753}. Best is trial 10 with value: 0.7650837053571429.\n",
      "[I 2025-03-13 00:47:53,505] Trial 12 finished with value: 0.757732889320501 and parameters: {'learning_rate': 0.08169351773010718, 'max_depth': 7, 'subsample': 0.6818439601704416, 'colsample_bytree': 0.8075908549349935, 'gamma': 0.5854073039480282, 'reg_alpha': 0.0437507259994947, 'reg_lambda': 0.6305511564798995}. Best is trial 10 with value: 0.7650837053571429.\n",
      "[I 2025-03-13 00:47:55,030] Trial 13 finished with value: 0.7541528727968461 and parameters: {'learning_rate': 0.08263387326540203, 'max_depth': 10, 'subsample': 0.6909552639162031, 'colsample_bytree': 0.7511053899640605, 'gamma': 0.38208398860986403, 'reg_alpha': 0.9663169518403859, 'reg_lambda': 0.2543086946970126}. Best is trial 10 with value: 0.7650837053571429.\n",
      "[I 2025-03-13 00:47:55,892] Trial 14 finished with value: 0.7662033641581633 and parameters: {'learning_rate': 0.01451944949434128, 'max_depth': 7, 'subsample': 0.6616753298887685, 'colsample_bytree': 0.8471183010128744, 'gamma': 0.6770533521029003, 'reg_alpha': 0.8175490297506218, 'reg_lambda': 0.5385427229450648}. Best is trial 14 with value: 0.7662033641581633.\n",
      "[I 2025-03-13 00:47:56,686] Trial 15 finished with value: 0.7574453197472171 and parameters: {'learning_rate': 0.07946035982856517, 'max_depth': 7, 'subsample': 0.6603883417965194, 'colsample_bytree': 0.6078927359393113, 'gamma': 0.6727900296937881, 'reg_alpha': 0.8439224582650429, 'reg_lambda': 0.4978685230195401}. Best is trial 14 with value: 0.7662033641581633.\n",
      "[I 2025-03-13 00:47:57,238] Trial 16 finished with value: 0.7588053325023192 and parameters: {'learning_rate': 0.11822655619674347, 'max_depth': 5, 'subsample': 0.6077866977838782, 'colsample_bytree': 0.8386211551166293, 'gamma': 0.48520890356464363, 'reg_alpha': 0.8112571182789685, 'reg_lambda': 0.14462374135467032}. Best is trial 14 with value: 0.7662033641581633.\n",
      "[I 2025-03-13 00:47:58,504] Trial 17 finished with value: 0.7575188427643784 and parameters: {'learning_rate': 0.06135520894271766, 'max_depth': 9, 'subsample': 0.7126901288308273, 'colsample_bytree': 0.739801908945907, 'gamma': 0.6801874106161813, 'reg_alpha': 0.8456061718912254, 'reg_lambda': 0.4005307940481049}. Best is trial 14 with value: 0.7662033641581633.\n",
      "[I 2025-03-13 00:47:59,347] Trial 18 finished with value: 0.7673617230983303 and parameters: {'learning_rate': 0.010182322099186234, 'max_depth': 7, 'subsample': 0.6584985306347424, 'colsample_bytree': 0.8398310548010428, 'gamma': 0.45862192702927573, 'reg_alpha': 0.9913301295621808, 'reg_lambda': 0.0010727495144577937}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:47:59,897] Trial 19 finished with value: 0.7590668120941558 and parameters: {'learning_rate': 0.11809415141773819, 'max_depth': 5, 'subsample': 0.8338695222219552, 'colsample_bytree': 0.8514089455946507, 'gamma': 0.4041972933089302, 'reg_alpha': 0.2495300347341065, 'reg_lambda': 0.23186430001322245}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:00,531] Trial 20 finished with value: 0.760763095721243 and parameters: {'learning_rate': 0.04634388441329752, 'max_depth': 6, 'subsample': 0.6838439853735222, 'colsample_bytree': 0.600292584286064, 'gamma': 0.2560681883604967, 'reg_alpha': 0.7452470109833136, 'reg_lambda': 0.725750483929563}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:01,400] Trial 21 finished with value: 0.7648386044758813 and parameters: {'learning_rate': 0.01627339948228912, 'max_depth': 7, 'subsample': 0.6467213317411953, 'colsample_bytree': 0.770186483649988, 'gamma': 0.5037583983574712, 'reg_alpha': 0.9949552267871842, 'reg_lambda': 0.00021616991803726643}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:02,938] Trial 22 finished with value: 0.7635261697008349 and parameters: {'learning_rate': 0.01439187700959399, 'max_depth': 9, 'subsample': 0.6424999996059495, 'colsample_bytree': 0.8332831930676016, 'gamma': 0.6795043696647863, 'reg_alpha': 0.8965264211457395, 'reg_lambda': 0.10801286061007134}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:03,758] Trial 23 finished with value: 0.7606472127203154 and parameters: {'learning_rate': 0.05516990575900891, 'max_depth': 7, 'subsample': 0.6058398659989196, 'colsample_bytree': 0.7835316946631636, 'gamma': 0.5539805567404266, 'reg_alpha': 0.7599646920381716, 'reg_lambda': 0.07388438068905279}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:04,898] Trial 24 finished with value: 0.7656417773075139 and parameters: {'learning_rate': 0.012569666838395065, 'max_depth': 8, 'subsample': 0.7131014246027473, 'colsample_bytree': 0.81779028168932, 'gamma': 0.4259106389985483, 'reg_alpha': 0.8938752407614631, 'reg_lambda': 0.21534001139301756}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:05,582] Trial 25 finished with value: 0.7455645944457329 and parameters: {'learning_rate': 0.24619609424012776, 'max_depth': 6, 'subsample': 0.7205538419276547, 'colsample_bytree': 0.858023573244285, 'gamma': 0.434718465433165, 'reg_alpha': 0.8665605130866296, 'reg_lambda': 0.21613274557264617}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:06,887] Trial 26 finished with value: 0.7579890059717068 and parameters: {'learning_rate': 0.10333081997133271, 'max_depth': 9, 'subsample': 0.7747003361733612, 'colsample_bytree': 0.8228902041396307, 'gamma': 0.2984171212462303, 'reg_alpha': 0.685385817448878, 'reg_lambda': 0.4289547201084388}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:07,692] Trial 27 finished with value: 0.7602567689007422 and parameters: {'learning_rate': 0.049600934357164145, 'max_depth': 7, 'subsample': 0.6901858242659559, 'colsample_bytree': 0.929954414838418, 'gamma': 0.3575012613601824, 'reg_alpha': 0.9090172626657872, 'reg_lambda': 0.564669310201069}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:08,684] Trial 28 finished with value: 0.7561318703617811 and parameters: {'learning_rate': 0.07492299806782537, 'max_depth': 8, 'subsample': 0.7605495663169803, 'colsample_bytree': 0.701257704330986, 'gamma': 0.16370399927888762, 'reg_alpha': 0.7997481347222832, 'reg_lambda': 0.19347239450365558}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:09,776] Trial 29 finished with value: 0.7487551455241188 and parameters: {'learning_rate': 0.1715313362353322, 'max_depth': 8, 'subsample': 0.8288458546028599, 'colsample_bytree': 0.8781610267366783, 'gamma': 0.2252862265504263, 'reg_alpha': 0.9084525634662164, 'reg_lambda': 0.32584110433237634}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:10,577] Trial 30 finished with value: 0.7611776727736549 and parameters: {'learning_rate': 0.040199221732191696, 'max_depth': 7, 'subsample': 0.7114902320647022, 'colsample_bytree': 0.6521702689880506, 'gamma': 0.46715218179754914, 'reg_alpha': 0.02947778151259539, 'reg_lambda': 0.44937901463854185}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:12,071] Trial 31 finished with value: 0.7639796643089054 and parameters: {'learning_rate': 0.011623097210402306, 'max_depth': 9, 'subsample': 0.6613166995971202, 'colsample_bytree': 0.7767755042910504, 'gamma': 0.6299503558999955, 'reg_alpha': 0.9994959376599087, 'reg_lambda': 0.0551013771449792}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:13,252] Trial 32 finished with value: 0.7657769016697589 and parameters: {'learning_rate': 0.01088188854988662, 'max_depth': 8, 'subsample': 0.6258852225220766, 'colsample_bytree': 0.8087116634287778, 'gamma': 0.7479003979000028, 'reg_alpha': 0.9305324163978874, 'reg_lambda': 0.04419558376813447}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:13,902] Trial 33 finished with value: 0.7595889740839518 and parameters: {'learning_rate': 0.06543089902294354, 'max_depth': 6, 'subsample': 0.6188655725217173, 'colsample_bytree': 0.8260442761854846, 'gamma': 0.7331026291473527, 'reg_alpha': 0.9150265312479668, 'reg_lambda': 0.1561328698807677}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:14,905] Trial 34 finished with value: 0.7589860418599258 and parameters: {'learning_rate': 0.040396047807576205, 'max_depth': 8, 'subsample': 0.6669949625158376, 'colsample_bytree': 0.8929491553013562, 'gamma': 0.8733612950029583, 'reg_alpha': 0.796902236336776, 'reg_lambda': 0.3199828271970915}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:15,730] Trial 35 finished with value: 0.761921237244898 and parameters: {'learning_rate': 0.029249246346644456, 'max_depth': 7, 'subsample': 0.627163203625369, 'colsample_bytree': 0.7453813155443192, 'gamma': 0.9828672838471024, 'reg_alpha': 0.5259028247896579, 'reg_lambda': 0.10505678388900463}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:17,752] Trial 36 finished with value: 0.7586434659090909 and parameters: {'learning_rate': 0.02378302840364182, 'max_depth': 10, 'subsample': 0.9436001654365442, 'colsample_bytree': 0.9240476898008584, 'gamma': 0.750795617476084, 'reg_alpha': 0.6764955689538805, 'reg_lambda': 0.282120740002024}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:18,439] Trial 37 finished with value: 0.7417294034090909 and parameters: {'learning_rate': 0.24306655819296225, 'max_depth': 6, 'subsample': 0.7276778483787085, 'colsample_bytree': 0.8565186391124154, 'gamma': 0.8016406551310054, 'reg_alpha': 0.9298203327955705, 'reg_lambda': 0.15885716736229066}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:19,726] Trial 38 finished with value: 0.7517739375579777 and parameters: {'learning_rate': 0.09840444832705852, 'max_depth': 9, 'subsample': 0.7055094296186212, 'colsample_bytree': 0.8187395964126143, 'gamma': 0.5100744304542404, 'reg_alpha': 0.8523751722849531, 'reg_lambda': 0.07113724387564418}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:20,191] Trial 39 finished with value: 0.7582007696544527 and parameters: {'learning_rate': 0.15650749709771963, 'max_depth': 4, 'subsample': 0.6674815793679769, 'colsample_bytree': 0.7260142603989828, 'gamma': 0.8466651172120161, 'reg_alpha': 0.23953211124148877, 'reg_lambda': 0.5434572620029032}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:21,234] Trial 40 finished with value: 0.7573980678919295 and parameters: {'learning_rate': 0.038893358864159885, 'max_depth': 8, 'subsample': 0.6257230151411515, 'colsample_bytree': 0.8761754113114781, 'gamma': 0.6352887885316533, 'reg_alpha': 0.6982899372754158, 'reg_lambda': 0.8015399133137078}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:22,361] Trial 41 finished with value: 0.7651973054846939 and parameters: {'learning_rate': 0.013522057138316468, 'max_depth': 8, 'subsample': 0.6463279578030983, 'colsample_bytree': 0.7921688332153337, 'gamma': 0.5376903391189806, 'reg_alpha': 0.9322075565141704, 'reg_lambda': 0.03251658301385183}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:23,224] Trial 42 finished with value: 0.765981526843692 and parameters: {'learning_rate': 0.01166602930422751, 'max_depth': 7, 'subsample': 0.6024551302905975, 'colsample_bytree': 0.7681804317308707, 'gamma': 0.4424742040688691, 'reg_alpha': 0.8845256947541454, 'reg_lambda': 0.035660241724774}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:24,050] Trial 43 finished with value: 0.7602825689935064 and parameters: {'learning_rate': 0.03146442196399942, 'max_depth': 7, 'subsample': 0.6081111995092237, 'colsample_bytree': 0.7648795034446132, 'gamma': 0.44008509111109667, 'reg_alpha': 0.7864336110896674, 'reg_lambda': 0.10498741517663193}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:24,849] Trial 44 finished with value: 0.7578308354591836 and parameters: {'learning_rate': 0.05951113891711845, 'max_depth': 7, 'subsample': 0.6000882123536416, 'colsample_bytree': 0.8128473890111104, 'gamma': 0.33915552953164696, 'reg_alpha': 0.875814317193297, 'reg_lambda': 0.686606471489593}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:25,514] Trial 45 finished with value: 0.7652900698631725 and parameters: {'learning_rate': 0.024954690226734896, 'max_depth': 6, 'subsample': 0.671136813531241, 'colsample_bytree': 0.8413568696134076, 'gamma': 0.407207720081516, 'reg_alpha': 0.9561923130863023, 'reg_lambda': 0.1767332128730149}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:26,546] Trial 46 finished with value: 0.7594653003246752 and parameters: {'learning_rate': 0.04772848557426081, 'max_depth': 8, 'subsample': 0.6983123049029577, 'colsample_bytree': 0.8044596319680407, 'gamma': 0.7341804966010438, 'reg_alpha': 0.9969218484007697, 'reg_lambda': 0.03344022314891837}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:27,609] Trial 47 finished with value: 0.7624677498840445 and parameters: {'learning_rate': 0.022768458339789785, 'max_depth': 8, 'subsample': 0.6327271318293911, 'colsample_bytree': 0.7213494776872508, 'gamma': 0.6165943294353281, 'reg_alpha': 0.8226934039507314, 'reg_lambda': 0.974817281706855}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:28,445] Trial 48 finished with value: 0.7586366897611316 and parameters: {'learning_rate': 0.06940965487400826, 'max_depth': 7, 'subsample': 0.7341849976953918, 'colsample_bytree': 0.9047089969885734, 'gamma': 0.2693993678521222, 'reg_alpha': 0.589201945149485, 'reg_lambda': 0.28036138946972844}. Best is trial 18 with value: 0.7673617230983303.\n",
      "[I 2025-03-13 00:48:29,128] Trial 49 finished with value: 0.7416030481794991 and parameters: {'learning_rate': 0.22552656642750904, 'max_depth': 6, 'subsample': 0.6526923955466759, 'colsample_bytree': 0.8678718044019051, 'gamma': 0.5285551824950869, 'reg_alpha': 0.5117021854767237, 'reg_lambda': 0.11469222038648838}. Best is trial 18 with value: 0.7673617230983303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (With Cubic Polynomial SMOTE): {'learning_rate': 0.010182322099186234, 'max_depth': 7, 'subsample': 0.6584985306347424, 'colsample_bytree': 0.8398310548010428, 'gamma': 0.45862192702927573, 'reg_alpha': 0.9913301295621808, 'reg_lambda': 0.0010727495144577937}\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No SMOTE   With Cubic Polynomial SMOTE\n",
      "Accuracy             0.8199   0.7717\n",
      "ROC-AUC              0.7809   0.7674\n",
      "PR-AUC               0.5444   0.5247\n",
      "Recall (Sensitivity) 0.3474   0.5439\n",
      "F1                   0.4566   0.5092\n",
      "Specificity          0.9514   0.8351\n",
      "FP-Rate              0.0486   0.1649\n",
      "G-Mean               0.5750   0.6739\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Taiwan Credit dataset: \\n\")\n",
    "output=model(X,y)\n",
    "output_2=model_smote_poly(X,y['Y'])\n",
    "output.append(output_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a47e546b-9e11-47d6-8bfd-635106eca9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b42e858-3353-4e65-80b3-e592f3ecc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance ---\n",
      "Metric                         No SMOTE   With Regular SMOTE   With Cubic Polynomial Interpolation SMOTE\n",
      "Accuracy                       0.822              0.782              0.772\n",
      "ROC-AUC                        0.781              0.770              0.767\n",
      "PR-AUC                         0.543              0.527              0.525\n",
      "Recall (Sensitivity)           0.356              0.529              0.544\n",
      "F1                             0.465              0.513              0.509\n",
      "Specificity                    0.952              0.852              0.835\n",
      "FP-Rate                        0.048              0.148              0.165\n",
      "G-Mean                         0.582              0.671              0.674\n"
     ]
    }
   ],
   "source": [
    "metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \n",
    "                \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "\n",
    "print(\"\\n--- Model Performance ---\")\n",
    "print(\"{:<30} {:<10} {:<20} {:<20}\".format(\"Metric\", \"No SMOTE\",\n",
    "                                            \"With Regular SMOTE\",\n",
    "                                            \"With Cubic Polynomial Interpolation SMOTE\"))\n",
    "\n",
    "for i in range(len(metric_names)):\n",
    "    print(\"{:<30} {:.3f}              {:.3f}              {:.3f}\".format(\n",
    "        metric_names[i], metrics[0][i], metrics[1][i], metrics[2][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cee6d-d1e1-4220-87c9-9cf80f387bf9",
   "metadata": {},
   "source": [
    "# **ADASYN: Adaptive Synthetic Sampling Approach with Cubic Polynomial Interpolation**\n",
    "\n",
    "## **1. Introduction**\n",
    "In classification tasks with imbalanced datasets, where the number of instances in the minority class is significantly lower than in the majority class, machine learning models tend to be biased towards the majority class. To address this, various oversampling techniques have been developed, including the **Adaptive Synthetic Sampling (ADASYN) algorithm**.  \n",
    "\n",
    "ADASYN improves upon traditional oversampling techniques, such as **SMOTE**, by adaptively generating synthetic samples according to the **local distribution** of the minority class. Specifically, it focuses on generating more synthetic samples in **harder-to-learn regions**, where the local class imbalance is more pronounced.  \n",
    "\n",
    "In this work, we **replace the standard linear interpolation method** used in ADASYN with **cubic polynomial interpolation**, which provides a smoother and more diverse distribution of synthetic samples in high-dimensional feature spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Algorithm Description**\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times m}$ represent the dataset, where $n$ is the number of samples, and $m$ is the number of features. Let the class labels be $y \\in \\{C_1, C_2\\}$, where $C_1$ is the minority class and $C_2$ is the majority class.\n",
    "\n",
    "### **Step 1: Define the Minority and Majority Classes**\n",
    "The number of instances in each class is computed as:\n",
    "\n",
    "$$\n",
    "n_{\\text{min}} = |X_{\\text{min}}|, \\quad n_{\\text{maj}} = |X_{\\text{maj}}|\n",
    "$$\n",
    "\n",
    "where $X_{\\text{min}}$ and $X_{\\text{maj}}$ represent the subsets of $X$ belonging to the minority and majority classes, respectively.\n",
    "\n",
    "The class imbalance ratio is then given by:\n",
    "\n",
    "$$\n",
    "d = \\frac{n_{\\text{min}}}{n_{\\text{maj}}}\n",
    "$$\n",
    "\n",
    "ADASYN aims to **balance the dataset** by generating synthetic samples until $d \\approx 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute the Number of Synthetic Samples**\n",
    "The total number of synthetic samples to be generated is:\n",
    "\n",
    "$$\n",
    "G = n_{\\text{maj}} - n_{\\text{min}}\\cdot\\tilde{β} \n",
    "$$\n",
    "\n",
    "Each minority sample $x_i$ is assigned a weight based on its difficulty of classification.Where β∈ [0, 1] is a parameter used to specify the desired\n",
    "balance level after generation of the synthetic data. β = 1\n",
    "means a fully balanced data set is created after the generalization process.\n",
    "\n",
    "For each $x_i \\in X_{\\text{min}}$, we compute the number of its $k$-nearest neighbors belonging to the majority class $X_{\\text{maj}}$. Let $k_i^{\\text{maj}}$ denote this count. The local distribution ratio $r_i$ is computed as:\n",
    "\n",
    "$$\n",
    "r_i = \\frac{k_i^{\\text{maj}}}{k}\n",
    "$$\n",
    "\n",
    "where $k$ is the total number of nearest neighbors considered.\n",
    "\n",
    "The normalized weight for each $x_i$ is then:\n",
    "\n",
    "$$\n",
    "\\tilde{r}_i = \\frac{r_i}{\\sum_{j=1}^{n_{\\text{min}}} r_j}\n",
    "$$\n",
    "\n",
    "The number of synthetic samples required for each $x_i$ is:\n",
    "\n",
    "$$\n",
    "G_i = G \\cdot \\tilde{r}_i\n",
    "$$\n",
    "\n",
    "where $G_i$ is an integer value indicating the number of new samples to generate for $x_i$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Generate Synthetic Samples Using Cubic Polynomial Interpolation**\n",
    "For each minority sample $x_i$ requiring $G_i$ synthetic samples, a random neighbor $x_j \\in X_{\\text{min}}$ from its $k$-nearest neighbors is selected.\n",
    "\n",
    "#### **Cubic Polynomial Interpolation**\n",
    "Instead of using linear interpolation, we apply cubic polynomial interpolation for smoother synthetic data generation.\n",
    "\n",
    "For each feature $f \\in \\{1, 2, \\dots, m\\}$, we define **four control points**:\n",
    "- $P_0 = x_{i,f}$ (original sample)\n",
    "- $P_1 = 0.5 (x_{i,f} + x_{j,f})$ (midpoint control)\n",
    "- $P_2 = 0.5 (x_{i,f} + x_{j,f})$ (another midpoint control)\n",
    "- $P_3 = x_{j,f}$ (selected neighbor)\n",
    "\n",
    "The corresponding interpolation domain values are:\n",
    "\n",
    "$$\n",
    "X_{\\text{points}} = [0, 0.33, 0.67, 1]\n",
    "$$\n",
    "\n",
    "The values at these points are:\n",
    "\n",
    "$$\n",
    "Y_{\\text{points}} = [P_0, P_1, P_2, P_3]\n",
    "$$\n",
    "\n",
    "Using **CubicSpline interpolation**, we generate a synthetic sample by selecting a random interpolation coefficient $g \\sim U(0,1)$ and computing:\n",
    "\n",
    "$$\n",
    "\\tilde{x}_{f} = \\text{CubicSpline}(g)\n",
    "$$\n",
    "\n",
    "This process is repeated for all $m$ features, resulting in a synthetic sample $\\tilde{x}$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Update the Dataset**\n",
    "The newly generated synthetic samples $\\tilde{X}$ are added to the original dataset:\n",
    "\n",
    "$$\n",
    "X' = X \\cup \\tilde{X}, \\quad y' = y \\cup \\tilde{y}\n",
    "$$\n",
    "\n",
    "where $\\tilde{y}$ contains the label of the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Conclusion**\n",
    "The proposed ADASYN implementation with cubic polynomial interpolation provides several advantages over standard linear interpolation methods:\n",
    "- **Enhanced diversity of synthetic samples**: The cubic interpolation technique generates smoother and more naturally distributed synthetic points in the feature space.\n",
    "- **Better generalization**: By adapting sample generation based on difficulty, ADASYN reduces the risk of overfitting caused by naive oversampling.\n",
    "- **Improved robustness in high-dimensional spaces**: Unlike linear interpolation, cubic interpolation mitigates abrupt transitions in feature values, making the synthetic data more realistic.\n",
    "\n",
    "This approach is particularly beneficial for imbalanced datasets where minority class samples exhibit complex distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f7369bf-673c-421e-a5b0-0c977260d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "def model_adasyn(X,y):\n",
    "    # Train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ### Train XGBoost WITHOUT ADASYN\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Train XGBoost without ADASYN\n",
    "    model_no_smote = xgb.train(params, dtrain, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_no_smote = model_no_smote.predict(dtest)\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_no_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    y_pred_binary_no_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_no_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_no_smote = calculate_metrics(y_test, y_pred_proba_no_smote)\n",
    "\n",
    "    \n",
    "    ### Apply ADASYN\n",
    "    adasyn = ADASYN(sampling_strategy=1, random_state=42)  # Fully balance classes\n",
    "    X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Train XGBoost WITH ADASYN\n",
    "    dtrain_resampled = xgb.DMatrix(X_train_resampled, label=y_train_resampled)\n",
    "    model_with_smote = xgb.train(params, dtrain_resampled, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_with_smote = model_with_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_with_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_with_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_with_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_with_smote = calculate_metrics(y_test, y_pred_proba_with_smote)\n",
    "\n",
    "    # Print comparison\n",
    "    metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(\"{:<20} {:<10} {:<10}\".format(\"Metric\", \"No ADASYN\", \"With Regular ADASYN\"))\n",
    "    for name, no_smote, with_smote in zip(metric_names, metrics_no_smote, metrics_with_smote):\n",
    "        print(f\"{name:<20} {no_smote:.4f}   {with_smote:.4f}\")\n",
    "\n",
    "    return [metrics_no_smote, metrics_with_smote]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ae260cc-c8d7-453e-a576-24dfa9783b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def custom_adasyn_with_cubic_interpolation(X, y, k_neighbors, beta, random_state=42):\n",
    "    \"\"\"\n",
    "    Implements the ADASYN algorithm with cubic polynomial interpolation for high-dimensional data.\n",
    "\n",
    "    Parameters:\n",
    "    - X: ndarray, shape (n_samples, n_features)\n",
    "        Feature matrix.\n",
    "    - y: ndarray, shape (n_samples,)\n",
    "        Target labels.\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled: ndarray\n",
    "        Resampled feature matrix with synthetic samples.\n",
    "    - y_resampled: ndarray\n",
    "        Resampled target labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure `y` is a 1D array\n",
    "    y = y.reset_index(drop=True)  # Ensure proper indexing\n",
    "    \n",
    "    # Separate minority class\n",
    "\n",
    "    # Identify minority and majority class\n",
    "    classes, class_counts = np.unique(y, return_counts=True)\n",
    "    minority_class = classes[np.argmin(class_counts)]\n",
    "    majority_class = classes[np.argmax(class_counts)]\n",
    "    \n",
    "    X_minority = X[y == minority_class]\n",
    "    n_minority = len(X_minority)\n",
    "    n_majority = len(X[y == majority_class])\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # Step 1: Compute the number of synthetic samples to generate\n",
    "    d = n_majority - n_minority  # Imbalance factor\n",
    "    G = d*beta  # Total synthetic samples needed\n",
    "\n",
    "    # Step 2: Find k-nearest neighbors for each minority sample\n",
    "    k = k_neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "    neighbors = knn.kneighbors(X_minority, return_distance=False)[:, 1:]\n",
    "\n",
    "    # Step 3: Compute the imbalance degree ri for each minority sample\n",
    "    ri = np.array([sum(y[neighbors[i]] != minority_class) / k for i in range(n_minority)])\n",
    "    if ri.sum() == 0:\n",
    "        return X, y  # No synthetic samples needed\n",
    "    ri = ri / ri.sum()  # Normalize ri to sum to 1\n",
    "\n",
    "    # Step 4: Generate synthetic samples using cubic polynomial interpolation\n",
    "    X_synthetic = []\n",
    "    for i in tqdm(range(n_minority)):\n",
    "        Gi = int(G * ri[i])  # Number of samples to generate for instance i\n",
    "        for _ in range(Gi):\n",
    "\n",
    "            neighbor_idx = np.random.choice(neighbors[i])  # Select a random neighbor\n",
    "            x_selected = X_minority.iloc[i]  # Minority instance\n",
    "            x_neighbor = X.iloc[neighbor_idx]  # Chosen neighbor\n",
    "\n",
    "            # Create synthetic sample feature-wise\n",
    "            \n",
    "            idx = np.random.choice(neighbors[i])  # Select a random neighbor\n",
    "            t_values = np.array([0, 0.33, 0.66, 1])  # 4 reference points in [0,1]\n",
    "            x_values = np.vstack([x_selected, \n",
    "                                  (2*x_selected + x_neighbor)/3, \n",
    "                                  (x_selected + 2*x_neighbor)/3, \n",
    "                                  x_neighbor])  # Intermediate points\n",
    "            # Generate polynomial coefficients for each feature\n",
    "            x_synthetic = np.zeros_like(x_selected)\n",
    "            t_random = np.random.rand()  # Random t in [0,1]\n",
    "            \n",
    "            for feature_idx in range(X.shape[1]):  # Iterate over all features\n",
    "                poly = Polynomial.fit(t_values, x_values[:, feature_idx], 3)  # Fit cubic polynomial\n",
    "                x_synthetic[feature_idx] = poly(t_random)  # Sample new point\n",
    "            \n",
    "\n",
    "            X_synthetic.append(x_synthetic)\n",
    "\n",
    "    X_synthetic = np.array(X_synthetic)\n",
    "    y_synthetic = np.full(len(X_synthetic), minority_class)\n",
    "\n",
    "    # Step 5: Return the augmented dataset\n",
    "    X_resampled = np.vstack((X, X_synthetic))\n",
    "    y_resampled = np.hstack((y, y_synthetic))\n",
    "\n",
    "    print(X_resampled.shape)\n",
    "\n",
    "    return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ee9a384-6c2d-4882-b177-9543acc4a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "637daa8c-dab8-489c-8f3a-cf34777585ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_adasyn_poly(X,y):\n",
    "    # Train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardize numerical features \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ### Train XGBoost WITHOUT ADASYN\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Train XGBoost without ADASYN\n",
    "    model_no_smote = xgb.train(params, dtrain, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_no_smote = model_no_smote.predict(dtest)\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_no_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    y_pred_binary_no_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_no_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_no_smote = calculate_metrics(y_test, y_pred_proba_no_smote)\n",
    "\n",
    "    pd.DataFrame(X_train,columns=X.columns)\n",
    "\n",
    "    X_train_resampled, y_train_resampled = custom_adasyn_with_cubic_interpolation(pd.DataFrame(X_train,columns=X.columns), y_train,5,1)\n",
    "    \n",
    "    # Train XGBoost WITH ADASYN\n",
    "    dtrain_resampled = xgb.DMatrix(np.array(X_train_resampled), label=y_train_resampled)\n",
    "    model_with_smote = xgb.train(params, dtrain_resampled, num_boost_round=200)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_proba_with_smote = model_with_smote.predict(dtest)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_with_smote)\n",
    "    f1_scores = (2 * precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "    optimal_idx = f1_scores.argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    y_pred_binary_with_smote = [1 if p > optimal_threshold else 0 for p in y_pred_proba_with_smote]  # Convert probabilities to binary predictions\n",
    "\n",
    "    metrics_with_smote = calculate_metrics(y_test, y_pred_proba_with_smote)\n",
    "\n",
    "    # Print comparison\n",
    "    metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(\"{:<20} {:<10} {:<10}\".format(\"Metric\", \"No ADASYN\", \"With Cubic Polynomial ADASYN\"))\n",
    "    for name, no_smote, with_smote in zip(metric_names, metrics_no_smote, metrics_with_smote):\n",
    "        print(f\"{name:<20} {no_smote:.4f}   {with_smote:.4f}\")\n",
    "\n",
    "    return [metrics_no_smote, metrics_with_smote]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0a8df4e-9e67-4666-b63f-d32976b63b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taiwan Credit dataset: \n",
      "\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No ADASYN  With Regular ADASYN\n",
      "Accuracy             0.8190   0.8018\n",
      "ROC-AUC              0.7771   0.7582\n",
      "PR-AUC               0.5392   0.5191\n",
      "Recall (Sensitivity) 0.3566   0.4306\n",
      "F1                   0.4618   0.4862\n",
      "Specificity          0.9477   0.9051\n",
      "FP-Rate              0.0523   0.0949\n",
      "G-Mean               0.5814   0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4676/4676 [00:16<00:00, 278.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30908, 23)\n",
      "\n",
      "--- Model Performance ---\n",
      "Metric               No ADASYN  With Cubic Polynomial ADASYN\n",
      "Accuracy             0.8190   0.8036\n",
      "ROC-AUC              0.7771   0.7596\n",
      "PR-AUC               0.5392   0.5172\n",
      "Recall (Sensitivity) 0.3566   0.4388\n",
      "F1                   0.4618   0.4931\n",
      "Specificity          0.9477   0.9051\n",
      "FP-Rate              0.0523   0.0949\n",
      "G-Mean               0.5814   0.6302\n"
     ]
    }
   ],
   "source": [
    "print(\"Taiwan Credit dataset: \\n\")\n",
    "output=model_adasyn(X,y)\n",
    "output_2=model_adasyn_poly(X,y['Y'])\n",
    "output.append(output_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4bd5c2bb-7139-495b-afdc-cd1daec6d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "163fde3a-d4c3-4ca9-b34f-ae3256997672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance ---\n",
      "Metric                         No ADASYN  With Regular ADASYN  With Cubic Polynomial Interpolation ADASYN\n",
      "Accuracy                       0.819              0.802              0.804\n",
      "ROC-AUC                        0.777              0.758              0.760\n",
      "PR-AUC                         0.539              0.519              0.517\n",
      "Recall (Sensitivity)           0.357              0.431              0.439\n",
      "F1                             0.462              0.486              0.493\n",
      "Specificity                    0.948              0.905              0.905\n",
      "FP-Rate                        0.052              0.095              0.095\n",
      "G-Mean                         0.581              0.624              0.630\n"
     ]
    }
   ],
   "source": [
    "metric_names = [\"Accuracy\", \"ROC-AUC\", \"PR-AUC\", \"Recall (Sensitivity)\", \n",
    "                \"F1\", \"Specificity\", \"FP-Rate\", \"G-Mean\"]\n",
    "\n",
    "print(\"\\n--- Model Performance ---\")\n",
    "print(\"{:<30} {:<10} {:<20} {:<20}\".format(\"Metric\", \"No ADASYN\",\n",
    "                                            \"With Regular ADASYN\",\n",
    "                                            \"With Cubic Polynomial Interpolation ADASYN\"))\n",
    "\n",
    "for i in range(len(metric_names)):\n",
    "    print(\"{:<30} {:.3f}              {:.3f}              {:.3f}\".format(\n",
    "        metric_names[i], metrics[0][i], metrics[1][i], metrics[2][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa86591-b3bb-459a-910f-45d8cdf1591e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963dd59f-60a3-4d72-9aa2-023dbd2dc097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd72180-7710-4afd-8c2f-7a8009b8411a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
